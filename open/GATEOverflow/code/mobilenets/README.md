The mobilenet models are run using the [TFLite CPP implementation](https://github.com/mlcommons/mlperf-automations/tree/main/script/app-mlperf-inference-tflite-cpp). Tensorflow lite is built from source with XNNPACK enabled. 

Please follow the [MLPerf Inference docs](https://docs.mlcommons.org/inference/benchmarks/image_classification/mobilenets/#__tabbed_1_4) for the commands to run the models.

