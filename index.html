
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.19">
    
    
      
        <title>MLPerf Inference Results Comparison</title>
      
    
    
  
      <link rel="stylesheet" href="assets/stylesheets/main.7e37652d.min.css">
      
        
        <link rel="stylesheet" href="assets/stylesheets/palette.06af60db.min.css">
      
      


    
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/jquery.tablesorter.pager.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blackice.min.css">
<link rel="stylesheet" rel="preload" as="style" media="all" href="thirdparty/tablesorter/dist/css/theme.blue.min.css">
<style type="text/css">

table.resultstable, table.counttable {
    overflow-x: auto;
}

.pager1{
    display: none!important;
}

.pagerSavedHeightSpacer {
    display: none!important;
}
.resultstable_wrapper, .counttable_wrapper {
    overflow-x: auto;
}

/* General form styling */
form {
    max-width: 800px;
    margin: 0 auto;
    padding: 20px;
    background-color: #f5f5f5;
    border-radius: 8px;
    box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
}

/* Style for form group */
.form-group {
    margin-bottom: 20px;
    position: relative;
}

/* Label styling */
label {
    display: block;
    font-size: 14px;
    color: #333;
    margin-bottom: 5px;
    font-weight: 500;
}

/* Select styling */
select {
    width: 100%;
    padding: 10px;
    font-size: 16px;
    color: #333;
    background-color: #fff;
    border: 1px solid #ccc;
    border-radius: 4px;
    box-shadow: none;
    appearance: none;
    transition: border-color 0.3s ease;
}

select:focus {
    border-color: #6200ee;
    outline: none;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

/* Custom arrow for select */
select::-ms-expand {
    display: none;
}

.select-wrapper {
    position: relative;
}

.select-wrapper::after {
    content: '';
    position: absolute;
    top: 50%;
    right: 10px;
    width: 0;
    height: 0;
    border-left: 5px solid transparent;
    border-right: 5px solid transparent;
    border-top: 5px solid #333;
    pointer-events: none;
    transform: translateY(-50%);
}

/* Button styling */
button {
    width: 100%;
    padding: 12px;
    font-size: 16px;
    color: #fff;
    background-color: #6200ee;
    border: none;
    border-radius: 4px;
    cursor: pointer;
    transition: background-color 0.3s ease;
}

button:hover {
    background-color: #3700b3;
}

button:active {
    background-color: #6200ee;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.2);
}

button:disabled {
    background-color: #ccc;
    cursor: not-allowed;
}

/* Input and button ripple effect */
.button-ripple, .input-ripple {
    position: relative;
    overflow: hidden;
}

.button-ripple::after, .input-ripple::after {
    content: '';
    position: absolute;
    top: 50%;
    left: 50%;
    width: 100px;
    height: 100px;
    background: rgba(255, 255, 255, 0.4);
    border-radius: 50%;
    transform: translate(-50%, -50%) scale(0);
    transition: transform 0.5s ease, opacity 1s ease;
    opacity: 0;
    pointer-events: none;
}

.button-ripple:active::after, .input-ripple:focus::after {
    transform: translate(-50%, -50%) scale(1);
    opacity: 1;
    transition: 0s;
}

/* Responsive Design */
@media (max-width: 768px) {
    form {
        max-width: 100%;
        padding: 15px;
    }

    button {
        padding: 10px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }
}

@media (max-width: 480px) {
    form {
        padding: 10px;
    }

    button {
        padding: 8px;
        font-size: 14px;
    }

    select {
        font-size: 14px;
        padding: 8px;
    }

    label {
        font-size: 13px;
    }
}

select.pagesize, select.gotoPage {
    width: fit-content;
    padding: 5px;
}


/*Testing*/
/* Base Table Styles */
#results_table {
    margin: 20px 0;
    overflow-x: auto;
}

.tablesorter {
    width: 100%;
    border-collapse: collapse;
    background-color: #fff;
    font-size: 14px;
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1);
    border-radius: 8px;
    overflow: hidden;
}

/* Header and Footer Styles */
.tablesorter thead th,
.tablesorter tfoot th {
    background-color: #6200ea; /* Material Design Purple */
    color: #ffffff;
    padding: 12px;
    text-align: left;
    text-transform: uppercase;
    font-weight: 500;
    border-bottom: 2px solid #512da8; /* Darker purple for bottom border */
}

.tablesorter tfoot th {
    background-color: #6200ea;
}

/* Body Row Styles */
.tablesorter tbody tr {
    border-bottom: 1px solid #e0e0e0;
    transition: background-color 0.3s ease;
}

.tablesorter tbody tr:nth-child(even) {
    background-color: #f9f9f9;
}

.tablesorter tbody tr:hover {
    background-color: #eeeeee;
}

.tablesorter tbody td {
    padding: 12px;
    text-align: left;
    color: #424242;
}

/* Specific Column Styles */
#col-id,
#col-system,
#col-submitter,
#col-accelerator {
    font-weight: bold;
}
#col-id, .col-id {
    min-width: 80px;
    max-width: 80px;
    width: 80px;
    left: 0px;
}
td.col-result {
    min-width: 100px;
    text-align: right!important;
}
td.col-system, th.col-system {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    word-break: break-word;
    left: 80px;
}
td.col-submitter, th.col-submitter {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 230px;
}
td.col-accelerator, th.col-accelerator {
    min-width: 120px;
    max-width: 120px;
    width: 120px;
    word-break: break-word;
    left: 350px;
}

td.count-submitter, th.count-submitter {
    min-width: 150px;
    max-width: 150px;
    width: 150px;
    left: 0px;
    background: white;
    font-weight: 600;
    font-size: Large;
    word-break: break-word;
    position:sticky;
}
.col-scenario {
    font-weight: 500;
    color: #6200ea;
    text-align: center;
}

.col-result {
    text-align: right;
    color: #303f9f; /* Material Design Blue */
    font-weight: 600;
}

.headcol {
  position: sticky;
  background:white;
  border:none!important;
}

.collapsible {
    color: white;
    cursor: pointer;
    padding: 10px;
    width: 100%;
    border: none;
    text-align: left;
    outline: none;
    font-size: 15px;
}

.chart {
    /* padding: 0 18px; */
    display: none; /* Hidden by default */
    overflow: hidden;
    height: 500px;
    width: 100%;
    background-color: white;
}


/* Responsive Design */
@media (max-width: 768px) {
    .tablesorter thead,
    .tablesorter tfoot {
        display: none;
    }

    .tablesorter tbody tr {
        display: block;
        margin-bottom: 15px;
        border: 1px solid #ddd;
        border-radius: 8px;
    }

    .tablesorter tbody td {
        display: flex;
        justify-content: space-between;
        padding: 10px;
        text-align: right;
    }

    .tablesorter tbody td::before {
        content: attr(data-label);
        font-weight: bold;
        color: #6200ea;
        text-transform: uppercase;
    }

    .tablesorter tbody td:first-child {
        border-top-left-radius: 8px;
        border-bottom-left-radius: 8px;
    }

    .tablesorter tbody td:last-child {
        border-top-right-radius: 8px;
        border-bottom-right-radius: 8px;
    }
}



</style>

  <!-- Add scripts that need to run afterwards here -->

    
  
      
    
<!-- load jQuery and tablesorter scripts -->
<script src="https://code.jquery.com/jquery-3.7.1.min.js" integrity="sha256-/JqT3SQfawRcv/BIHPThkBvs0OEvtFFmqPF/lYI/Cxo=" crossorigin="anonymous"></script>


<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.js"></script>
<!-- tablesorter widgets (optional) -->

<script type="text/javascript" src="thirdparty/tablesorter/dist/js/jquery.tablesorter.widgets.js"></script>
<script type="text/javascript" src="thirdparty/tablesorter/dist/js/extras/jquery.tablesorter.pager.min.js"></script>
  <!-- Add scripts that need to run before here -->
  <script src="https://cdn.canvasjs.com/ga/jquery.canvasjs.min.js"></script>
  
<script type="text/javascript" src="javascripts/config.js"></script>
<script type="text/javascript" src="javascripts/common.js"></script>
  <!-- Add scripts that need to run afterwards here -->

    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL(".",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="yellow">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="." title="MLPerf Inference Results Comparison" class="md-header__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MLPerf Inference Results Comparison
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Results
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/inference_results_v5.1" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="." class="md-tabs__link">
        
  
  
    
  
  Results

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="." title="MLPerf Inference Results Comparison" class="md-nav__button md-logo" aria-label="MLPerf Inference Results Comparison" data-md-component="logo">
      
  <img src="img/logo_v2.svg" alt="logo">

    </a>
    MLPerf Inference Results Comparison
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/inference_results_v5.1" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path fill="currentColor" d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
      <a href="." class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Results
    
  </span>
  

      </a>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  



  
  


  <h1>Results</h1>

<html>

        <h2 id="results_heading_available" class="results_table_heading">Datacenter Category: Available submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_available" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_available"><thead> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></thead><tfoot> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 5.1-0001 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.9.0.2.dev108+g71faa1880.d20250730.rocm641, Pytorch 2.7.0+gitf717b2a, ROCm 6.4.1.60401-83~22.04
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/AMD/systems/8xMI300X_2xEPYC_9575F.json"> Supermicro AS-8125GS-TNMR2 </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99/Server"> 24593.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99/Interactive"> 8840.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99/Offline"> 27803.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99.9/Server"> 24593.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99.9/Interactive"> 8840.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI300X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 27803.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0002 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.9.0.2.dev108+g71faa1880.d20250731.rocm641, Pytorch 2.7.0+gitf717b2a, ROCm 6.4.1.60401-83~22.04
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Machine 1
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/AMD/systems/8xMI325X_2xEPYC_9575F_1.json"> QuantaGrid D74A-7U  </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_1/llama2-70b-99/Server"> 32027.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_1/llama2-70b-99/Offline"> 34520.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_1/llama2-70b-99.9/Server"> 32027.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_1/llama2-70b-99.9/Offline"> 34520.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0003 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Machine 2
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/AMD/systems/8xMI325X_2xEPYC_9575F_2.json"> QuantaGrid D74A-7U </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_2/llama2-70b-99/Interactive"> 18407.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_2/llama2-70b-99/Offline"> 34555.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_2/llama2-70b-99.9/Interactive"> 18407.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325X_2xEPYC_9575F_2/llama2-70b-99.9/Offline"> 34555.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0004 </td>
        <td class="col-system headcol" title="
Processor: 2xAMD EPYC 9655
Software: shark-ai 3.3.0
Cores per processor: 
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/AMD/systems/8xMI325x_2xEPYC-9655.json"> Quanta S7PA </a> </td>
        <td class="col-submitter headcol"> AMD </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8,fp16,int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325x_2xEPYC-9655/stable-diffusion-xl/Server"> 16.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8,fp16,int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/AMD/results/8xMI325x_2xEPYC-9655/stable-diffusion-xl/Offline"> 18.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0005 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9374F 32-Core Processor
Software: TensorRT 10.8, CUDA 12.9
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: Data bandwidth for GPU-PCIe: 504 GB/s; PCIe-NIC: 126 GB/s
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/ASUSTeK/systems/ESC8000_H200_NVLx8_TRT.json"> ESC8000A-E12 (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/llama2-70b-99/Server"> 25614.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/llama2-70b-99/Offline"> 30487.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/llama2-70b-99.9/Server"> 24554.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/llama2-70b-99.9/Offline"> 30490.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/dlrm-v2-99/Server"> 450100.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/dlrm-v2-99/Offline"> 580460.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/dlrm-v2-99.9/Server"> 330555.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/dlrm-v2-99.9/Offline"> 348225.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/retinanet/Server"> 12801.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC8000_H200_NVLx8_TRT/retinanet/Offline"> 13319.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0006 </td>
        <td class="col-system headcol" title="
Processor: 2x AMD EPYC 9475F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/ASUSTeK/systems/ESC_A8A_MI325X_256GBx8.json"> ESC_A8A_MI325X_256GBx8 </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Server"> 31241.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Interactive"> 15566.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99/Offline"> 33987.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Server"> 31241.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Interactive"> 15566.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_A8A_MI325X_256GBx8/llama2-70b-99.9/Offline"> 33987.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0007 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8580
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 60
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/ASUSTeK/systems/ESC_N8_H200-SXM-141GBx8_TRT.json"> ASUSTeK ESC N8 H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> ASUSTeK </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 34193.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 35075.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 29026.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34887.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 591162.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 643122.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 370066.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 388058.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/retinanet/Server"> 14406.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/ASUSTeK/results/ESC_N8_H200-SXM-141GBx8_TRT/retinanet/Offline"> 14451.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0008 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 1
Notes: NVIDIA GB200 NVL4
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Azure/systems/ND_GB200_v6.json"> ND_GB200_v6 </a> </td>
        <td class="col-submitter headcol"> Azure </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Azure/results/ND_GB200_v6/llama2-70b-99/Server"> 46016.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Azure/results/ND_GB200_v6/llama2-70b-99/Offline"> 52061.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0009 </td>
        <td class="col-system headcol" title="
Processor: AMD TURIN 9965
Software: TensorRT 10.11, CUDA 12.8
Cores per processor: 192
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W.  VMware ESXi 9.0.0.0.24755229
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Broadcom_Supermicro/systems/vSYS-422GA-NBRT-B200-SXM-180GBx8_TRT.json"> Supermicro SYS-422GA-NBRT-LCC (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Broadcom_Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS-422GA-NBRT-B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Broadcom_Supermicro/results/vSYS-422GA-NBRT-B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 31.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0010 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/C240M8-1-node-2S-GNR_86C.json"> C240M8-1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C240M8-1-node-2S-GNR_86C/retinanet/Server"> 400.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C240M8-1-node-2S-GNR_86C/retinanet/Offline"> 498.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0011 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F 64-Core Processor
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: H200-NVL TGP 600W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/C845A_H200_NVLx8_TRT.json"> Cisco UCS C845A M8 (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C845A_H200_NVLx8_TRT/retinanet/Server"> 13202.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C845A_H200_NVLx8_TRT/retinanet/Offline"> 13263.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0012 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F 64-Core Processor
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: L40S TGP 350W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/C845A_L40Sx8_TRT.json"> Cisco UCS C845A M8 (8x L40S-48GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C845A_L40Sx8_TRT/retinanet/Server"> 6101.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C845A_L40Sx8_TRT/retinanet/Offline"> 6518.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0013 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 128
Processors per node: 2
Nodes: 2
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/C885A_M8_H200_SXM_141GBx16_TRT.json"> Cisco UCS C885A M8 (16x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx16_TRT/llama2-70b-99/Server"> 66439.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx16_TRT/llama2-70b-99/Offline"> 68894.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx16_TRT/llama2-70b-99.9/Server"> 66439.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885A_M8_H200_SXM_141GBx16_TRT/llama2-70b-99.9/Offline"> 68894.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0014 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F 64-Core Processor
Software: TensorRT 10.10, CUDA 12.9
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/C885_H200x8_TRT.json"> Cisco UCS C885A M8 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99/Server"> 33163.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99/Interactive"> 19621.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99/Offline"> 34548.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99.9/Server"> 33163.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99.9/Interactive"> 19621.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/llama2-70b-99.9/Offline"> 34548.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/stable-diffusion-xl/Server"> 18.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/stable-diffusion-xl/Offline"> 19.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/retinanet/Server"> 14355.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/C885_H200x8_TRT/retinanet/Offline"> 14923.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0015 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8592+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 64
Processors per node: 2
Nodes: 2
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/HGX-H100_H100-SXM-80GBx16_TRT.json"> HPF HGX System (16xH100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx16_TRT/llama2-70b-99/Server"> 61101.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx16_TRT/llama2-70b-99/Offline"> 62351.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx16_TRT/llama2-70b-99.9/Server"> 61101.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx16_TRT/llama2-70b-99.9/Offline"> 62351.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0016 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8592+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 64
Processors per node: 2
Nodes: 4
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/HGX-H100_H100-SXM-80GBx32_TRT.json"> HPF HGX System (32xH100-SXM-80GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx32_TRT/llama2-70b-99/Server"> 122274.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx32_TRT/llama2-70b-99/Offline"> 124879.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx32_TRT/llama2-70b-99.9/Server"> 122274.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/HGX-H100_H100-SXM-80GBx32_TRT/llama2-70b-99.9/Offline"> 124879.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0017 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Cisco/systems/X210M8-1-node-2S-GNR_86C.json"> X210M8-1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Cisco </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/X210M8-1-node-2S-GNR_86C/dlrm-v2-99.9/Server"> 11801.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/X210M8-1-node-2S-GNR_86C/dlrm-v2-99.9/Offline"> 12503.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/X210M8-1-node-2S-GNR_86C/retinanet/Server"> 400.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Cisco/results/X210M8-1-node-2S-GNR_86C/retinanet/Offline"> 501.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0018 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6787P
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 86
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/R470-L4-PCIE-24GBx2_TRT.json"> PowerEdge R470 (2x L4-PCIE-24GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L4-PCIe-24GB x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R470-L4-PCIE-24GBx2_TRT/retinanet/Server"> 428.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R470-L4-PCIE-24GBx2_TRT/retinanet/Offline"> 458.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0019 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6761P
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 64
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/R570-L4-PCIE-24GBx4_TRT.json"> PowerEdge R570 (4x L4-PCIE-24GBx4, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L4-PCIe-24GB x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R570-L4-PCIE-24GBx4_TRT/stable-diffusion-xl/Server"> 0.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R570-L4-PCIE-24GBx4_TRT/stable-diffusion-xl/Offline"> 1.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R570-L4-PCIE-24GBx4_TRT/retinanet/Server"> 814.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R570-L4-PCIE-24GBx4_TRT/retinanet/Offline"> 905.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0020 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: Dell PowerEdge R770. INT8
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/R770_XEON6787Px2.json"> R770_XEON6787Px2 </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R770_XEON6787Px2/dlrm-v2-99.9/Server"> 11801.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/R770_XEON6787Px2/dlrm-v2-99.9/Offline"> 12421.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0021 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9655P
Software: TensorRT 10.8.0, CUDA 12.9
Cores per processor: 96
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/R7715-H100-NVL-94GBx3_TRT.json"> Dell PowerEdge R7715 (3x H100-NVL-94GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H100-NVL-94GB x 3 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0022 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon 6787P 86-Core Processor
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 172
Processors per node: 2
Nodes: 1
Notes: L40S TGP 350W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE7740_L40S_PCIe_48GBx8_TRT.json"> Dell PowerEdge XE7740 (8x L40S, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7740_L40S_PCIe_48GBx8_TRT/stable-diffusion-xl/Server"> 5.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7740_L40S_PCIe_48GBx8_TRT/stable-diffusion-xl/Offline"> 5.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7740_L40S_PCIe_48GBx8_TRT/retinanet/Server"> 6101.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7740_L40S_PCIe_48GBx8_TRT/retinanet/Offline"> 6509.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0023 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9655 96-Core Processor
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 96
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE7745_H200_NVL_141GBx8_TRT.json"> Dell PowerEdge XE7745 (8x H200-NVL-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Server"> 29070.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Interactive"> 16419.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99/Offline"> 31267.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Server"> 29070.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Interactive"> 16419.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/llama2-70b-99.9/Offline"> 31267.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/stable-diffusion-xl/Server"> 16.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/stable-diffusion-xl/Offline"> 17.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/retinanet/Server"> 12601.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE7745_H200_NVL_141GBx8_TRT/retinanet/Offline"> 13421.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0024 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8562Y+
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE9680L_B200_SXM_180GBx8_TRT.json"> Dell PowerEdge XE9680L (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99/Server"> 99139.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99/Interactive"> 60137.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99/Offline"> 101253.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server"> 99139.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Interactive"> 60137.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Offline"> 101253.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0025 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8580
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 120
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE9680L_H200_SXM_141GBx8_TRT.json"> Dell PowerEdge XE9680L (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 33161.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 34588.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 33161.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 34588.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/retinanet/Server"> 13603.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680L_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14542.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0026 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) PLATINUM 8568Y+
Software: TensorRT 10.11, CUDA 12.8
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE9680_H200_SXM_141GBx8_TRT.json"> Dell PowerEdge XE9680 (8xH200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 33244.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Interactive"> 21915.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 35316.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 33244.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Interactive"> 21915.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 35316.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/retinanet/Server"> 13603.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14960.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0027 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon 8462Y+
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE9680_MI300X_192GBx8.json"> Dell Poweredge XE9680 </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99/Server"> 24747.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99/Interactive"> 8455.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99/Offline"> 27325.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99.9/Server"> 24747.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99.9/Interactive"> 8455.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9680_MI300X_192GBx8/llama2-70b-99.9/Offline"> 27325.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0028 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9965 192-Core Processor
Software: TensorRT 10.11, CUDA 12.8
Cores per processor: 192
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell/systems/XE9685L_B200_SXM_180GBx8_TRT.json"> Dell PowerEdge XE9685L (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Dell </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/llama2-70b-99/Server"> 99214.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/llama2-70b-99/Offline"> 102131.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Server"> 99214.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/llama2-70b-99.9/Offline"> 102131.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell/results/XE9685L_B200_SXM_180GBx8_TRT/stable-diffusion-xl/Offline"> 32.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0029 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y
Software: TensorRT 10.11, CUDA 12.8
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: VM Specifications 48vCPU out of 96 and memory of 2TB GB out of 2TB
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell_Broadcom/systems/vXE9680_H200-SXM-141GBx8_TRT.json"> Dell PowerEdge XE9680 (8x H200-SXM-141GB,VMware ESXi 9.0.0) </a> </td>
        <td class="col-submitter headcol"> Dell_Broadcom </td>
        <td class="col-accelerator headcol"> Virtualized NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33384.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34301.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33370.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34485.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Server"> 18.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp16', 'clip2': 'fp16', 'unet': 'fp8', 'vae': 'int8'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_Broadcom/results/vXE9680_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 18.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0030 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Platinum 8460Y+
Software: Mango LLMBoost AI Enterprise Platform, vllm-0.9.0.2.dev108+g71faa1880.d20250717.rocm641, ROCm 6.12.12
Cores per processor: 80
Processors per node: 2
Nodes: 2
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell_MangoBoost/systems/16xMI300X_4xXeon_8460Y.json"> Dell PowerEdge XE9680 (2x8x MI300X-192GB MangoBoost-LLMBoost) </a> </td>
        <td class="col-submitter headcol"> Dell_MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/16xMI300X_4xXeon_8460Y/llama2-70b-99/Server"> 47561.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/16xMI300X_4xXeon_8460Y/llama2-70b-99/Offline"> 53572.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/16xMI300X_4xXeon_8460Y/llama2-70b-99.9/Server"> 47561.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/16xMI300X_4xXeon_8460Y/llama2-70b-99.9/Offline"> 53572.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0031 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Platinum 8460Y+
Software: Mango LLMBoost AI Enterprise Platform, vllm-0.9.0.2.dev108+g71faa1880.d20250717.rocm641, ROCm 6.12.12
Cores per processor: 80
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Dell_MangoBoost/systems/8xMI300X_2xXeon_8460Y.json"> Dell PowerEdge XE9680 (8x MI300X-192GB MangoBoost-LLMBoost) </a> </td>
        <td class="col-submitter headcol"> Dell_MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/8xMI300X_2xXeon_8460Y/llama2-70b-99/Server"> 24532.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/8xMI300X_2xXeon_8460Y/llama2-70b-99/Offline"> 27678.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/8xMI300X_2xXeon_8460Y/llama2-70b-99.9/Server"> 24532.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Dell_MangoBoost/results/8xMI300X_2xXeon_8460Y/llama2-70b-99.9/Offline"> 27678.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0032 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) w7-2495X
Software: TensorRT
Cores per processor: 24
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/GATEOverflow/systems/764987c5c33d-nvidia-gpu-TensorRT-default_config.json"> GATE Overflow Intel Sapphire Rapids (2x RTX 4090) </a> </td>
        <td class="col-submitter headcol"> GATEOverflow </td>
        <td class="col-accelerator headcol"> NVIDIA GeForce RTX 4090 x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GATEOverflow/results/764987c5c33d-nvidia-gpu-TensorRT-default_config/retinanet/server"> 1449.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GATEOverflow/results/764987c5c33d-nvidia-gpu-TensorRT-default_config/retinanet/offline"> 1760.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0033 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) w7-2495X
Software: vllm
Cores per processor: 24
Processors per node: 1
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/GATEOverflow/systems/764987c5c33d-reference-gpu-vllm-default_config.json"> GATE Overflow Intel Sapphire Rapids (2x RTX 4090) </a> </td>
        <td class="col-submitter headcol"> GATEOverflow </td>
        <td class="col-accelerator headcol"> NVIDIA GeForce RTX 4090 x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0045 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/GigaComputing/systems/8xMI325X_2xEPYC_9575F.json"> G893-ZX1-AAX2 </a> </td>
        <td class="col-submitter headcol"> GigaComputing </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server"> 32139.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Interactive"> 18825.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Offline"> 34483.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server"> 32139.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Interactive"> 18825.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 34483.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0046 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6745P
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/GigaComputing/systems/G894-SD1_B200-SXM-180GBx8_TRT.json"> G894-SD1 </a> </td>
        <td class="col-submitter headcol"> GigaComputing </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99066.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99/Interactive"> 62851.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 98607.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99066.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 62851.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 98607.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/GigaComputing/results/G894-SD1_B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 31.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0047 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Google/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99169.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Interactive"> 52140.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 78463.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99169.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 52140.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 78463.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 32.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0048 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480C
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Google/systems/H200-SXM-141GBx8_TRT.json"> NVIDIA H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Google </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Interactive"> 19554.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 35043.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Interactive"> 19554.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 35043.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/retinanet/Server"> 13603.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Google/results/H200-SXM-141GBx8_TRT/retinanet/Offline"> 14718.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0049 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8562Y+
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 32
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/HPE/systems/HPE_Cray_XD670_H200_SXM_141GBx8_TRT.json"> HPE Cray XD670 (8x H200-SXM-141GB) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Server"> 33164.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Interactive"> 20432.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99/Offline"> 34964.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Server"> 33144.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Interactive"> 20356.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/llama2-70b-99.9/Offline"> 34797.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/retinanet/Server"> 14205.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_Cray_XD670_H200_SXM_141GBx8_TRT/retinanet/Offline"> 14996.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0050 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6787P
Software: TensorRT 10.11.0.33, CUDA 12.9
Cores per processor: 172
Processors per node: 2
Nodes: 1
Notes: H200 TGP 600W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/HPE/systems/HPE_PROLIANT_DL380A_H200_NVL_141GBX10_TRT.json"> HPE ProLiant Compute DL380a Gen12 (10x H200-NVL-141GB) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 10 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX10_TRT/dlrm-v2-99/Server"> 650211.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX10_TRT/dlrm-v2-99/Offline"> 713817.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX10_TRT/dlrm-v2-99.9/Server"> 413565.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_H200_NVL_141GBX10_TRT/dlrm-v2-99.9/Offline"> 433591.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0051 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6787P
Software: TensorRT 10.11.0.33, CUDA 12.9
Cores per processor: 172
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/HPE/systems/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT.json"> HPE ProLiant Compute DL380a Gen12 (8x RTX Pro 6000 Blackwell Server Edition) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA RTX PRO 6000 Blackwell Server Edition x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/llama2-70b-99/Server"> 26005.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/llama2-70b-99/Offline"> 26205.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/llama2-70b-99.9/Server"> 26001.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/llama2-70b-99.9/Offline"> 26205.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/stable-diffusion-xl/Server"> 10.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_PROLIANT_DL380A_RTX_PRO_6000_PCIE_96GBx8_TRT_TRT/stable-diffusion-xl/Offline"> 11.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0052 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9455
Software: TensorRT 10.11.0, CUDA 12.9
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: H200
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/HPE/systems/HPE_PROLIANT_DL385_H200NVL_PCIE_141GBx2_TRT_TRT.json"> HPE ProLiant DL385 Gen11 (2x H200-NVL-141GB) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0053 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.10, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 1
Notes: NVIDIA GH200 144GB HBM3e
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/HPE/systems/HPE_ProLiant_DL384_GH200_144GBx2_TRT.json"> HPE ProLiant Compute DL384 Gen12 (2x GH200-144GB_aarch64) </a> </td>
        <td class="col-submitter headcol"> HPE </td>
        <td class="col-accelerator headcol"> NVIDIA GH200 Grace Hopper Superchip 144GB x 2 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL384_GH200_144GBx2_TRT/dlrm-v2-99/Server"> 161030.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/HPE/results/HPE_ProLiant_DL384_GH200_144GBx2_TRT/dlrm-v2-99/Offline"> 174456.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0055 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6980P
Software: PyTorch
Cores per processor: 128
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Intel/systems/1-node-2S-GNR_128C.json"> 1-node-2S-GNR_128C </a> </td>
        <td class="col-submitter headcol"> Intel </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Server"> 18190.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99/Offline"> 18495.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server"> 18190.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Offline"> 18495.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/retinanet/Server"> 579.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-2S-GNR_128C/retinanet/Offline"> 708.3 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0056 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) w9-3475X
Software: vLLM 0.9.0, PyTorch 2.7.0
Cores per processor: 36
Processors per node: 1
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Intel/systems/1-node-4x-BMG-Pro-B60-Dual.json"> 1-node-4x-BMG-Pro-B60-Dual </a> </td>
        <td class="col-submitter headcol"> Intel </td>
        <td class="col-accelerator headcol"> [MAXSUN] MS-Intel Arc Pro B60 Dual 48G Turbo x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: UINT4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-4x-BMG-Pro-B60-Dual/llama2-70b-99/Server"> 2110.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: UINT4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-4x-BMG-Pro-B60-Dual/llama2-70b-99/Offline"> 3009.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: UINT4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-4x-BMG-Pro-B60-Dual/llama2-70b-99.9/Server"> 2110.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: UINT4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel/results/1-node-4x-BMG-Pro-B60-Dual/llama2-70b-99.9/Offline"> 3009.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0058 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Intel_VMware/systems/1-node-2S-GNR_86C_BareMetal.json"> 1-node-2S-GNR_86C_BareMetal </a> </td>
        <td class="col-submitter headcol"> Intel_VMware </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_BareMetal/dlrm-v2-99.9/Server"> 11801.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_BareMetal/dlrm-v2-99.9/Offline"> 12274.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_BareMetal/retinanet/Server"> 365.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_BareMetal/retinanet/Offline"> 479.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0059 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: VM Configuration:VCPUs:172,VCPUs per NUMA 43,VM version 22. VMware ESXi 9.0.0.0.24755229
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Intel_VMware/systems/1-node-2S-GNR_86C_ESXi_172VCPU-VM.json"> 1-node-2S-GNR_86C_ESXi_172VCPU-VM </a> </td>
        <td class="col-submitter headcol"> Intel_VMware </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_ESXi_172VCPU-VM/dlrm-v2-99.9/Server"> 11566.1 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_ESXi_172VCPU-VM/dlrm-v2-99.9/Offline"> 11981.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_ESXi_172VCPU-VM/retinanet/Server"> 361.7 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Intel_VMware/results/1-node-2S-GNR_86C_ESXi_172VCPU-VM/retinanet/Offline"> 467.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0060 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Lambda/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lambda </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lambda/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99993.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lambda/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 102725.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lambda/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: {'clip1': 'fp32', 'clip2': 'fp32', 'unet': 'fp8', 'vae': 'fp32'}" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lambda/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 32.6 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0061 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Lenovo/systems/SR680a_V3_B200SXMx8_TRT.json"> ThinkSystem SR680a V3 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR680a_V3_B200SXMx8_TRT/llama2-70b-99/Server"> 99159.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR680a_V3_B200SXMx8_TRT/llama2-70b-99/Offline"> 102909.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR680a_V3_B200SXMx8_TRT/llama2-70b-99.9/Server"> 99203.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR680a_V3_B200SXMx8_TRT/llama2-70b-99.9/Offline"> 102909.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0062 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8568Y+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Lenovo/systems/SR780a_V3_B200SXMx8_TRT.json"> ThinkSystem SR780a V3 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR780a_V3_B200SXMx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/SR780a_V3_B200SXMx8_TRT/stable-diffusion-xl/Offline"> 31.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0063 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: N/A. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Lenovo/systems/ThinkSystem_SR650_V4_GNR_86C.json"> ThinkSystem_SR650_V4_GNR_86C </a> </td>
        <td class="col-submitter headcol"> Lenovo </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/ThinkSystem_SR650_V4_GNR_86C/dlrm-v2-99.9/Server"> 11991.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/ThinkSystem_SR650_V4_GNR_86C/dlrm-v2-99.9/Offline"> 12230.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/ThinkSystem_SR650_V4_GNR_86C/retinanet/Server"> 375.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Lenovo/results/ThinkSystem_SR650_V4_GNR_86C/retinanet/Offline"> 468.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0065 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9534 (on MI300X node), AMD EPYC 9655 (on MI325X node)
Software: Mango LLMBoost AI Enterprise Platform, vllm-0.9.0.2.dev108+g71faa1880.d20250717.rocm641, ROCm 6.12.12
Cores per processor: 64(AMD EPYC 9534) and 96(AMD EPYC 9655)
Processors per node: 2
Nodes: 6
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/MangoBoost/systems/32xMI300X_2xEPYC_9534_16xMI325X_2xEPYC_9655.json"> MangoBoost Heterogeneous Cluster (4x8x MI300X-192GB, 2x8x MI325X-256GB, LLMBoost) </a> </td>
        <td class="col-submitter headcol"> MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 (x32), AMD Instinct MI325X 256GB HBM3e (x16) x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534_16xMI325X_2xEPYC_9655/llama2-70b-99/Server"> 153076.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534_16xMI325X_2xEPYC_9655/llama2-70b-99/Offline"> 169197.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534_16xMI325X_2xEPYC_9655/llama2-70b-99.9/Server"> 153076.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/32xMI300X_2xEPYC_9534_16xMI325X_2xEPYC_9655/llama2-70b-99.9/Offline"> 169197.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0066 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9534
Software: Mango LLMBoost AI Enterprise Platform, vllm-0.9.0.2.dev108+g71faa1880.d20250717.rocm641, ROCm 6.12.12
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/MangoBoost/systems/8xMI300X_2xEPYC_9534.json"> Supermicro AS-8125GS-TNMR2 (8x MI300X-192GB MangoBoost-LLMBoost) </a> </td>
        <td class="col-submitter headcol"> MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI300X 192GB HBM3 x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI300X_2xEPYC_9534/llama2-70b-99/Server"> 24860.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI300X_2xEPYC_9534/llama2-70b-99/Offline"> 27854.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI300X_2xEPYC_9534/llama2-70b-99.9/Server"> 24860.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI300X_2xEPYC_9534/llama2-70b-99.9/Offline"> 27854.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0067 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9655
Software: Mango LLMBoost AI Enterprise Platform, vllm-0.9.0.2.dev108+g71faa1880.d20250717.rocm641, ROCm 6.12.12
Cores per processor: 96
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/MangoBoost/systems/8xMI325X_2xEPYC_9655.json"> MangoBoost (8x MI325X-256GB, LLMBoost) </a> </td>
        <td class="col-submitter headcol"> MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI325X_2xEPYC_9655/llama2-70b-99/Server"> 31671.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI325X_2xEPYC_9655/llama2-70b-99/Offline"> 34454.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI325X_2xEPYC_9655/llama2-70b-99.9/Server"> 31671.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MangoBoost/results/8xMI325X_2xEPYC_9655/llama2-70b-99.9/Offline"> 34454.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0068 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9755
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 128
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/MiTAC/systems/8xMI325X_2xEPYC_9755.json"> G8825Z5 </a> </td>
        <td class="col-submitter headcol"> MiTAC </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99/Server"> 31755.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99/Interactive"> 18846.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99/Offline"> 33703.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99.9/Server"> 31755.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99.9/Interactive"> 18846.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/MiTAC/results/8xMI325X_2xEPYC_9755/llama2-70b-99.9/Offline"> 33703.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0069 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/NVIDIA/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99123.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Interactive"> 59545.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 101527.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99123.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 59545.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101527.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 31.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0070 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 1
Notes: GB200 TGP 1200W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/NVIDIA/systems/GB200-NVL72_GB200-186GB_aarch64x4_TRT.json">  (4x GB200-186GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99/Server"> 49359.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99/Interactive"> 29745.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99/Offline"> 51736.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99.9/Server"> 49359.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99.9/Interactive"> 29745.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/NVIDIA/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99.9/Offline"> 51736.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0071 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 18
Notes: GB200 TGP 1200W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/NVIDIA/systems/GB200-NVL72_GB200-186GB_aarch64x72_TRT.json">  (72x GB200-186GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0072 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.13, CUDA 13.0
Cores per processor: 144
Processors per node: 2
Nodes: 18
Notes: GB300 TGP 1400W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/NVIDIA/systems/GB300-NVL72_GB300-288GB_aarch64x72_TRT.json">  (72x GB300-288GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> NVIDIA </td>
        <td class="col-accelerator headcol"> NVIDIA GB300 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0073 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8580
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 60
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Nebius/systems/B200-SXM-180GBx8_TRT.json"> Nebius B200 (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Nebius </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 101611.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Interactive"> 59622.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 101246.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 101611.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 59622.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101246.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0074 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 1
Notes: NVIDIA GB200 NVL72
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Nebius/systems/GB200-NVL72_GB200-186GB_aarch64x4_TRT.json"> Nebius GB200 (4x GB200-186GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Nebius </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99/Server"> 49215.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99/Offline"> 49683.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99.9/Server"> 49215.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/GB200-NVL72_GB200-186GB_aarch64x4_TRT/llama2-70b-99.9/Offline"> 49683.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0075 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: H200 TGP 700W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Nebius/systems/H200-SXM-141GBx8_TRT.json"> Nebius H200 (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Nebius </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 34029.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Interactive"> 23079.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 34812.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 34029.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Interactive"> 23079.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Nebius/results/H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34812.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0076 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Oracle/systems/B200-SXM-180GBx8_TRT.json"> BM.GPU.B200.8 </a> </td>
        <td class="col-submitter headcol"> Oracle </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0077 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 144
Processors per node: 2
Nodes: 1
Notes: NVIDIA GB200 NVL72
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Oracle/systems/GB200-NVL72_GB200-186GB_aarch64x4_TRT.json"> BM.GPU.GB200.4 </a> </td>
        <td class="col-submitter headcol"> Oracle </td>
        <td class="col-accelerator headcol"> NVIDIA GB200 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0078 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6787P
Software: PyTorch
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D75E-4U. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Quanta_Cloud_Technology/systems/1-node-2S-GNR_86C.json"> 1-node-2S-GNR_86C </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Server"> 11801.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/dlrm-v2-99.9/Offline"> 12560.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/retinanet/Server"> 365.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/1-node-2S-GNR_86C/retinanet/Offline"> 458.8 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0079 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8480+
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D74H-7U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Quanta_Cloud_Technology/systems/D74U-7U_H200-SXM-141GBx8_TRT.json"> QuantaGrid D74H-7U (8x H200-SXM-141GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA H200-SXM-141GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/llama2-70b-99/Server"> 33356.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/llama2-70b-99/Offline"> 35091.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Server"> 33352.6 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/llama2-70b-99.9/Offline"> 34876.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Server"> 18.4 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/stable-diffusion-xl/Offline"> 19.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/dlrm-v2-99/Server"> 585168.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/dlrm-v2-99/Offline"> 647861.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Server"> 370068.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/dlrm-v2-99.9/Offline"> 389330.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/retinanet/Server"> 13603.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D74U-7U_H200-SXM-141GBx8_TRT/retinanet/Offline"> 14539.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0080 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 6787P
Software: TensorRT 10.8, CUDA 12.8
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D75E-4U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Quanta_Cloud_Technology/systems/D75E-4U_H200-NVL-141GBx4_TRT.json"> D75E-4U_H200-NVL-141GBx4 </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> NVIDIA H200-NVL-141GB x 4 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99/Server"> 13736.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99/Interactive"> 6709.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99/Offline"> 15057.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99.9/Server"> 13730.7 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99.9/Interactive"> 6713.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/llama2-70b-99.9/Offline"> 15167.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/stable-diffusion-xl/Server"> 7.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/stable-diffusion-xl/Offline"> 8.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/dlrm-v2-99/Server"> 202030.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/dlrm-v2-99/Offline"> 263090.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/dlrm-v2-99.9/Server"> 154025.0 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/dlrm-v2-99.9/Offline"> 159060.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/retinanet/Server"> 6600.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75E-4U_H200-NVL-141GBx4_TRT/retinanet/Offline"> 6619.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0081 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9755
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: QuantaGrid D75T-7U
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Quanta_Cloud_Technology/systems/D75T-7U_8xMI325X.json"> D75T-7U_8xMI325X </a> </td>
        <td class="col-submitter headcol"> Quanta_Cloud_Technology </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99/Server"> 31401.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99/Interactive"> 17633.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99/Offline"> 32760.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99.9/Server"> 31401.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99.9/Interactive"> 17633.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Quanta_Cloud_Technology/results/D75T-7U_8xMI325X/llama2-70b-99.9/Offline"> 32760.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0082 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon Processor (SapphireRapids)
Software: vLLM 0.10.0, RHEL 9.6
Cores per processor: 48
Processors per node: 2
Nodes: 1
Notes: IBM Cloud gx3 instance with 2xL40S-PCIE-48G (1 GPU used). IBM Cloud instance gx3-48x240x2l40s ( 1 GPU used), RHEL 9.6, vllm 0.10.0
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/RedHat/systems/IBM_Cloud_gx3-48x240x2l40s_vLLM.json"> IBM Cloud gx3 instance 1xL40S-PCIE-48GB ,vLLM </a> </td>
        <td class="col-submitter headcol"> RedHat </td>
        <td class="col-accelerator headcol"> NVIDIA L40S x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0083 </td>
        <td class="col-system headcol" title="
Processor: Intel Xeon 6448Y 2.1GHz(2x 32 cores each)
Software: vLLM 0.10.0, RHEL 9.6
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: Dell PowerEdge XE8640 4xH100-SXM-80GB system (1 GPU used). OCP version: 4.18.16, OCP virt version: 4.18.11,VM OS: RHEL 9.6, vllm 0.10.0
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/RedHat/systems/XE8640_4xH100_vLLM.json"> Dell PowerEdge XE8640 (1xH100-SXM-80GB, vLLM) </a> </td>
        <td class="col-submitter headcol"> RedHat </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0084 </td>
        <td class="col-system headcol" title="
Processor: INTEL(R) XEON(R) 6980P
Software: PyTorch: Retinanet: 1.12.0a0+gitb278502, Whisper: 2.7.0+cpu, dlrmv2: 2.1.1+cpu, rgat: 2.7.0+cpu, llama3.1-8b: 2.8.0.dev20250627+cpu
Cores per processor: 128
Processors per node: 2
Nodes: 1
Notes: SYS-822GA-NGR3. N/A
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/1-node-2S-GNR_128C.json"> 1-node-2S-GNR_128C </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol">  </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Server"> 18704.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: int8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/dlrm-v2-99.9/Offline"> 18928.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/retinanet/Server"> 579.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: INT8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/1-node-2S-GNR_128C/retinanet/Offline"> 716.7 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0085 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: Mango LLMBoost AI Enterprise Platform, ROCm 6.12.12
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/8xMI325X_2xEPYC_9575F.json"> AS-8126GS-TNMR (8x MI325X) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Server"> 31646.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99/Offline"> 33468.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server"> 31646.2 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/8xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 33468.4 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0086 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/AS-4126GS-NBR-LCC_B200-SXM-180GBx8_TRT.json"> AS-4126GS-NBR-LCC (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/AS-4126GS-NBR-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99186.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/AS-4126GS-NBR-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 101516.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/AS-4126GS-NBR-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99182.3 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/AS-4126GS-NBR-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101865.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0087 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6960P
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 72
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/SYS-422GA-NBRT-LCC_B200-SXM-180GBx8_TRT.json"> SYS-422GA-NBRT-LCC (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GA-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99195.8 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GA-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 60140.3 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GA-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101859.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0088 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) 6787P
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 86
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT.json"> SYS-422GS-NBRT-LCC (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99/Server"> 99181.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99/Interactive"> 60131.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99/Offline"> 102498.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99179.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 60119.9 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-422GS-NBRT-LCC_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101696.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0089 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro/systems/SYS-A21GE-NBRT_B200-SXM-180GBx8_TRT.json"> SYS-A21GE-NBRT (8x B200-SXM-180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> Supermicro </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-A21GE-NBRT_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Server"> 99128.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-A21GE-NBRT_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Interactive"> 57418.0 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp4" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro/results/SYS-A21GE-NBRT_B200-SXM-180GBx8_TRT/llama2-70b-99.9/Offline"> 101359.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0090 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: Mango LLMBoost AI Enterprise Platform, ROCm 6.12.12
Cores per processor: 64
Processors per node: 2
Nodes: 2
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro_MangoBoost/systems/16xMI325X_2xEPYC_9575F.json"> Supermicro AS-8126GS-TNMR (2x8x MI325X-256GB MangoBoost-LLMBoost) </a> </td>
        <td class="col-submitter headcol"> Supermicro_MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_2xEPYC_9575F/llama2-70b-99/Server"> 57391.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_2xEPYC_9575F/llama2-70b-99/Offline"> 65320.1 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_2xEPYC_9575F/llama2-70b-99.9/Server"> 57391.8 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 65320.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0091 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F (MI325X), AMD EPYC 9654 (MI300X)
Software: Mango LLMBoost AI Enterprise Platform, ROCm 6.12.12
Cores per processor: 64
Processors per node: 2
Nodes: 3
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Supermicro_MangoBoost/systems/16xMI325X_8xMI300X_2xEPYC_9575F.json"> Supermicro AS-8126GS-TNMR (2x8x MI325X-256GB MangoBoost-LLMBoost) and Supermicro AS-8125GS-TNMR2 (1x8x MI300X-192GB MangoBoost-LLMBoost) </a> </td>
        <td class="col-submitter headcol"> Supermicro_MangoBoost </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3E (x16), AMD Instinct MI300X 192GB HBM3 (x8) x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_8xMI300X_2xEPYC_9575F/llama2-70b-99/Server"> 80594.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_8xMI300X_2xEPYC_9575F/llama2-70b-99/Offline"> 92158.2 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_8xMI300X_2xEPYC_9575F/llama2-70b-99.9/Server"> 80594.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Supermicro_MangoBoost/results/16xMI325X_8xMI300X_2xEPYC_9575F/llama2-70b-99.9/Offline"> 92158.2 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0092 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8468
Software: TheStageAI
Cores per processor: 128
Processors per node: 2
Nodes: 1
Notes: Powered by the KRAI X automation technology
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/The_Stage/systems/h100_x8.json"> Nebius H100 (8x H100-SXM-80GB, TheStageAI) </a> </td>
        <td class="col-submitter headcol"> The_Stage </td>
        <td class="col-accelerator headcol"> NVIDIA H100-SXM-80GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/The_Stage/results/h100_x8/stable-diffusion-xl/server"> 17.9 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/The_Stage/results/h100_x8/stable-diffusion-xl/offline"> 18.1 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0093 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000 W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/University-of-Florida/systems/B200-SXM-180GBx1_TRT.json"> NVIDIA DGX B200 (1x B200‑SXM‑180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> University-of-Florida </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 1 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: clip1:fp32,clip2:fp32,unet:fp8,vae:fp32" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/University-of-Florida/results/B200-SXM-180GBx1_TRT/stable-diffusion-xl/Server"> 3.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: clip1:fp32,clip2:fp32,unet:fp8,vae:fp32" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/University-of-Florida/results/B200-SXM-180GBx1_TRT/stable-diffusion-xl/Offline"> 4.0 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0094 </td>
        <td class="col-system headcol" title="
Processor: Intel(R) Xeon(R) Platinum 8570
Software: TensorRT 10.11, CUDA 12.9
Cores per processor: 56
Processors per node: 2
Nodes: 1
Notes: B200 TGP 1000 W
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/University-of-Florida/systems/B200-SXM-180GBx8_TRT.json"> NVIDIA DGX B200 (8x B200‑SXM‑180GB, TensorRT) </a> </td>
        <td class="col-submitter headcol"> University-of-Florida </td>
        <td class="col-accelerator headcol"> NVIDIA B200-SXM-180GB x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

            <td class="col-result"><a target="_blank" title="Model precision: clip1:fp32,clip2:fp32,unet:fp8,vae:fp32" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/University-of-Florida/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Server"> 28.5 </a> </td>
        <td class="col-result"></td>
            <td class="col-result"><a target="_blank" title="Model precision: clip1:fp32,clip2:fp32,unet:fp8,vae:fp32" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/University-of-Florida/results/B200-SXM-180GBx8_TRT/stable-diffusion-xl/Offline"> 31.9 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0095 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Vultr/systems/8xMI325X_2xEPYC_9554.json"> Supermicro AS -8126GS-TNMR (8x MI325X) </a> </td>
        <td class="col-submitter headcol"> Vultr </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99/Server"> 30339.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99/Interactive"> 17709.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99/Offline"> 33762.5 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99.9/Server"> 30339.4 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99.9/Interactive"> 17709.6 </a> </td>

            <td class="col-result"><a target="_blank" title="Model precision: fp8" href="https://github.com/mlcommons/inference_results_v5.1/tree/mlcommons/closed/Vultr/results/8xMI325X_2xEPYC_9554/llama2-70b-99.9/Offline"> 33762.5 </a> </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>

        <tr>
        <td class="col-id headcol"> 5.1-0096 </td>
        <td class="col-system headcol" title="
Processor: AMD EPYC 9575F
Software: vLLM 0.6.5.dev964+mlperf50, Pytorch 2.7.0a0+git3a58512, ROCm 6.3.1
Cores per processor: 64
Processors per node: 2
Nodes: 1
Notes: 
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/Vultr/systems/8xMI325X_2xEPYC_9575F.json"> Supermicro AS -8126GS-TNMR (8x MI325X) </a> </td>
        <td class="col-submitter headcol"> Vultr </td>
        <td class="col-accelerator headcol"> AMD Instinct MI325X 256GB HBM3e x 8 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

        <h2 id="results_heading_preview" class="results_table_heading">Datacenter Category: Preview submissions in Closed division</h2>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<div id="results_table_preview" class="resultstable_wrapper"> <table class="resultstable tablesorter tableclosed tabledatacenter" id="results_preview"><thead> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></thead><tfoot> <tr>
            <th id="col-id" class="headcol col-id">ID</th>
            <th id="col-system" class="headcol col-system">System</th>
            <th id="col-submitter" class="headcol col-submitter">Submitter</th>
            <th id="col-accelerator" class="headcol col-accelerator">Accelerator</th>
            <th id="col-llama2-99" colspan="2">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9" colspan="2">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99" colspan="2">GPTJ-99</th>
            <th id="col-gptj-99.9" colspan="2">GPTJ-99.9</th>
            <th id="col-bert-99" colspan="2">Bert-99</th>
            <th id="col-bert-99.9" colspan="2">Bert-99.9</th>
            <th id="col-sdxl" colspan="2">Stable Diffusion</th>
            <th id="col-dlrm-v2-99" colspan="2">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9" colspan="2">DLRM-v2-99.9</th>
            <th id="col-retinanet" colspan="2">Retinanet</th>
            <th id="col-resnet50" colspan="2">ResNet50</th>
            <th id="col-3d-unet-99" colspan="1">3d-unet-99</th>
            <th id="col-3d-unet-99.9" colspan="1">3d-unet-99.9</th>
            </tr>
        <tr>
        <th class="headcol col-id"></th>
        <th class="headcol col-system"></th>
        <th class="headcol col-submitter"></th>
        <th class="headcol col-accelerator"></th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Server</th>
                <th class="col-scenario">Interactive</th>
                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>

                <th class="col-scenario">Offline</th>
                </tr></tfoot>
        <tr>
        <td class="col-id headcol"> 5.1-0097 </td>
        <td class="col-system headcol" title="
Processor: NVIDIA Grace CPU
Software: TensorRT 10.13, CUDA 13.0
Cores per processor: 144
Processors per node: 2
Nodes: 2
Notes: NVIDIA GB300 NVL8
        "> <a target="_blank" href="https://github.com/mlcommons/inference_systems_v5.1/tree/main/closed/CoreWeave/systems/GB300-NVL8-CoreWeave_TRT.json">  (8x GB300-288GB_aarch64, TensorRT) </a> </td>
        <td class="col-submitter headcol"> CoreWeave </td>
        <td class="col-accelerator headcol"> NVIDIA GB300 x 4 </td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                    <td></td>

                <td></td>

                <td></td>

        </tr>
        </table></div>

<!-- pager -->
<div class="pager1 PAGER_CLASS">
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/first.png" class="first"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/prev.png" class="prev"/>
            <span class="pagedisplay"></span> <!-- this can be any element, including an input -->
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/next.png" class="next"/>
            <img src="https://mottie.github.io/tablesorter/addons/pager/icons/last.png" class="last"/>
            <select class="pagesize" title="Select page size">
            <option selected="selected" value="10">10</option>
            <option value="20">20</option>
            <option value="30">30</option>
            <option value="all">All</option>
            </select>
            <select class="gotoPage" title="Select page number"></select>
</div>

<hr>

<h2 id="count_heading">Count of Results </h2>

    <div class="counttable_wrapper">
    <table class="tablesorter counttable" id="results_summary">
    <thead>
    <tr>
    <th class="count-submitter">Submitter</th>

            <th id="col-llama2-99">LLAMA2-70B-99</th>
            <th id="col-llama2-99.9">LLAMA2-70B-99.9</th>
            <th id="col-gptj-99">GPTJ-99</th>
            <th id="col-gptj-99.9">GPTJ-99.9</th>
            <th id="col-bert-99">Bert-99</th>
            <th id="col-bert-99.9">Bert-99.9</th>
            <th id="col-dlrm-v2-99">Stable Diffusion</th>
            <th id="col-dlrm-v2-99">DLRM-v2-99</th>
            <th id="col-dlrm-v2-99.9">DLRM-v2-99.9</th>
            <th id="col-retinanet">Retinanet</th>
            <th id="col-resnet50">ResNet50</th>
            <th id="col-3d-unet-99">3d-unet-99</th>
            <th id="col-3d-unet-99.9">3d-unet-99.9</th>
            <th id="all-models">Total</th>
            </tr>
            </thead>
            <tr><td class="count-submitter"> AMD </td><td class="col-result"> 7 </td><td class="col-result"> 7 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 16 </td></tr><tr><td class="count-submitter"> ASUSTeK </td><td class="col-result"> 7 </td><td class="col-result"> 7 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 26 </td></tr><tr><td class="count-submitter"> Azure </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td></tr><tr><td class="count-submitter"> Broadcom_Supermicro </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td></tr><tr><td class="count-submitter"> Cisco </td><td class="col-result"> 9 </td><td class="col-result"> 9 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 10 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 32 </td></tr><tr><td class="count-submitter"> Dell </td><td class="col-result"> 16 </td><td class="col-result"> 16 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 12 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 54 </td></tr><tr><td class="count-submitter"> Dell_Broadcom </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td></tr><tr><td class="count-submitter"> Dell_MangoBoost </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> GATEOverflow </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td></tr><tr><td class="count-submitter"> GigaComputing </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 14 </td></tr><tr><td class="count-submitter"> Google </td><td class="col-result"> 5 </td><td class="col-result"> 5 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 14 </td></tr><tr><td class="count-submitter"> HPE </td><td class="col-result"> 5 </td><td class="col-result"> 5 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 4 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 20 </td></tr><tr><td class="count-submitter"> Intel </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 10 </td></tr><tr><td class="count-submitter"> Intel_VMware </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> Lambda </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td></tr><tr><td class="count-submitter"> Lenovo </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 10 </td></tr><tr><td class="count-submitter"> MangoBoost </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 12 </td></tr><tr><td class="count-submitter"> MiTAC </td><td class="col-result"> 3 </td><td class="col-result"> 3 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td></tr><tr><td class="count-submitter"> NVIDIA </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 14 </td></tr><tr><td class="count-submitter"> Nebius </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 16 </td></tr><tr><td class="count-submitter"> Oracle </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td></tr><tr><td class="count-submitter"> Quanta_Cloud_Technology </td><td class="col-result"> 8 </td><td class="col-result"> 8 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 6 </td><td class="col-result"> 6 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 36 </td></tr><tr><td class="count-submitter"> RedHat </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td></tr><tr><td class="count-submitter"> Supermicro </td><td class="col-result"> 7 </td><td class="col-result"> 13 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 24 </td></tr><tr><td class="count-submitter"> Supermicro_MangoBoost </td><td class="col-result"> 4 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 8 </td></tr><tr><td class="count-submitter"> The_Stage </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 2 </td></tr><tr><td class="count-submitter"> University-of-Florida </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 4 </td></tr><tr><td class="count-submitter"> Vultr </td><td class="col-result"> 3 </td><td class="col-result"> 3 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 6 </td></tr><tr><td class="count-submitter"> CoreWeave </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td></tr>
    <tr>
    <td class="count-submitter">Total</td>
    <td class="col-result"> 114 </td><td class="col-result"> 116 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 38 </td><td class="col-result"> 14 </td><td class="col-result"> 26 </td><td class="col-result"> 48 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 0 </td><td class="col-result"> 356 </td></tr></table></div>
<hr>

    <div id="submittervssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>
    <div id="modelvssubmissionchartContainer" class="bgtext" style="height:370px; width:80%; margin:auto;"></div>

    <form id="resultSelectionForm" method="post" action="">
        <h3>Select Category and Division</h3>

        <div class="form-field">
            <label for="category">Category</label>
            <select id="category" name="category" class="col">
                <option value='datacenter' selected>Datacenter</option>
<option value='edge' >Edge</option>

            </select>
        </div>

        <div class="form-field">
            <label for="division">Division</label>
            <select id="division" name="division" class="col">
                <option value='closed' selected>Closed</option>
<option value='open' >Open</option>

            </select>
        </div>

        <div class="form-field">
            <label for="with_power">Power</label>
            <select id="with_power" name="with_power" class="col">
                <option value="true" >Performance and Power</option>
                <option value="false" selected>Performance</option>
            </select>
        </div>

        <div class="form-field">
            <button type="submit" name="submit" value="1" id="results_tablesorter">Submit</button>
        </div>
    </form>


<script type="text/javascript">
var sortcolumnindex = 6, perfsortorder = 1;
$('#submittervssubmissionchartContainer').hide();
$('#modelvssubmissionchartContainer').hide();
</script>

<script type="text/javascript" src="javascripts/init_tablesorter.js"></script>
<script type="text/javascript" src="javascripts/results_tablesorter.js"></script>
<script type="text/javascript" src="javascripts/chart_results.js"></script>

</html>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": ".", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.sections", "navigation.indexes", "navigation.instant", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "toc.follow"], "search": "assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
  
      <script src="assets/javascripts/bundle.92b07e13.min.js"></script>
      
    
  <script type"text/javascript">
  var resort = true, // re-apply the current sort
        callback = function() {
          // do something after the updateAll method has completed
        };

      // let the plugin know that we made a update, then the plugin will
      // automatically sort the table based on the header settings
      $("table").trigger("updateAll", [ resort, callback ]);
  </script>


  </body>
</html>