From 470a0172b5c218cb43bfee990b94e0f16397d26e Mon Sep 17 00:00:00 2001
From: MLPerf <mlperf>
Date: Wed, 1 Jan 2025 00:00:00 +0000
Subject: [PATCH] Prevent surplus tensor reshape

---
 vllm/v1/attention/backends/triton_attn.py | 6 +++---
 1 file changed, 3 insertions(+), 3 deletions(-)

diff --git a/vllm/v1/attention/backends/triton_attn.py b/vllm/v1/attention/backends/triton_attn.py
index 30da1a2ea..486d2e927 100644
--- a/vllm/v1/attention/backends/triton_attn.py
+++ b/vllm/v1/attention/backends/triton_attn.py
@@ -320,16 +320,16 @@ class TritonAttentionImpl(AttentionImpl):
             key_cache = key_cache.view(self.fp8_dtype)
             value_cache = value_cache.view(self.fp8_dtype)
             num_tokens, num_heads, head_size = query.shape
-            assert layer._q_scale == 1.0, \
-                "A non 1.0 q_scale is not currently supported."
             if not current_platform.is_rocm():
+                assert layer._q_scale == 1.0, \
+                    "A non 1.0 q_scale is not currently supported."
                 # Skip Q quantization on ROCm, since dequantizing back to
                 # f32 in the attention kernel is not supported.
                 query, _ = ops.scaled_fp8_quant(
                     query.reshape(
                         (num_tokens, num_heads * head_size)).contiguous(),
                     layer._q_scale)
-            query = query.reshape((num_tokens, num_heads, head_size))
+                query = query.reshape((num_tokens, num_heads, head_size))

         use_local_attn = \
             (self.use_irope and attn_metadata.local_attn_metadata is not None)
--
2.34.1

