
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Mixed Precision &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'onnx/tutorial_mix_precision';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="BFP16 (Block floating point) Quantization" href="bfp16.html" />
    <link rel="prev" title="Accelerate with GPUs" href="gpu_usage_guide.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Mixed Precision</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Mixed Precision</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-mixed-precision-quantization">What is Mixed Precision Quantization?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benefits-of-mixed-precision-quantization">Benefits of Mixed Precision Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-quantization-in-amd-quark-for-onnx">Mixed Precision Quantization in AMD Quark for ONNX</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-enable-mixed-precision-in-amd-quark-for-onnx">How to Enable Mixed Precision in AMD Quark for ONNX?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-based-on-sensitivity-analysis">Automatic Mixed Precision based on Sensitivity Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#license">License</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <!-- omit in toc --><section id="mixed-precision">
<h1>Mixed Precision<a class="headerlink" href="#mixed-precision" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this documentation, <strong>AMD Quark</strong> is sometimes referred to simply as <strong>“Quark”</strong> for ease of reference. When you  encounter the term “Quark” without the “AMD” prefix, it specifically refers to the AMD Quark quantizer unless otherwise stated. Please do not confuse it with other products or technologies that share the name “Quark.”</p>
</div>
<p>As the scale and complexity of AI models continue to grow, optimizing their performance and efficiency becomes a top priority. Quantizing models to mixed precision emerges as a powerful technique, allowing AI practitioners to balance computational speed, memory usage, and model accuracy. This tutorial introduces the characteristics and usage of AMD Quark for ONNX’s mixed precision.</p>
<section id="what-is-mixed-precision-quantization">
<h2>What is Mixed Precision Quantization?<a class="headerlink" href="#what-is-mixed-precision-quantization" title="Link to this heading">#</a></h2>
<p>Mixed precision quantization involves using different precision levels for different parts of a neural network, such as using 8-bit integers for some layers while retaining higher precision, for example, 16-bit or 32-bit floating point, for others. This approach leverages the fact that not all parts of a model are equally sensitive to quantization. By carefully selecting which parts of the model can tolerate lower precision, you achieve significant computational savings while minimizing the impact on model accuracy.</p>
</section>
<section id="benefits-of-mixed-precision-quantization">
<h2>Benefits of Mixed Precision Quantization<a class="headerlink" href="#benefits-of-mixed-precision-quantization" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Enhanced Efficiency</strong>: By using lower precision where possible, mixed precision quantization significantly reduces computational load and memory usage, leading to faster inference times and lower power consumption.</p></li>
<li><p><strong>Maintained Accuracy</strong>: By selectively applying higher precision to sensitive parts of the model, mixed precision quantization minimizes the accuracy loss that typically accompanies uniform quantization.</p></li>
<li><p><strong>Flexibility</strong>: Mixed precision quantization is adaptable to various types of neural networks and can be tailored to specific hardware capabilities, making it suitable for a wide range of applications.</p></li>
</ol>
</section>
<section id="mixed-precision-quantization-in-amd-quark-for-onnx">
<h2>Mixed Precision Quantization in AMD Quark for ONNX<a class="headerlink" href="#mixed-precision-quantization-in-amd-quark-for-onnx" title="Link to this heading">#</a></h2>
<p>AMD Quark for ONNX is designed to push the boundaries of what is possible with mixed precision. Here is what sets it apart:</p>
<ol class="arabic simple">
<li><p><strong>Support for All Types of Granularity</strong></p></li>
</ol>
<p>Granularity refers to the level at which precision can be controlled within a model. AMD Quark for ONNX mixed precision supports:</p>
<ul class="simple">
<li><p><strong>Element-wise Granularity</strong></p></li>
</ul>
<p>Element-wise mixed precision allows assigning different numeric precisions to activations and weights at the individual computation level. For example: INT8 Weights for efficient storage and computation and INT16 Activation to preserve dynamic range.</p>
<ul class="simple">
<li><p><strong>Layer-wise Granularity</strong></p></li>
</ul>
<p>Different layers of a neural network can have varying levels of sensitivity to quantization. Layer-wise mixed precision assigns precision levels to layers based on their sensitivity, optimizing both performance and accuracy. For example, INT16 to sensitive layers for high accuracy while INT8 to others for efficient inference.</p>
<ul class="simple">
<li><p><strong>Tensor-wise Granularity</strong></p></li>
</ul>
<p>Tensor-wise mixed precision enables assigning different precisions to individual tensors within a layer. For example, in an INT8 quantized model, specifying any sensitive tensor as INT16.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Support for Various Data Types</strong></p></li>
</ol>
<p>AMD Quark for ONNX mixed precision is not limited to a few integer data types, it supports a wide range of precisions, including but not limited to:</p>
<ul class="simple">
<li><p><strong>More Integer Data Types</strong></p></li>
</ul>
<p>Traditional INT8/UINT8 for significant memory and computation savings, INT16/UINT16 for higher precision and INT32/UINT32 for experimental usage.</p>
<ul class="simple">
<li><p><strong>Half Floating-Point Data Types</strong></p></li>
</ul>
<p>Float16 and BFloat16, the former can be used for iGPU/GPU applications, while the latter can be used for NPU deployment.</p>
<ul class="simple">
<li><p><strong>Block Floating-Point Data Types</strong></p></li>
</ul>
<p>The bit-width for shared exponents and elements can be set arbitrarily. The typical data type is BFP16.</p>
<ul class="simple">
<li><p><strong>Microexponents Data Types</strong></p></li>
</ul>
<p>Supports all the Microexponents data types, including MX4, MX6 and MX9.</p>
<ul class="simple">
<li><p><strong>Microscaling Data Types</strong></p></li>
</ul>
<p>Supports all the Microscaling data types, including MXINT8, MXFP8_E4M3, MXFP8_E5M2, MXFP6_E3M2, MXFP6_E2M3 and MXFP4.</p>
</section>
<section id="how-to-enable-mixed-precision-in-amd-quark-for-onnx">
<h2>How to Enable Mixed Precision in AMD Quark for ONNX?<a class="headerlink" href="#how-to-enable-mixed-precision-in-amd-quark-for-onnx" title="Link to this heading">#</a></h2>
<p>Here, BF16 mixed with BFP16 is used as an example to illustrate how to build configurations for mixed precision quantization.
In fact, you can mix any two other data types equally.</p>
<ul class="simple">
<li><p><strong>Element-wise</strong></p></li>
</ul>
<p>In this configuration, BFP16 is assigned to activations and BFloat16 to weights. Here the BFP16 quantization is
executed by BFPFixNeuron, whose default attributes make it work on BFP16 mode.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="p">,</span> <span class="n">VitisQuantFormat</span><span class="p">,</span> <span class="n">VitisQuantType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationConfig</span>

<span class="c1"># Build the configuration</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFloat16</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>

<span class="c1"># Create an ONNX quantizer</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Quantize the ONNX model. Users need to provide the input model path, output model path,</span>
<span class="c1"># and a data reader for calibration.</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">output_model_path</span><span class="p">,</span> <span class="n">data_reader</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also assign BFloat16 to activations while BFP16 to weights as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBloat16</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Layer-wise</strong></p></li>
</ul>
<p>This is one of the common configurations for deploying models on hardware devices, where the computationally intensive layers are quantized into BFP16 to maintain accuracy while improving computational efficiency, and the remaining layers are quantized into BFloat16.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBloat16</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBloat16</span><span class="p">,</span>
    <span class="n">include_auto_mp</span><span class="o">=</span><span class="n">true</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;AutoMixprecision&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;TargetOpType&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;ConvTranspose&quot;</span><span class="p">,</span> <span class="s2">&quot;Gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;MatMul&quot;</span><span class="p">],</span>
            <span class="s2">&quot;TargetQuantType&quot;</span><span class="p">:</span> <span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<p>At this point, there are many tensors on the precision boundary whose consumers have different precision from the producers.
Some backend compilers require that two types of quantization nodes exist simultaneously on these tensors, such as inserting
BFPFixNeuron of BFP16 and VitisQDQ pair of BF16 onto the same tensor. In this case, you can enable the <code class="docutils literal notranslate"><span class="pre">DualQuantNodes</span></code> option.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBloat16</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBloat16</span><span class="p">,</span>
    <span class="n">include_auto_mp</span><span class="o">=</span><span class="n">true</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;AutoMixprecision&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;TargetOpType&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;ConvTranspose&quot;</span><span class="p">,</span> <span class="s2">&quot;Gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;MatMul&quot;</span><span class="p">],</span>
            <span class="s2">&quot;TargetQuantType&quot;</span><span class="p">:</span> <span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
            <span class="s2">&quot;DualQuantNodes&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Tensor-wise</strong></p></li>
</ul>
<p>Certain tensors in a neural network are particularly sensitive to quantization, including weight and activation tensors. Applying
appropriate precision for these sensitive tensors can help maintain model accuracy while reaping the benefits of quantization.
Therefore, after identifying these tensors through sensitivity analysis, you can set the precision separately for these tensors.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
    <span class="n">specific_tensor_precision</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># MixedPrecisionTensor is a dictionary in which the key is data type and the value</span>
        <span class="c1"># is a list of the names of sensitive tensors.</span>
        <span class="s2">&quot;MixedPrecisionTensor&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFloat16</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;weight_tensor_name&#39;</span><span class="p">,</span> <span class="s1">&#39;activation_tensor_name&#39;</span><span class="p">],</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can also assign more data types to more tensors as needed, for example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFP</span><span class="p">,</span>
    <span class="n">specific_tensor_precision</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="c1"># MixedPrecisionTensor is a dictionary in which the key is data type and the value</span>
        <span class="c1"># is a list of the names of sensitive tensors.</span>
        <span class="s2">&quot;MixedPrecisionTensor&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QBFloat16</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;weight_tensor_name1&#39;</span><span class="p">,</span> <span class="s1">&#39;activation_tensor_name1&#39;</span><span class="p">],</span>
            <span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QInt16</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;weight_tensor_name2&#39;</span><span class="p">,</span> <span class="s1">&#39;activation_tensor_name2&#39;</span><span class="p">],</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="automatic-mixed-precision-based-on-sensitivity-analysis">
<h2>Automatic Mixed Precision based on Sensitivity Analysis<a class="headerlink" href="#automatic-mixed-precision-based-on-sensitivity-analysis" title="Link to this heading">#</a></h2>
<p>The previous examples are manually specified mixed precision, but in the practical applications automatically identifying sensitive layers and then
applying mixed precision becomes more critical.</p>
<p>AMD Quark for ONNX supports automatic mixed precision as follows:</p>
<p><strong>Step 1</strong> Sensitivity analysis. This step can involve profiling the model with a new precision settings and measuring the impact on accuracy.</p>
<p><strong>Step 2</strong> Sort layers by sensitivity. Layers that show significant accuracy degradation when quantized are deemed “sensitive” and are kept at higher
precision. Less sensitive parts can be quantized more aggressively to lower precision without significant impact on overall model performance.</p>
<p><strong>Step 3</strong> Perform mixed precision operations. Perform layer by layer until reach the accuracy target which is specified by users.</p>
<p>We provide two types of accuracy target: general L2 Norm metric and Top1 metric specific to image classification models. Here is a simple example of
how to use the L2 Norm metric to achieve automatic mixed precision:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span><span class="p">,</span> <span class="n">CalibrationMethod</span><span class="p">,</span> <span class="n">QuantType</span><span class="p">,</span> <span class="n">VitisQuantFormat</span><span class="p">,</span> <span class="n">VitisQuantType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.onnx.quantization.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationConfig</span>

<span class="c1"># Build the configuration</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">calibrate_method</span><span class="o">=</span><span class="n">CalibrationMethod</span><span class="o">.</span><span class="n">MinMax</span><span class="p">,</span>
    <span class="n">quant_format</span><span class="o">=</span><span class="n">VitisQuantFormat</span><span class="o">.</span><span class="n">QDQ</span><span class="p">,</span>
    <span class="n">activation_type</span><span class="o">=</span><span class="n">VitisQuantType</span><span class="o">.</span><span class="n">QInt16</span><span class="p">,</span>
    <span class="n">weight_type</span><span class="o">=</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>
    <span class="n">extra_options</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;AutoMixprecision&#39;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;TargetOpType&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;Conv&quot;</span><span class="p">,</span> <span class="s2">&quot;ConvTranspose&quot;</span><span class="p">,</span> <span class="s2">&quot;Gemm&quot;</span><span class="p">,</span> <span class="s2">&quot;MatMul&quot;</span><span class="p">],</span>  <span class="c1"># The operation types to perform mixed precision</span>
            <span class="s1">&#39;ActTargetQuantType&#39;</span><span class="p">:</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>  <span class="c1"># The activation input of insensitive layers will be assign to this precision</span>
            <span class="s1">&#39;WeightTargetQuantType&#39;</span><span class="p">:</span><span class="n">QuantType</span><span class="o">.</span><span class="n">QInt8</span><span class="p">,</span>  <span class="c1"># The weight input of insensitive layers will be assign to this precision</span>
            <span class="s1">&#39;OutputIndex&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># The index of outputs for evaluating accuracy indicator</span>
            <span class="s1">&#39;L2Target&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span>  <span class="c1"># If L2 is less than this value after assigning a new precision to a certain layer, the process continues</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">)</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">quant_config</span><span class="p">)</span>

<span class="c1"># Create an ONNX quantizer</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>

<span class="c1"># Quantize the ONNX model. Users need to provide the input model path, output model path,</span>
<span class="c1"># and a data reader for calibration.</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">input_model_path</span><span class="p">,</span> <span class="n">output_model_path</span><span class="p">,</span> <span class="n">data_reader</span><span class="p">)</span>
</pre></div>
</div>
<p>For a detailed example of using Top1 metric for mixed precision, refer to the <a class="reference internal" href="example_quark_onnx_mixed_precision.html"><span class="doc">Mixed Precision Example</span></a>.</p>
<!-- omit in toc --></section>
<section id="license">
<h2>License<a class="headerlink" href="#license" title="Link to this heading">#</a></h2>
<p>Copyright (C) 2025, Advanced Micro Devices, Inc. All rights reserved.
SPDX-License-Identifier: MIT</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="gpu_usage_guide.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Accelerate with GPUs</p>
      </div>
    </a>
    <a class="right-next"
       href="bfp16.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">BFP16 (Block floating point) Quantization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-mixed-precision-quantization">What is Mixed Precision Quantization?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#benefits-of-mixed-precision-quantization">Benefits of Mixed Precision Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mixed-precision-quantization-in-amd-quark-for-onnx">Mixed Precision Quantization in AMD Quark for ONNX</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-enable-mixed-precision-in-amd-quark-for-onnx">How to Enable Mixed Precision in AMD Quark for ONNX?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automatic-mixed-precision-based-on-sensitivity-analysis">Automatic Mixed Precision based on Sensitivity Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#license">License</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>