From 7309b002405d098b10b36d8b0bf750da023bfd54 Mon Sep 17 00:00:00 2001
From: MLPerf <mlperf>
Date: Fri, 25 Jul 2025 07:27:53 +0000
Subject: [PATCH] use fp16 gemm for small fp8 inputs for moe

---
 vllm/envs.py                                   | 6 ++++++
 vllm/model_executor/layers/quantization/fp8.py | 8 ++++++++
 2 files changed, 14 insertions(+)

diff --git a/vllm/envs.py b/vllm/envs.py
index 58547c452..89034550d 100644
--- a/vllm/envs.py
+++ b/vllm/envs.py
@@ -5,6 +5,7 @@ import tempfile
 from typing import TYPE_CHECKING, Any, Callable, Optional
 
 if TYPE_CHECKING:
+    VLLM_MOE_USE_FP16_GEMM_FOR_SMALL_FP8_INPUTS: bool = False
     VLLM_LLAMA2_MLPERF_SCHED: bool = False
     VLLM_LLAMA2_MLPERF_MAX_TARGET_DECODE_BATCH: int = 1536
     VLLM_LLAMA2_MLPERF_MIN_TARGET_DECODE_BATCH: int = 1536
@@ -148,6 +149,11 @@ def maybe_convert_int(value: Optional[str]) -> Optional[int]:
 
 environment_variables: dict[str, Callable[[], Any]] = {
 
+
+    # MLPERF MOE
+    "VLLM_MOE_USE_FP16_GEMM_FOR_SMALL_FP8_INPUTS":
+    lambda: bool(int(os.getenv("VLLM_MOE_USE_FP16_GEMM_FOR_SMALL_FP8_INPUTS", "0"))),
+
     # MLPERF LLAMA2
     "VLLM_LLAMA2_MLPERF_SCHED":
     lambda: bool(int(os.getenv("VLLM_LLAMA2_MLPERF_SCHED", "0"))),
diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index a1c0a2461..baf9d9418 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -37,6 +37,8 @@ from vllm.platforms import current_platform
 from vllm.utils import (aiter_2stage_moe_enabled, aiter_fp8_block_moe_enabled,
                         aiter_moe_enabled)
 
+from aiter.tuned_gemm import tgemm
+import vllm.envs as envs
 if aiter_moe_enabled():
     from aiter.fused_moe_bf16_asm import asm_moe
     if aiter_2stage_moe_enabled():
@@ -395,6 +397,9 @@ class Fp8LinearMethod(LinearMethodBase):
             # Activations not quantized for marlin.
             del layer.input_scale
 
+        if envs.VLLM_MOE_USE_FP16_GEMM_FOR_SMALL_FP8_INPUTS:
+            layer.weight_fp16 = Parameter(layer.weight.to(torch.float16).t() * layer.weight_scale, requires_grad=False)
+
     def apply(self,
               layer: torch.nn.Module,
               x: torch.Tensor,
@@ -422,6 +427,9 @@ class Fp8LinearMethod(LinearMethodBase):
                 cutlass_block_fp8_supported=self.cutlass_block_fp8_supported,
             )
 
+        if envs.VLLM_MOE_USE_FP16_GEMM_FOR_SMALL_FP8_INPUTS and x.shape[0] <= 2560:
+            return tgemm.mm(x, layer.weight_fp16, bias)
+
         return self.fp8_linear.apply(input=x,
                                      weight=layer.weight,
                                      weight_scale=layer.weight_scale,
-- 
2.43.0

