[2025-08-01 06:25:58,515 main.py:237 INFO] Detected system ID: gateoverflow_rtx4090x2
[2025-08-01 06:25:58,535 harness.py:241 INFO] The harness will load 2 plugins: ['build/plugins/NMSOptPlugin/libnmsoptplugin.so', 'build/plugins/retinanetConcatPlugin/libretinanetconcatplugin.so']
[2025-08-01 06:25:58,535 generate_conf_files.py:107 INFO] Generated measurements/ entries for gateoverflow_rtx4090x2_TRT/retinanet/Offline
[2025-08-01 06:25:58,535 harness.py:357 INFO] Using harness launch command...
[2025-08-01 06:25:58,535 __init__.py:46 INFO] Running command: ./build/bin/harness_default --plugins="build/plugins/NMSOptPlugin/libnmsoptplugin.so,build/plugins/retinanetConcatPlugin/libretinanetconcatplugin.so" --logfile_outdir="/home/ubuntu/MLC/repos/local/cache/get-mlperf-inference-results-dir_94a235cb/valid_results/764987c5c33d-nvidia_original-gpu-tensorrt-vdefault-default_config/retinanet/offline/accuracy" --logfile_prefix="mlperf_log_" --performance_sample_count=64 --test_mode="AccuracyOnly" --gpu_copy_streams=2 --gpu_inference_streams=2 --use_deque_limit=true --gpu_batch_size=2 --map_path="data_maps/open-images-v6-mlperf/val_map.txt" --mlperf_conf_path="/home/ubuntu/MLC/repos/local/cache/get-git-repo_inference-src_08c59f8e/inference/mlperf.conf" --tensor_path="build/preprocessed_data/open-images-v6-mlperf/validation/Retinanet/int8_linear" --use_graphs=false --user_conf_path="/home/ubuntu/MLC/repos/gateoverflow@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/32b7acc013a744e9b16901ca02c918e7.conf" --gpu_engine_batch_size="2" --gpu_engines="./build/engines/gateoverflow_rtx4090x2/retinanet/Offline/retinanet-Offline-gpu-retinanet-b2-int8.lwis_k_99_MaxP.plan" --max_dlas=0 --scenario="Offline" --model="retinanet" --scenario Offline --model retinanet --response_postprocess openimageeffnms
[2025-08-01 06:25:58,535 __init__.py:53 INFO] Overriding Environment
benchmark : Benchmark.Retinanet
buffer_manager_thread_count : 0
data_dir : /home/ubuntu/MLC/repos/local/cache/get-mlperf-inference-nvidia-scratch-space_b4d38010/data
gpu_batch_size : 2
gpu_copy_streams : 2
gpu_inference_streams : 2
input_dtype : int8
input_format : linear
log_dir : /home/ubuntu/MLC/repos/local/cache/get-git-repo_mlperf-inferenc_2b08c1b6/repo/closed/NVIDIA/build/logs/2025.08.01-06.25.55
map_path : data_maps/open-images-v6-mlperf/val_map.txt
mlperf_conf_path : /home/ubuntu/MLC/repos/local/cache/get-git-repo_inference-src_08c59f8e/inference/mlperf.conf
offline_expected_qps : 1650.0
precision : int8
preprocessed_data_dir : /home/ubuntu/MLC/repos/local/cache/get-mlperf-inference-nvidia-scratch-space_b4d38010/preprocessed_data
scenario : Scenario.Offline
system : System(cpu=CPU(name='Intel(R) Xeon(R) w7-2495X', architecture=<CPUArchitecture.x86_64: AliasedName(name='x86_64', aliases=(), patterns=())>, vendor='GenuineIntel', cores_per_group=24, threads_per_core=2, n_groups=1, group_type=<GroupType.Socket: 'socket'>, numa_nodes=[[Interval(start=0, end=47)]], flags={'nonstop_tsc', 'aes', 'ibpb', 'syscall', 'pclmulqdq', 'ds_cpl', 'cdp_l3', 'pconfig', 'aperfmperf', 'ept_ad', 'rep_good', 'lm', 'hwp', 'cx8', 'mmx', 'avx512f', 'ssbd', 'pcid', 'de', 'x2apic', 'avx512vl', 'dts', 'cat_l3', 'tpr_shadow', 'md_clear', 'hwp_epp', 'pge', 'cqm_occup_llc', 'intel_pt', 'avx512bw', 'avx512_bf16', 'rdt_a', 'fma', 'arat', 'dtes64', 'avx512_vbmi2', 'nopl', 'mba', 'ss', 'la57', 'cx16', 'gfni', 'xtopology', 'split_lock_detect', 'cpuid_fault', 'amx_int8', 'vnmi', 'abm', 'wbnoinvd', 'smap', 'pku', 'ibrs_enhanced', 'bmi1', 'pts', 'fsgsbase', 'user_shstk', 'clflush', 'hwp_act_window', 'flush_l1d', 'constant_tsc', 'dca', 'tsc_deadline_timer', 'mtrr', 'avx', 'ida', 'art', 'avx512_vnni', 'fpu', 'pse36', 'smep', 'adx', 'msr', 'amx_tile', 'waitpkg', 'est', 'cqm_mbm_local', 'erms', 'pni', 'sdbg', 'pse', 'tsc', 'enqcmd', 'epb', 'ospke', 'sse4_2', 'rdtscp', 'pln', 'arch_perfmon', 'ssse3', 'invpcid', 'arch_capabilities', 'ht', 'cmov', 'ept', 'ibt', 'tsc_known_freq', 'amx_bf16', 'sse', 'smx', 'avx512_bitalg', 'avx512dq', 'avx_vnni', 'movdir64b', 'pdcm', 'acpi', 'rdseed', 'fsrm', 'tm', 'bts', 'rdpid', 'avx512ifma', 'bus_lock_detect', 'flexpriority', 'lahf_lm', 'fxsr', 'cpuid', 'vpid', 'dtherm', 'arch_lbr', 'clflushopt', 'mce', 'xgetbv1', 'xsaves', 'sse4_1', 'pae', 'clwb', 'xtpr', 'intel_ppin', 'pdpe1gb', 'pbe', 'rdrand', 'monitor', 'movbe', 'apic', 'stibp', 'tsc_adjust', 'cat_l2', 'vaes', 'pebs', 'umip', 'pat', 'tsxldtrk', 'avx512vbmi', 'cdp_l2', 'xsave', 'vpclmulqdq', 'hwp_pkg_req', 'serialize', '3dnowprefetch', 'cqm', 'cqm_mbm_total', 'bmi2', 'sse2', 'mca', 'nx', 'vmx', 'popcnt', 'cqm_llc', 'sep', 'f16c', 'ibrs', 'xsaveopt', 'cldemote', 'xsavec', 'movdiri', 'tm2', 'vme', 'sha_ni', 'avx2', 'avx512cd', 'avx512_vpopcntdq', 'avx512_fp16'}, vulnerabilities={'spec store bypass': 'Mitigation; Speculative Store Bypass disabled via prctl', 'spectre v1': 'Mitigation; usercopy/swapgs barriers and __user pointer sanitization', 'spectre v2': 'Mitigation; Enhanced / Automatic IBRS; IBPB conditional; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S'}), host_memory=HostMemory(capacity=Memory(quantity=197.334716, byte_suffix=<ByteSuffix.GB: (1000, 3)>, _num_bytes=197334716000)), accelerators={<class 'nvmitten.nvidia.accelerator.GPU'>: [GPU(name='NVIDIA GeForce RTX 4090', pci_id='0x268410DE', compute_sm=ComputeSM(major=8, minor=9), vram=Memory(quantity=23.5164794921875, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25250627584), max_power_limit=450.0, is_integrated=False, gpu_index=0), GPU(name='NVIDIA GeForce RTX 4090', pci_id='0x268410DE', compute_sm=ComputeSM(major=8, minor=9), vram=Memory(quantity=23.5135498046875, byte_suffix=<ByteSuffix.GiB: (1024, 3)>, _num_bytes=25247481856), max_power_limit=500.0, is_integrated=False, gpu_index=1)], <class 'nvmitten.nvidia.accelerator.DLA'>: []}, extras={'id': 'gateoverflow_rtx4090x2', 'tags': {'is_ada', 'multi_gpu', 'is_ampere', 'gpu_based', 'custom'}, 'name': 'gateoverflow_rtx4090x2', 'primary_compute_sm': ComputeSM(major=8, minor=9)})
tensor_path : build/preprocessed_data/open-images-v6-mlperf/validation/Retinanet/int8_linear
test_mode : AccuracyOnly
use_deque_limit : True
use_graphs : False
user_conf_path : /home/ubuntu/MLC/repos/gateoverflow@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/32b7acc013a744e9b16901ca02c918e7.conf
system_id : gateoverflow_rtx4090x2
config_name : gateoverflow_rtx4090x2_retinanet_Offline
workload_setting : WorkloadSetting(HarnessType.LWIS, AccuracyTarget.k_99, PowerSetting.MaxP)
optimization_level : plugin-enabled
num_profiles : 2
config_ver : lwis_k_99_MaxP
accuracy_level : 99%
inference_server : lwis
skip_file_checks : False
power_limit : None
cpu_freq : None
gpu_engine_batch_size : 2
&&&& RUNNING Default_Harness # ./build/bin/harness_default
[I] mlperf.conf path: /home/ubuntu/MLC/repos/local/cache/get-git-repo_inference-src_08c59f8e/inference/mlperf.conf
[I] user.conf path: /home/ubuntu/MLC/repos/gateoverflow@mlperf-automations/script/generate-mlperf-inference-user-conf/tmp/32b7acc013a744e9b16901ca02c918e7.conf
Creating QSL.
Finished Creating QSL.
Setting up SUT.
[I] [TRT] Loaded engine size: 77 MiB
[I] Device:0.GPU: [0] ./build/engines/gateoverflow_rtx4090x2/retinanet/Offline/retinanet-Offline-gpu-retinanet-b2-int8.lwis_k_99_MaxP.plan has been successfully loaded.
[I] [TRT] Loaded engine size: 77 MiB
[W] [TRT] Using an engine plan file across different models of devices is not recommended and is likely to affect performance or even cause errors.
[I] Device:1.GPU: [0] ./build/engines/gateoverflow_rtx4090x2/retinanet/Offline/retinanet-Offline-gpu-retinanet-b2-int8.lwis_k_99_MaxP.plan has been successfully loaded.
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1528, now: CPU 1, GPU 1671 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +1527, now: CPU 2, GPU 3198 (MiB)
[I] [TRT] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +1528, now: CPU 2, GPU 4726 (MiB)
[I] [TRT] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +1, GPU +1528, now: CPU 3, GPU 6254 (MiB)
[I] [TRT] Switching optimization profile from: 0 to 1. Please ensure there are no enqueued operations pending in this context prior to switching profiles
[I] Creating batcher thread: 0 EnableBatcherThreadPerDevice: false
Finished setting up SUT.
Starting warmup. Running for a minimum of 5 seconds.
Finished warmup. Ran for 5.10221s.
Starting running actual test.

No warnings encountered during test.

No errors encountered during test.
Finished running actual test.
Device Device:0.GPU processed:
  6344 batches of size 2
  Memcpy Calls: 0
  PerSampleCudaMemcpy Calls: 0
  BatchedCudaMemcpy Calls: 6344
Device Device:1.GPU processed:
  1 batches of size 1
  6046 batches of size 2
  Memcpy Calls: 0
  PerSampleCudaMemcpy Calls: 0
  BatchedCudaMemcpy Calls: 6047
&&&& PASSED Default_Harness # ./build/bin/harness_default
[2025-08-01 06:28:13,224 run_harness.py:170 INFO] Result: Accuracy run detected.
[2025-08-01 06:28:13,225 __init__.py:46 INFO] Running command: PYTHONPATH=/home/ubuntu/MLC/repos/local/cache/get-git-repo_mlperf-inferenc_2b08c1b6/repo/closed/NVIDIA:/home/ubuntu/MLC/repos/local/cache/get-git-repo_inference-src_08c59f8e/inference/vision/classification_and_detection/python:/home/ubuntu/MLC/repos/local/cache/get-git-repo_inference-src_08c59f8e/inference/tools/submission:/home/ubuntu/MLC/repos/gateoverflow@mlperf-automations/script/get-mlperf-inference-utils:/usr/lib/python312.zip:/usr/lib/python3.12:/usr/lib/python3.12/lib-dynload:/home/ubuntu/.local/lib/python3.12/site-packages:/usr/local/lib/python3.12/dist-packages:/usr/local/lib/python3.12/dist-packages/nvfuser-0.2.23a0+6627725-py3.12-linux-x86_64.egg:/usr/local/lib/python3.12/dist-packages/lightning_thunder-0.2.0.dev0-py3.12.egg:/usr/local/lib/python3.12/dist-packages/dill-0.3.9-py3.12.egg:/usr/local/lib/python3.12/dist-packages/opt_einsum-3.4.0-py3.12.egg:/usr/local/lib/python3.12/dist-packages/lightning_utilities-0.12.0.dev0-py3.12.egg:/usr/local/lib/python3.12/dist-packages/looseversion-1.3.0-py3.12.egg:/usr/lib/python3/dist-packages python3 -S /home/ubuntu/MLC/repos/local/cache/get-git-repo_mlperf-inferenc_2b08c1b6/repo/closed/NVIDIA/build/inference/vision/classification_and_detection/tools/accuracy-openimages.py --mlperf-accuracy-file /home/ubuntu/MLC/repos/local/cache/get-mlperf-inference-results-dir_94a235cb/valid_results/764987c5c33d-nvidia_original-gpu-tensorrt-vdefault-default_config/retinanet/offline/accuracy/mlperf_log_accuracy.json --openimages-dir /home/ubuntu/MLC/repos/local/cache/get-mlperf-inference-nvidia-scratch-space_b4d38010/preprocessed_data/open-images-v6-mlperf --output-file build/retinanet-results.json
NOTE! Installing ujson may make loading annotations faster.
Loading annotations into memory...
Done (t=0.71s)
Creating index...
index created!
Loading and preparing results...
DONE (t=14.20s)
Creating index...
index created!
Running per image evaluation...
Evaluate annotation type *bbox*
DONE (t=123.86s).
Accumulating evaluation results...
DONE (t=28.00s).
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.37314
 Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.52198
 Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.40387
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.02303
 Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.12427
 Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.41246
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.41897
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.59877
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.62804
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.08253
 Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.34314
 Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.67780
mAP=37.314%
 
======================== Result summaries: ========================

