From d179b11871c5727aead5d6823d16670b4d36f777 Mon Sep 17 00:00:00 2001
From: MLPerf <mlperf>
Date: Wed, 1 Jan 2025 00:00:00 +0000
Subject: [PATCH] Add_ck_moe_2_stages_support.patch

---
 .../model_executor/layers/quantization/fp8.py | 21 ++++++++++++++-----
 1 file changed, 16 insertions(+), 5 deletions(-)

diff --git a/vllm/model_executor/layers/quantization/fp8.py b/vllm/model_executor/layers/quantization/fp8.py
index 3da19f2b6..64c5df5e4 100644
--- a/vllm/model_executor/layers/quantization/fp8.py
+++ b/vllm/model_executor/layers/quantization/fp8.py
@@ -41,6 +41,7 @@ if aiter_moe_enabled():
     from aiter.fused_moe_bf16_asm import asm_moe
     if aiter_2stage_moe_enabled():
         from aiter.fused_moe_bf16_asm import ck_moe_2stages
+        from aiter import QuantType
     if aiter_fp8_block_moe_enabled():
         from aiter.fused_moe_bf16_asm import moe_sorting_ck
         from aiter import fmoe_fp8_blockscale_g1u1
@@ -637,10 +638,10 @@ class Fp8MoEMethod(FusedMoEMethodBase):
 
                 if aiter_2stage_moe_enabled():
                     layer.w13_weight = torch.nn.Parameter(shuffle_weight(
-                        layer.w13_weight, layout=(32, 32)),
+                        layer.w13_weight, layout=(16, 16)),
                                                           requires_grad=False)
                     layer.w2_weight = torch.nn.Parameter(shuffle_weight(
-                        layer.w2_weight, layout=(32, 32)),
+                        layer.w2_weight, layout=(16, 16)),
                                                          requires_grad=False)
                 else:
                     layer.w13_weight = torch.nn.Parameter(shuffle_weight(
@@ -721,10 +722,10 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                     max_w13_scales = max_w13_scales.unsqueeze(-1)
                     w2_scales = layer.w2_weight_scale.data.unsqueeze(-1)
                     layer.w13_weight = torch.nn.Parameter(shuffle_weight(
-                        layer.w13_weight, layout=(32, 32)),
+                        layer.w13_weight, layout=(16, 16)),
                                                           requires_grad=False)
                     layer.w2_weight = torch.nn.Parameter(shuffle_weight(
-                        layer.w2_weight, layout=(32, 32)),
+                        layer.w2_weight, layout=(16, 16)),
                                                          requires_grad=False)
                 else:
                     max_w13_scales = max_w13_scales.unsqueeze(-1).unsqueeze(
@@ -790,15 +791,25 @@ class Fp8MoEMethod(FusedMoEMethodBase):
                     block_shape=self.quant_config.weight_block_size)
 
             if aiter_2stage_moe_enabled():
+                size = x.shape[0]
+                if size <= 2048:
+                    block_size = 96
+                elif size <= 12000:
+                    block_size = 192
+                else:
+                    block_size = 256
                 return ck_moe_2stages(a1=x,
                                       w1=layer.w13_weight,
                                       w2=layer.w2_weight,
                                       topk_weight=topk_weights,
+                                      quant_type=QuantType.per_Tensor,
                                       topk_ids=topk_ids,
                                       fc1_scale=layer.w13_weight_scale,
                                       fc2_scale=layer.w2_weight_scale,
                                       a1_scale=layer.w13_input_scale,
-                                      a2_scale=layer.w2_input_scale)
+                                      a2_scale=layer.w2_input_scale,
+                                      block_size=block_size,
+                                      pipe_ver = 3)
 
             return asm_moe(
                 hidden_states=x,
-- 
2.34.1

