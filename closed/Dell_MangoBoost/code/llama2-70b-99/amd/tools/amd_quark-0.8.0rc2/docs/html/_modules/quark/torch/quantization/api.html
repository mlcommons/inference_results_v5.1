
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.quantization.api &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/quantization/api';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.quantization.api</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.quantization.api</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#
&quot;&quot;&quot;Quark Quantization API for PyTorch.&quot;&quot;&quot;

import torch
import torch.nn as nn
import torch.fx
from torch.utils.data import DataLoader
from tqdm import tqdm
from typing import Dict, Any, Optional, Union, List, Tuple, Iterable, Type
from dataclasses import fields
from quark.torch.quantization.config.type import QuantizationMode, Dtype, QSchemeType
from quark.torch.quantization.model_transformation import process_model_transformation
from quark.torch.quantization.config.config import Config, QuantizationConfig, QuantizationSpec
from quark.torch.quantization.config.config_verification import init_quantization_config, verify_quantization_spec
from quark.torch.quantization.graph.processor.processor import prepare_quant_model, check_supported_model_and_config
from quark.torch.quantization.graph.processor.processor import post_quant_optimize
from quark.torch.quantization.utils import set_op_by_name, get_op_by_name
from quark.torch.quantization.nn.modules.mixin import QuantMixin
from quark.torch.quantization.tensor_quantize import FakeQuantizeBase, ScaledFakeQuantize, NonScaledFakeQuantize
from quark.torch.quantization.utils import deep_compare
from quark.shares.utils.log import ScreenLogger, log_errors
from quark.torch.algorithm.api import apply_pre_quantization_optimization, apply_advanced_quant_algo, add_algorithm_config_by_model
import logging
from quark.torch.quantization.nn.modules import QuantConv2d, QuantConvTranspose2d, QuantLinear, QuantEmbedding, QuantEmbeddingBag
from quark.torch.utils.pack import create_pack_method
import quark.torch.kernel
from transformers.feature_extraction_utils import BatchFeature

import os
from pathlib import Path

from quark.torch.quantization.debug import insert_stats_hooks, collect_quantization_statistics

__all__ = [&quot;ModelQuantizer&quot;, &quot;load_params&quot;]

logger = ScreenLogger(__name__)

QUARK_QUANT_OPS: Dict[str, Type[Union[QuantConv2d, QuantConvTranspose2d, QuantLinear, QuantEmbedding,
                                      QuantEmbeddingBag]]] = {
                                          &quot;QuantConv2d&quot;: QuantConv2d,
                                          &quot;QuantConvTranspose2d&quot;: QuantConvTranspose2d,
                                          &quot;QuantLinear&quot;: QuantLinear,
                                          &quot;QuantEmbedding&quot;: QuantEmbedding,
                                          &quot;QuantEmbeddingBag&quot;: QuantEmbeddingBag,
                                      }


<div class="viewcode-block" id="ModelQuantizer">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/api/index.html#quark.torch.quantization.api.ModelQuantizer">[docs]</a>
class ModelQuantizer:
    &quot;&quot;&quot;
    Provides an API for quantizing deep learning models using PyTorch. This class handles the configuration and processing of the model for quantization based on user-defined parameters. It is essential to ensure that the &#39;config&#39; provided has all necessary quantization parameters defined. This class assumes that the model is compatible with the quantization settings specified in &#39;config&#39;.

    Args:
        config (Config): Configuration object containing settings for quantization.

    &quot;&quot;&quot;

    def __init__(self, config: Config) -&gt; None:
        self.config = config
        self.is_all_dynamic: Optional[bool] = None
        self.is_weight_only: Optional[bool] = None
        self._is_accelerate: Optional[bool] = None
        self.init_config()

    def set_logging_level(self) -&gt; None:
        if self.config.log_severity_level == 0:
            ScreenLogger.set_shared_level(logging.DEBUG)
        elif self.config.log_severity_level == 1:
            ScreenLogger.set_shared_level(logging.INFO)
        elif self.config.log_severity_level == 2:
            ScreenLogger.set_shared_level(logging.WARNING)
        elif self.config.log_severity_level == 3:
            ScreenLogger.set_shared_level(logging.ERROR)
        else:
            ScreenLogger.set_shared_level(logging.CRITICAL)

<div class="viewcode-block" id="ModelQuantizer.quantize_model">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/api/index.html#quark.torch.quantization.api.ModelQuantizer.quantize_model">[docs]</a>
    def quantize_model(
        self,
        model: nn.Module,
        dataloader: Optional[Union[DataLoader[torch.Tensor], DataLoader[List[Dict[str, torch.Tensor]]],
                                   DataLoader[Dict[str, torch.Tensor]], DataLoader[List[BatchFeature]]]] = None
    ) -&gt; nn.Module:
        &quot;&quot;&quot;
        This function aims to quantize the given PyTorch model to optimize its performance and reduce its size. This function accepts a model and a torch dataloader. The dataloader is used to provide data necessary for calibration during the quantization process. Depending on the type of data provided (either tensors directly or structured as lists or dictionaries of tensors), the function will adapt the quantization approach accordingly.It&#39;s important that the model and dataloader are compatible in terms of the data they expect and produce. Misalignment in data handling between the model and the dataloader can lead to errors during the quantization process.

        Parameters:
            model (nn.Module): The PyTorch model to be quantized. This model should be already trained and ready for quantization.
            dataloader (Union[DataLoader[torch.Tensor], DataLoader[List[Dict[str, torch.Tensor]]], DataLoader[Dict[str, torch.Tensor]]]):
                The DataLoader providing data that the quantization process will use for calibration. This can be a simple DataLoader returning
                tensors, or a more complex structure returning either a list of dictionaries or a dictionary of tensors.

        Returns:
            nn.Module: The quantized version of the input model. This model is now optimized for inference with reduced size and potentially improved
            performance on targeted devices.

        **Examples**:

            .. code-block:: python

                # Model &amp; Data preparation
                from transformers import AutoModelForCausalLM, AutoTokenizer
                model = AutoModelForCausalLM.from_pretrained(&quot;facebook/opt-125m&quot;)
                model.eval()
                tokenizer = AutoTokenizer.from_pretrained(&quot;facebook/opt-125m&quot;)
                from quark.torch.quantization.config.config import Config
                from quark.torch.quantization.config.type import Dtype, ScaleType, RoundType, QSchemeType
                from quark.torch.quantization.observer.observer import PerGroupMinMaxObserver
                DEFAULT_UINT4_PER_GROUP_ASYM_SPEC = QuantizationSpec(dtype=Dtype.uint4,
                                                            observer_cls=PerGroupMinMaxObserver,
                                                            symmetric=False,
                                                            scale_type=ScaleType.float,
                                                            round_method=RoundType.half_even,
                                                            qscheme=QSchemeType.per_group,
                                                            ch_axis=1,
                                                            is_dynamic=False,
                                                            group_size=128)
                DEFAULT_W_UINT4_PER_GROUP_CONFIG = QuantizationConfig(weight=DEFAULT_UINT4_PER_GROUP_ASYM_SPEC)
                quant_config = Config(global_quant_config=DEFAULT_W_UINT4_PER_GROUP_CONFIG)
                from torch.utils.data import DataLoader
                text = &quot;Hello, how are you?&quot;
                tokenized_outputs = tokenizer(text, return_tensors=&quot;pt&quot;)
                calib_dataloader = DataLoader(tokenized_outputs[&#39;input_ids&#39;])

                from quark.torch import ModelQuantizer
                quantizer = ModelQuantizer(quant_config)
                quant_model = quantizer.quantize(model, calib_dataloader)

        &quot;&quot;&quot;
        logger.info(f&quot;Quantizing with the quantization configuration:\n{self.config}&quot;)

        # Step0-1: Pre quant device check
        self._check_model_device(model)

        # Step0-2: Enhance config
        self._generate_complete_config_by_model(model, dataloader)

        # Step1[optional]: Pre quant optimization
        model = self._apply_pre_quantization_optimization(model, dataloader)

        # Step2: Prepare quantization model for graph mode and eager mode
        model = self._prepare_model(model)

        # Step3[optional]: Apply Advanced quant algo such as gptq/awq ...
        model = self._apply_advanced_quant_algo(model, dataloader)

        # Step4[optional]: Do calibration
        model = self._do_calibration(model, dataloader)

        # Optionally, collect statistics on the quantization errors over the network weights/activations.
        if os.environ.get(&quot;QUARK_DEBUG&quot;, None) is not None:
            log_dir = Path(os.environ[&quot;QUARK_DEBUG&quot;])
            log_dir.mkdir(parents=True, exist_ok=True)

            stats: Dict[str, Any] = {}
            dataloader = dataloader if not self.is_all_dynamic else None

            with insert_stats_hooks(model, stats, log_dir):
                collect_quantization_statistics(model, dataloader, stats, log_dir)

        return model</div>


    def _check_model_device(self, model: nn.Module) -&gt; None:
        # using accelerate cause, device can not be cpu or disk, temporarily
        if hasattr(model, &#39;hf_device_map&#39;):
            for _, layer_device in model.hf_device_map.items():
                if layer_device == &quot;cpu&quot; or layer_device == &quot;disk&quot;:
                    raise MemoryError(
                        f&quot;Out of memory. The available GPU memory is insufficient to load the entire model. Portions of the model have been assigned to &#39;{layer_device}&#39;, &quot;
                        &quot;but Quark does not support loading models simultaneously across GPU, CPU and disk. Please consider freeing up resources or reducing memory usage.&quot;
                    )

            self._is_accelerate = True
        else:
            self._is_accelerate = False

    def _generate_complete_config_by_model(
        self, model: nn.Module, dataloader: Union[DataLoader[torch.Tensor], DataLoader[list[dict[str, torch.Tensor]]],
                                                  DataLoader[dict[str,
                                                                  torch.Tensor]], DataLoader[List[BatchFeature]], None]
    ) -&gt; None:
        &quot;&quot;&quot;
        Generates a complete configuration based on the provided model and dataloader.
        &quot;&quot;&quot;
        self.config = add_algorithm_config_by_model(model, dataloader, self.config)

    @staticmethod
<div class="viewcode-block" id="ModelQuantizer.freeze">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/api/index.html#quark.torch.quantization.api.ModelQuantizer.freeze">[docs]</a>
    def freeze(model: Union[nn.Module, torch.fx.GraphModule]) -&gt; Union[nn.Module, torch.fx.GraphModule]:
        &quot;&quot;&quot;
        Freezes the quantized model by replacing FakeQuantize modules with FreezedFakeQuantize modules.
        If Users want to export quantized model to torch_compile, please freeze model first.

        Args:
            model (nn.Module): The neural network model containing quantized layers.

        Returns:
            nn.Module: The modified model with FakeQuantize modules replaced by FreezedFakeQuantize modules.
        &quot;&quot;&quot;
        logger.info(&quot;Freeze model start.&quot;)
        # ----replace FakeQuantize to FreezedFakeQuantize --------------
        named_modules = dict(model.named_modules(remove_duplicate=False))
        for name, module in named_modules.items():
            if isinstance(module, FakeQuantizeBase):
                if module.is_dynamic:
                    # TODO: Add freeze for dynamic model
                    logger.warning(&quot;Cannot freeze dynamic quantize model for now. Keep use FakeQuantize.&quot;)
                    pass
                else:
                    freezed_quantized_module = module.to_freezed_module()
                    set_op_by_name(model, name, freezed_quantized_module)

        # ----if model is quantized in fx.graph mode--------------
        if isinstance(model, torch.fx.GraphModule):
            model = model.freeze_model()
            assert isinstance(model, torch.fx.GraphModule)
            model = post_quant_optimize(model=model, hw_constrain=True)  # TODO pass argument

        logger.info(&quot;Freeze model end.&quot;)
        return model</div>


    def _apply_pre_quantization_optimization(
        self,
        model: nn.Module,
        dataloader: Optional[Union[DataLoader[torch.Tensor], DataLoader[List[Dict[str, torch.Tensor]]],
                                   DataLoader[Dict[str, torch.Tensor]], DataLoader[List[BatchFeature]]]] = None
    ) -&gt; nn.Module:
        return apply_pre_quantization_optimization(model, self.config, dataloader=dataloader)

    def _prepare_model(self, model: nn.Module) -&gt; nn.Module:
        if self.config.quant_mode is QuantizationMode.eager_mode:
            return process_model_transformation(model, self.config)
        elif self.config.quant_mode is QuantizationMode.fx_graph_mode:
            # Quantization with torch.fx does not support some quantization config and some FX graphs.
            # This raises an error if the config / model used are not supported.
            check_supported_model_and_config(model, self.config)  # type: ignore [arg-type]

            return prepare_quant_model(model, self.config).eval()  # type: ignore [arg-type]

    def _apply_advanced_quant_algo(
        self,
        model: nn.Module,
        dataloader: Optional[Union[DataLoader[torch.Tensor], DataLoader[List[Dict[str, torch.Tensor]]],
                                   DataLoader[Dict[str, torch.Tensor]], DataLoader[List[BatchFeature]]]] = None
    ) -&gt; nn.Module:
        return apply_advanced_quant_algo(model, self.config, self._is_accelerate, dataloader)

    def _do_calibration(
        self,
        model: nn.Module,
        dataloader: Optional[Union[DataLoader[torch.Tensor], DataLoader[List[Dict[str, torch.Tensor]]],
                                   DataLoader[Dict[str, torch.Tensor]], DataLoader[List[BatchFeature]]]] = None
    ) -&gt; nn.Module:
        # just calib, turn off quantize
        if self.is_all_dynamic is True:
            logger.info(&quot;Dynamic quantization, no calibration.&quot;)
        elif self.is_weight_only is True:
            logger.info(&quot;Weight only quantization start.&quot;)
            for module in model.modules():
                if isinstance(module, ScaledFakeQuantize):
                    module.enable_observer()
                    module.disable_fake_quant()

            # Simply run through the observers to set min_val, max_val, scale and zero_point buffers for the weight and bias.
            for module in tqdm(model.modules()):
                if isinstance(module, QuantMixin):
                    if module._weight_quantizer is not None and isinstance(module._weight_quantizer, ScaledFakeQuantize) \
                            and module._weight_quantizer.scale.numel() == 1 and module._weight_quantizer.scale.item() == 1:
                        _ = module.get_quant_weight(module.weight)
                    if module._bias_quantizer is not None and isinstance(module._bias_quantizer, ScaledFakeQuantize) \
                            and module._bias_quantizer.scale.numel() == 1 and module._bias_quantizer.scale.item() == 1:
                        _ = module.get_quant_bias(module.bias)

            logger.info(&quot;Weight only quantization end.&quot;)
        else:
            logger.info(&quot;Calibration start.&quot;)
            for module in model.modules():
                if isinstance(module, ScaledFakeQuantize):
                    module.enable_observer()
                    module.disable_fake_quant()

            assert dataloader is not None
            for data in tqdm(dataloader):
                if isinstance(data, (dict, BatchFeature)):
                    with torch.no_grad():
                        model(**data)
                else:
                    with torch.no_grad():
                        model(data)
                torch.cuda.empty_cache()
            logger.info(&quot;Calibration end.&quot;)
        logger.info(&quot;Model quantization has been completed.&quot;)

        # step5[optional]: do evaluation, turn on quantize
        if (self.config.algo_config) and self.config.algo_config.name in [&#39;gptq&#39;] and hasattr(
                self.config.algo_config, &quot;static_groups&quot;
        ) and self.config.algo_config.static_groups is False:  # Dynamic group in GPTQ does not support FakeQuantize and exporting, turn off the FakeQuantize
            for module in model.modules():
                if isinstance(module, ScaledFakeQuantize):
                    module.disable_observer()
                    module.disable_fake_quant()
        else:
            for module in model.modules():
                if isinstance(module, ScaledFakeQuantize):
                    if module.is_dynamic:  # For dynamic quantization, observer should be enable and update qparam every time.
                        module.enable_observer()
                        module.enable_fake_quant()
                    else:
                        module.disable_observer()
                        module.enable_fake_quant()
                elif isinstance(module, NonScaledFakeQuantize):
                    module.enable_fake_quant()
        return model

    def init_config(self) -&gt; None:
        self.set_logging_level()  # set log level: default info
        logger.info(&quot;Configuration checking start.&quot;)
        config = self.config
        verify_quantization_spec(config)
        # TODO: Verify quant algo

        self.is_all_dynamic = True
        self.is_weight_only = True
        for field in fields(Config):
            if field.name in [&quot;global_quant_config&quot;]:
                quantization_config = getattr(config, field.name)
                is_dynamic, is_weight_only = init_quantization_config(quantization_config)
                if is_weight_only is False:
                    self.is_weight_only = is_weight_only
                if is_dynamic is False:
                    self.is_all_dynamic = False
            elif field.name in [&quot;layer_type_quant_config&quot;, &quot;layer_quant_config&quot;]:
                quantization_config_list = getattr(config, field.name)
                for quantization_config in quantization_config_list.values():
                    is_dynamic, is_weight_only = init_quantization_config(quantization_config)
                    if is_weight_only is False:
                        self.is_weight_only = is_weight_only
                    if is_dynamic is False:
                        self.is_all_dynamic = False

        config_parsing_result = &#39;&#39;
        if self.is_weight_only:
            config_parsing_result = &#39;weight only&#39;
        elif self.is_all_dynamic:
            config_parsing_result = &#39;dynamic&#39;
        else:
            config_parsing_result = &#39;static&#39;
        logger.info(
            f&quot;Configuration checking end. The configuration is effective. This is {config_parsing_result} quantization.&quot;
        )</div>



def get_name_and_info(model_info: Dict[str, Any], parent_key: str = &quot;&quot;) -&gt; Iterable[Tuple[str, Dict[str, Any]]]:
    for key, value in model_info.items():
        new_key = f&quot;{parent_key}.{key}&quot; if parent_key else key
        if isinstance(value, dict):
            if value.get(&quot;type&quot;, None) is not None and value.get(&quot;weight&quot;, None) is not None:
                yield new_key, value
            else:
                yield from get_name_and_info(value, new_key)
        else:
            continue


def from_float_and_dict(float_module: nn.Module,
                        quant_info: Dict[str, Any],
                        param_dict: Dict[str, torch.Tensor],
                        layer_name: str,
                        compressed: bool = False,
                        reorder: bool = True) -&gt; nn.Module:
    input_tensors = None
    quant_params: Dict[str, Optional[torch.Tensor]] = {}
    if quant_info.get(&quot;input_quant&quot;, None) is not None:
        input_tensors = QuantizationSpec.from_dict(quant_info[&quot;input_quant&quot;])
        quant_params[&quot;input_scale&quot;] = param_dict[layer_name + &quot;.input_scale&quot;]  # pragma: no cover
        quant_params[&quot;input_zero_point&quot;] = param_dict[layer_name + &quot;.input_zero_point&quot;]  # pragma: no cover

    output_tensors = None
    if quant_info.get(&quot;output_quant&quot;, None) is not None:
        output_tensors = QuantizationSpec.from_dict(quant_info[&quot;output_quant&quot;])
        quant_params[&quot;output_scale&quot;] = param_dict[layer_name + &quot;.output_scale&quot;]
        quant_params[&quot;output_zero_point&quot;] = param_dict[layer_name + &quot;.output_zero_point&quot;]

    weight_qspec: Optional[QuantizationSpec] = None
    weight_tensor = param_dict[quant_info.get(&quot;weight&quot;, None)]
    if quant_info.get(&quot;weight_quant&quot;, None) is not None:
        weight_qspec = QuantizationSpec.from_dict(quant_info[&quot;weight_quant&quot;])
        weight_scale = param_dict[layer_name + &quot;.weight_scale&quot;]
        weight_zero_point = param_dict[layer_name + &quot;.weight_zero_point&quot;]

        if compressed:
            assert isinstance(weight_qspec, QuantizationSpec), &quot;weight_qspec must be QuantizationSpec instance&quot;
            assert isinstance(weight_qspec.qscheme, QSchemeType), &quot;weight_qspec.qscheme must be QSchemeType instance&quot;
            assert isinstance(weight_qspec.dtype, Dtype), &quot;weight_qspec.dtype must be Dtype instance&quot;
            pack_method = create_pack_method(qscheme=weight_qspec.qscheme.value, dtype=weight_qspec.dtype.value)
            weight_tensor = pack_method.unpack(weight_tensor, reorder)
            weight_tensor = quark.torch.kernel.dequantize(  # type: ignore[attr-defined]
                weight_qspec.dtype.value, weight_tensor, weight_scale, weight_zero_point, weight_qspec.ch_axis,
                weight_qspec.group_size, weight_qspec.qscheme.value)

        quant_params[&quot;weight_scale&quot;] = weight_scale
        quant_params[&quot;weight_zero_point&quot;] = weight_zero_point

    module_config = QuantizationConfig(input_tensors=input_tensors, output_tensors=output_tensors, weight=weight_qspec)

    bias_tensor = None
    if quant_info.get(&quot;bias&quot;, None) is not None:
        bias_tensor = param_dict[quant_info.get(&quot;bias&quot;, None)]

    quant_module: nn.Module
    if quant_info[&#39;type&#39;] in QUARK_QUANT_OPS:
        quant_module = QUARK_QUANT_OPS[quant_info[&#39;type&#39;]].from_float(
            float_module,
            module_config,
            reload=True,
            weight_tensor=weight_tensor,
            bias_tensor=bias_tensor,
        )
    else:
        raise ValueError(f&quot;The type {quant_info[&#39;type&#39;]} dose not support in Quark now!&quot;)
    quant_module.load_quant_params(quant_params)
    return quant_module


@log_errors
<div class="viewcode-block" id="load_params">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/api/index.html#quark.torch.quantization.api.load_params">[docs]</a>
def load_params(model: Optional[nn.Module] = None,
                json_path: str = &quot;&quot;,
                safetensors_path: str = &quot;&quot;,
                pth_path: str = &quot;&quot;,
                quant_mode: QuantizationMode = QuantizationMode.eager_mode,
                compressed: bool = False,
                reorder: bool = True) -&gt; nn.Module:
    &quot;&quot;&quot;
    Instantiate a quantized model from saved model files, which is generated from &quot;save_params&quot; function.

    Parameters:
        model (torch.nn.Module): The original Pytorch model.
        json_path (str): The path of the saved json file. Only available for eager mode quantization.
        safetensors_path (str): The path of the saved safetensors file. Only available for eager mode quantization.
        pth_path (str): The path of the saved pth file. Only available for fx_graph mode quantization.
        quant_mode (QuantizationMode): The quantization mode. The choice includes &quot;QuantizationMode.eager_mode&quot; and &quot;QuantizationMode.fx_graph_mode&quot;. Default is &quot;QuantizationMode.eager_mode&quot;.

    Returns:
        nn.Module: The reloaded quantized version of the input model.

    **Examples**:

        .. code-block:: python

            # eager mode:
            from quark.torch import load_params
            model = load_params(model, json_path=json_path, safetensors_path=safetensors_path)

        .. code-block:: python

            # fx_graph mode:
            from quark.torch.quantization.api import load_params
            model = load_params(pth_path=model_file_path, quant_mode=QuantizationMode.fx_graph_mode)

    Note:
        This function does not support dynamic quantization for now.
    &quot;&quot;&quot;

    if quant_mode is QuantizationMode.eager_mode:
        if model is None:
            raise ValueError(&quot;Model should not be none if loading eager_mode quantized model&quot;)
        if json_path == &quot;&quot; or safetensors_path == &quot;&quot;:
            raise ValueError(&quot;Json_path and safetensors_path should not be empty if loading eager_mode quantized model&quot;)
        import json
        from safetensors.torch import load_file
        # load model structure and parameters
        with open(json_path, &quot;r&quot;) as file:
            model_dict = json.load(file)
        params_dict = load_file(safetensors_path)

        # verify exported model and float model have the same configuration
        model_config = model_dict[&quot;config&quot;]
        if model_config:
            float_model_config: Dict[str, Any] = {}
            if hasattr(model.config, &quot;to_diff_dict&quot;):
                float_model_config = model.config.to_diff_dict()
            elif hasattr(model.config, &quot;items&quot;):
                float_model_config = dict(model.config.items())

            if not deep_compare(model_config, float_model_config):
                raise RuntimeError(&quot;Exported model and float model are not the same model!&quot;)
        # assert ((json.dumps(model_config) == json.dumps(float_model_config)),
        #         &quot;Exported model and float model are not the same model!&quot;)

        logger.info(&quot;In-place OPs replacement start.&quot;)
        for name, module_info in get_name_and_info(model_dict[&quot;structure&quot;]):
            float_module = get_op_by_name(model, name)
            if module_info[&quot;type&quot;] in QUARK_QUANT_OPS:
                module = from_float_and_dict(float_module,
                                             module_info,
                                             params_dict,
                                             layer_name=name,
                                             compressed=compressed,
                                             reorder=reorder)
                set_op_by_name(model, name, module)
            else:
                device = float_module.weight.device
                float_module.weight.data = params_dict[module_info.get(&quot;weight&quot;, None)].to(device)
                if module_info.get(&quot;bias&quot;, None) is not None:
                    float_module.bias.data = params_dict[module_info.get(&quot;bias&quot;, None)].to(device)

        model = ModelQuantizer.freeze(model)
        logger.info(&quot;In-place OPs replacement end.&quot;)
    elif quant_mode is QuantizationMode.fx_graph_mode:
        if pth_path == &quot;&quot;:
            raise ValueError(&quot;Pth_path should not be empty if loading eager_mode quantized model&quot;)
        loaded_quantized_ep = torch.export.load(pth_path)
        model = loaded_quantized_ep.module()

    return model</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>