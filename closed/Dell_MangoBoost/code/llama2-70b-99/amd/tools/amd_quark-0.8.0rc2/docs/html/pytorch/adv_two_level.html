
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents) &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pytorch/adv_two_level';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Configuring ONNX Quantization" href="../onnx/user_guide_config_description.html" />
    <link rel="prev" title="Using MX (Microscaling)" href="adv_mx.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#context-uniform-integer-quantization">Context: Uniform Integer Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-level-quantization-mx6-and-mx9-data-types">Two-level Quantization: MX6 and MX9 Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-these-two-level-scales-obtained">How are These Two-Level Scales Obtained?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-mapping">Hardware Mapping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="two-level-quantization-formats-mx4-mx6-mx9-shared-microexponents">
<h1>Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)<a class="headerlink" href="#two-level-quantization-formats-mx4-mx6-mx9-shared-microexponents" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this documentation, <strong>AMD Quark</strong> is sometimes referred to simply as <strong>“Quark”</strong> for ease of reference. When you  encounter the term “Quark” without the “AMD” prefix, it specifically refers to the AMD Quark quantizer unless otherwise stated. Please do not confuse it with other products or technologies that share the name “Quark.”</p>
</div>
<p>AMD Quark supports the MX6 and MX9 quantization formats through <code class="docutils literal notranslate"><span class="pre">quark.torch</span></code>, as introduced in <a class="reference external" href="https://arxiv.org/abs/2302.08007">With Shared Microexponents, A Little Shifting Goes a Long Way</a>.</p>
<p>The novelty of these quantization formats lies in the way quantization scales are computed and stored. For a general introduction to quantization and its use in AMD Quark, refer to the <a class="reference internal" href="../intro.html"><span class="doc">Quantization with AMD Quark</span></a> documentation.</p>
<section id="context-uniform-integer-quantization">
<h2>Context: Uniform Integer Quantization<a class="headerlink" href="#context-uniform-integer-quantization" title="Link to this heading">#</a></h2>
<p>Quantization typically aims to use fewer bits per weight of a high-precision matrix <span class="math notranslate nohighlight">\(W\)</span> of shape <span class="math notranslate nohighlight">\([m, n]\)</span>, originally in float32, float16, or bfloat16 precision. A classic quantization technique is uniform integer quantization, such as INT8 quantization, which uses the following scheme:</p>
<div class="math notranslate nohighlight">
\[\begin{split}x_Q &amp;= \text{round}\left(\frac{x_R}{s}\right) \in [-128, -127,..., 127] \\
x_R &amp;= s \times x_Q \hspace{3ex}\text{(float value).}\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(s\)</span> is the <strong>scale</strong> factor, <span class="math notranslate nohighlight">\(x_Q\)</span> represents a quantized value (e.g., an int8 value), and <span class="math notranslate nohighlight">\(x_R\)</span> represents the high-precision value (typically float16, bfloat16, or float32).</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../_images/per_tensor.png"><img alt="../_images/per_tensor.png" src="../_images/per_tensor.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-text">Uniform integer per-tensor quantization.</span><a class="headerlink" href="#id3" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Such a quantization scheme necessarily leads to quantization error. To preserve model prediction quality, a strategy is to allow more granular scales. For example, instead of computing a single scale <span class="math notranslate nohighlight">\(s\)</span> for the whole matrix <span class="math notranslate nohighlight">\(W\)</span>, increase the granularity by computing one scale per column, or even one scale per group of size <span class="math notranslate nohighlight">\(k\)</span> within a column, as shown below.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../_images/block.png"><img alt="../_images/block.png" src="../_images/block.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-text">Per-block quantization, with the block size <span class="math notranslate nohighlight">\(k_1 = 6\)</span>.</span><a class="headerlink" href="#id4" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Increasing this granularity effectively means considering only a subset of values from <span class="math notranslate nohighlight">\(W\)</span> to compute the relevant scale <span class="math notranslate nohighlight">\(s\)</span> for this subset.</p>
<p>Another strategy to balance quantization error with the number of bits per weight is to use a different data type to store the scales. A common approach is to store scales as float16 or float32 values, but scales can also be constrained to be powers of two, implementing the dequantization operation <span class="math notranslate nohighlight">\(s \times x_Q\)</span> as a simple bit shift (similarly for the quantization operation). Thus, instead of storing the scale <span class="math notranslate nohighlight">\(s\)</span> on 16 or 32 bits, it can be stored on a lower bitwidth, e.g., 8 bits.</p>
</section>
<section id="two-level-quantization-mx6-and-mx9-data-types">
<h2>Two-level Quantization: MX6 and MX9 Data Types<a class="headerlink" href="#two-level-quantization-mx6-and-mx9-data-types" title="Link to this heading">#</a></h2>
<p>Refer to MX9, MX6, and MX4 specifications from <a class="reference external" href="https://arxiv.org/abs/2302.08007">[1]</a>. The MX6 and MX9 data types leverage both the granularity of the scale factors and the precision allocated to them to:</p>
<ul class="simple">
<li><p>Minimize the number of bits per weight</p></li>
<li><p>Minimize degradation in predictive performance due to quantization</p></li>
<li><p>Be hardware-friendly</p></li>
</ul>
<p>To achieve these goals, the classic quantization scheme <span class="math notranslate nohighlight">\(x_R = s \times x_Q\)</span> is decomposed into</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>x_R = s_b \times s_{sb} \times x_Q
</pre></div>
</div>
<p>where <span class="math notranslate nohighlight">\(s_b\)</span> stands for the block scale (1st level), and <span class="math notranslate nohighlight">\(s_{sb}\)</span> stands for the subblock scale (2nd level).</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../_images/mx_layout.png"><img alt="../_images/mx_layout.png" src="../_images/mx_layout.png" style="width: 75%;" />
</a>
<figcaption>
<p><span class="caption-text">A dummy example for a two-level quantization scheme, with the block size <span class="math notranslate nohighlight">\(k_1 = 6\)</span>. The different colors for <span class="math notranslate nohighlight">\(s_{sb}\)</span> indicate different values per subblock.</span><a class="headerlink" href="#id5" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>For example, in the MX9 data type, the block scale <span class="math notranslate nohighlight">\(s_b\)</span> is an 8-bit (<span class="math notranslate nohighlight">\(d_1 = 8\)</span>) power of two (within <span class="math notranslate nohighlight">\([2^{-127}, ..., 2^{127}]\)</span>) scale, shared over <span class="math notranslate nohighlight">\(k_1 = 16\)</span> values, while the subblock scale <span class="math notranslate nohighlight">\(s_{sb}\)</span> is a 1-bit (<span class="math notranslate nohighlight">\(d_2 = 1\)</span>) power of two scale (effectively, <span class="math notranslate nohighlight">\(2^{0}\)</span> or <span class="math notranslate nohighlight">\(2^{-1}\)</span>) shared over <span class="math notranslate nohighlight">\(k_2 = 2\)</span> values.</p>
<p>The mantissa bit-width <span class="math notranslate nohighlight">\(m\)</span> represents the number of bits used to store the quantized value <span class="math notranslate nohighlight">\(x_Q\)</span>, effectively using <span class="math notranslate nohighlight">\(2^m\)</span> possible different bins.</p>
<p>The total number of bits per value is</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(m + 1) + \frac{d_1}{k_1} + \frac{d_2}{k_2}
</pre></div>
</div>
<p>where <span class="math notranslate nohighlight">\(m + 1\)</span> accounts for the sign bit and the <span class="math notranslate nohighlight">\(m\)</span> bits for storing <span class="math notranslate nohighlight">\(x_Q\)</span>, and the two other terms split the storing cost of <span class="math notranslate nohighlight">\(s_b\)</span> and <span class="math notranslate nohighlight">\(s_{sb}\)</span> over the values within the block and subblock.</p>
<p>The intuition behind this quantization scheme is that while a few block scales <span class="math notranslate nohighlight">\(s_b\)</span> are stored in relatively high precision (8 bits per scale per block of 16 values), many more subscales <span class="math notranslate nohighlight">\(s_{sb}\)</span> are stored (with <span class="math notranslate nohighlight">\(k_2 = 2\)</span>, half the number of values in the matrix) to allow for lower quantization error for each floating point value in subblocks. As these subscales use a very low bitwidth (1 bit), it is a storage (and compute, as bit shifts are used) cost that can be afforded.</p>
</section>
<section id="how-are-these-two-level-scales-obtained">
<h2>How are These Two-Level Scales Obtained?<a class="headerlink" href="#how-are-these-two-level-scales-obtained" title="Link to this heading">#</a></h2>
<p>Several strategies can be chosen, as long as they respect the constraints on the scales and sub-scales. In AMD Quark, this can be found at <cite>quark/torch/kernel/hw_emulation/hw_emulation_interface.py</cite>. The scales and sub-scales are computed as follows (using MX9 as an example):</p>
<ol class="arabic simple">
<li><p>From the original float32, bfloat16, or float16 <span class="math notranslate nohighlight">\(W\)</span> matrix, retrieve the maximum power of two exponent of each block of size <span class="math notranslate nohighlight">\(k_1 = 16\)</span>, denoted <span class="math notranslate nohighlight">\(e_{b,max}\)</span>. This can be retrieved from the exponent bits from the floating point representation <span class="math notranslate nohighlight">\((-1)^s2^e \times 1.m\)</span>.</p></li>
<li><p>For each subblock of <span class="math notranslate nohighlight">\(k_2 = 2\)</span> values within the block, determine whether both floating point values have an exponent strictly smaller than <span class="math notranslate nohighlight">\(e_{b,max}\)</span>.</p>
<ul class="simple">
<li><p>If that is the case, the values within the block are comparatively small, hence a <strong>smaller scale</strong> is desired, which amounts to a smaller quantization range and finer quantization of small values. Choose <span class="math notranslate nohighlight">\(s_{sb} = 2^{-1}\)</span>.</p></li>
<li><p>If that is not the case, choose <span class="math notranslate nohighlight">\(s_{sb} = 1\)</span> (no bit shift, no subscale really applied).</p></li>
</ul>
</li>
<li><p>The block scale is chosen as <span class="math notranslate nohighlight">\(s_b = 2^{e_{b,max} - 8 + 2}\)</span>, where the <span class="math notranslate nohighlight">\(2^{-(8 - 1 - 1)}\)</span> term is an implementation detail accounting for the hidden bit of floating point numbers, and base 2 to base 10 conversion of the mantissa <span class="math notranslate nohighlight">\((1.m)_2\)</span> <a class="footnote-reference brackets" href="#id2" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>.</p></li>
</ol>
<p>Finally, the global scale for a subblock of two values is <span class="math notranslate nohighlight">\(s = s_b \times s_{sb} = 2^{e_{b,max} - 8 + 2} \times 2^{(\text{-1 or 0})}\)</span>.</p>
</section>
<section id="hardware-mapping">
<h2>Hardware Mapping<a class="headerlink" href="#hardware-mapping" title="Link to this heading">#</a></h2>
<p>Why is this quantization scheme interesting in terms of mapping it to hardware?</p>
<p>One element is that scaling can be implemented as bit shifts, both for the block scales and subblock scales, as these are stored as powers of two.</p>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Link to this heading">#</a></h2>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="id2" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>In short, for MX9: <span class="math notranslate nohighlight">\((1.m)_2 = 2^1 \times (0.m)_2 = 2^1 \times 2^{-7} \times m_{10}\)</span>, where subscripts represent the base 2 and 10.</p>
</aside>
</aside>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="adv_mx.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using MX (Microscaling)</p>
      </div>
    </a>
    <a class="right-next"
       href="../onnx/user_guide_config_description.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Configuring ONNX Quantization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#context-uniform-integer-quantization">Context: Uniform Integer Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#two-level-quantization-mx6-and-mx9-data-types">Two-level Quantization: MX6 and MX9 Data Types</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-these-two-level-scales-obtained">How are These Two-Level Scales Obtained?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hardware-mapping">Hardware Mapping</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notes">Notes</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>