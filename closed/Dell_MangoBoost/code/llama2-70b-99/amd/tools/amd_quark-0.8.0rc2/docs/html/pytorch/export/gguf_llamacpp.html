
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bridge from Quark to llama.cpp &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pytorch/export/gguf_llamacpp';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Quark Format" href="quark_export_quark.html" />
    <link rel="prev" title="GGUF Exporting" href="quark_export_gguf.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="quark_export.html">Exporting Quantized Models</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="quark_export_gguf.html">GGUF Format</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="quark_export.html" class="nav-link">Exporting Quantized Models</a></li>
    
    
    <li class="breadcrumb-item"><a href="quark_export_gguf.html" class="nav-link">GGUF Exporting</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Bridge from Quark to llama.cpp</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bridge from Quark to llama.cpp</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gguf">What Is GGUF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-quark-do-quantization">How Does Quark Do Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-gguf-export-in-quark">How to Use GGUF Export in Quark</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-quantize-your-model">Step 1: Quantize Your Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-export-to-gguf">Step 2: Export to GGUF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-run-with-llama-cpp">Step 3: Run with llama.cpp</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-it-work">How Does It Work</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="bridge-from-quark-to-llama-cpp">
<h1>Bridge from Quark to llama.cpp<a class="headerlink" href="#bridge-from-quark-to-llama-cpp" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://quark.docs.amd.com/latest/">Quark</a> is a deep learning model quantization toolkit for quantizing models from PyTorch, ONNX, and other frameworks. It provides easy-to-use APIs for quantization and more advanced features than native frameworks. Quark supports multiple hardware backends and a variety of data types with state-of-the-art quantization algorithms integrated, such as AWQ, SmoothQuant, GPTQ, and more.</p>
<p>After quantization, Quark can export the quantized model in different formats. Quark has already implemented <a class="reference internal" href="quark_export_onnx.html"><span class="doc">ONNX exporting</span></a> and <a class="reference internal" href="quark_export_quark.html"><span class="doc">Quark Format</span></a>. Now we introduce GGUF exporting in this tutorial. Thanks to this feature, you can obtain both high accuracy with Quark and high performance with GGML-based frameworks like <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>.</p>
</section>
<section id="what-is-gguf">
<h2>What Is GGUF<a class="headerlink" href="#what-is-gguf" title="Link to this heading">#</a></h2>
<p><a class="reference external" href="https://github.com/ggerganov/ggml/blob/master/docs/gguf.html">GGUF</a> is a file format that aims to store models weights for inference and also execute them based on GGML runtimes. GGUF is a binary format designed for fast loading, fast saving, and easy reading. Models are traditionally developed using PyTorch or another framework, and then converted to GGUF to be executed by <a class="reference external" href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>, a new popular inference framework aiming to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware - locally and in the cloud. Our experiments are all based on <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>.</p>
<p>The structure of the GGUF file is shown in Figure 1:</p>
<figure class="align-center" id="id1">
<img alt="GGUF file structure" src="https://github.com/ggerganov/ggml/assets/1991296/c3623641-3a1d-408e-bfaf-1b7c4e16aa63" />
<figcaption>
<p><span class="caption-text">Figure 1</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>One may think of a GGUF file as model config + PyTorch’s model state_dict. The <code class="docutils literal notranslate"><span class="pre">metadata</span></code> key-value pairs correspond to model config while the <code class="docutils literal notranslate"><span class="pre">tensors</span> <span class="pre">info</span></code> key-value pairs + tensors data correspond to model state_dict. The quantization process actually converts tensors in fp32 or fp16 to tensors in other data types with less memory usage and more computing efficiency. GGUF exporting is mainly about writing quantized tensors to the tensor part of the GGUF file in the appropriate format.</p>
</section>
<section id="how-does-quark-do-quantization">
<h2>How Does Quark Do Quantization<a class="headerlink" href="#how-does-quark-do-quantization" title="Link to this heading">#</a></h2>
<p>Quark implements quantization by inserting quantization operators before and after normal operators, as shown in Figure 2. Quantizers are quite versatile as to support several data types and quantization schemes.</p>
<figure class="align-center" id="id2">
<img alt="Quantization workflow" src="../../_images/quant_workflow.png" />
<figcaption>
<p><span class="caption-text">Figure 2</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Quantizers are stateful containing information on data types and quantization schemes, such as scale, zero_point, group size for per-group quantization, etc. Exporting is to store weights and quantizer states in some format.</p>
</section>
<section id="how-to-use-gguf-export-in-quark">
<h2>How to Use GGUF Export in Quark<a class="headerlink" href="#how-to-use-gguf-export-in-quark" title="Link to this heading">#</a></h2>
<section id="step-1-quantize-your-model">
<h3>Step 1: Quantize Your Model<a class="headerlink" href="#step-1-quantize-your-model" title="Link to this heading">#</a></h3>
<p>There’s a handy API named <code class="docutils literal notranslate"><span class="pre">ModelQuantizer</span></code> in Quark. After initializing quantization-related configs, a simple method call <code class="docutils literal notranslate"><span class="pre">quantizer.quantize_model</span></code> can get the work done.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 1. Set model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;llama2-7b&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;llama2-7b&quot;</span><span class="p">)</span>

<span class="c1"># 2. Set quantization configuration</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization.config.type</span><span class="w"> </span><span class="kn">import</span> <span class="n">Dtype</span><span class="p">,</span> <span class="n">ScaleType</span><span class="p">,</span> <span class="n">RoundType</span><span class="p">,</span> <span class="n">QSchemeType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">Config</span><span class="p">,</span> <span class="n">QuantizationSpec</span><span class="p">,</span> <span class="n">QuantizationConfig</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.quantization.observer.observer</span><span class="w"> </span><span class="kn">import</span> <span class="n">PerTensorMinMaxObserver</span>
<span class="n">DEFAULT_UINT4_PER_GROUP_ASYM_SPEC</span> <span class="o">=</span> <span class="n">QuantizationSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">Dtype</span><span class="o">.</span><span class="n">uint4</span><span class="p">,</span>
                                                    <span class="n">observer_cls</span><span class="o">=</span><span class="n">PerChannelMinMaxObserver</span><span class="p">,</span>
                                                    <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                    <span class="n">scale_type</span><span class="o">=</span><span class="n">ScaleType</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
                                                    <span class="n">round_method</span><span class="o">=</span><span class="n">RoundType</span><span class="o">.</span><span class="n">half_even</span><span class="p">,</span>
                                                    <span class="n">qscheme</span><span class="o">=</span><span class="n">QSchemeType</span><span class="o">.</span><span class="n">per_group</span><span class="p">,</span>
                                                    <span class="n">ch_axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                                    <span class="n">is_dynamic</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                                                    <span class="n">group_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="n">DEFAULT_W_UINT4_PER_GROUP_CONFIG</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span><span class="n">weight</span><span class="o">=</span><span class="n">DEFAULT_UINT4_PER_GROUP_ASYM_SPEC</span><span class="p">)</span>
<span class="n">quant_config</span> <span class="o">=</span> <span class="n">Config</span><span class="p">(</span><span class="n">global_quant_config</span><span class="o">=</span><span class="n">DEFAULT_W_UINT4_PER_GROUP_CONFIG</span><span class="p">)</span>

<span class="c1"># 3. Define calibration dataloader (still need this step for weight only and dynamic quantization)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.data</span><span class="w"> </span><span class="kn">import</span> <span class="n">DataLoader</span>
<span class="n">text</span> <span class="o">=</span> <span class="s2">&quot;Hello, how are you?&quot;</span>
<span class="n">tokenized_outputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">tokenized_outputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span>

<span class="c1"># 4. In-place replacement with quantized modules in model</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelQuantizer</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">ModelQuantizer</span><span class="p">(</span><span class="n">quant_config</span><span class="p">)</span>
<span class="n">quant_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">quantize_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">calib_dataloader</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="step-2-export-to-gguf">
<h3>Step 2: Export to GGUF<a class="headerlink" href="#step-2-export-to-gguf" title="Link to this heading">#</a></h3>
<p>There’s another easy-to-use API named <code class="docutils literal notranslate"><span class="pre">ModelExporter</span></code> to export quantized models. To export GGUF models, call <code class="docutils literal notranslate"><span class="pre">exporter.export_gguf_model</span></code></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># If you want to export the quantized model, please freeze the quantized model first</span>
<span class="n">freezed_quantized_model</span> <span class="o">=</span> <span class="n">quantizer</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">quant_model</span><span class="p">)</span>
<span class="n">export_path</span> <span class="o">=</span> <span class="s2">&quot;./output_dir&quot;</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="s2">&quot;&lt;HuggingFace model directory&gt;&quot;</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">ModelExporter</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">quark.torch.export.config.config</span><span class="w"> </span><span class="kn">import</span> <span class="n">ExporterConfig</span><span class="p">,</span> <span class="n">JsonExporterConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">ExporterConfig</span><span class="p">(</span><span class="n">json_export_config</span><span class="o">=</span><span class="n">JsonExporterConfig</span><span class="p">())</span>
<span class="n">exporter</span> <span class="o">=</span> <span class="n">ModelExporter</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">export_dir</span><span class="o">=</span><span class="n">export_path</span><span class="p">)</span>
<span class="n">exporter</span><span class="o">.</span><span class="n">export_gguf_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_dir</span><span class="p">,</span> <span class="n">model_type</span><span class="p">)</span>
</pre></div>
</div>
<p>After running the code above successfully, there will be a <code class="docutils literal notranslate"><span class="pre">.gguf</span></code> file under export_path, <code class="docutils literal notranslate"><span class="pre">./output_dir/llama.gguf</span></code> for example. You can refer to <cite>user guide &lt;quark-torch-gguf-exporting&gt;</cite> for more information.</p>
</section>
<section id="step-3-run-with-llama-cpp">
<h3>Step 3: Run with llama.cpp<a class="headerlink" href="#step-3-run-with-llama-cpp" title="Link to this heading">#</a></h3>
<p>First, follow the official <a class="reference external" href="https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#build">docs</a> to build <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>. After building successfully, there will be a few executables, such as <em>main</em> for inference, <em>perplexity</em> for evaluation, <em>quantize</em> for quantization, etc. Most of the executables take GGUF model as input. You can evaluate the exported GGUF model to get the perplexity value by running:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>perplexity<span class="w"> </span>-m<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>exported<span class="w"> </span>GGUF<span class="w"> </span>model&gt;<span class="w"> </span>-f<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>wiki.test.raw&gt;
</pre></div>
</div>
</section>
</section>
<section id="how-does-it-work">
<h2>How Does It Work<a class="headerlink" href="#how-does-it-work" title="Link to this heading">#</a></h2>
<p>As mentioned above, the export API stores weights and quantizer states into GGUF files. To export quantized models to valid GGUF models, weights and quantizer states have to be encoded into valid GGUF data types. There are some defined GGUF data types corresponding to different quantization schemes, such as <code class="docutils literal notranslate"><span class="pre">Q4_0</span></code>, <code class="docutils literal notranslate"><span class="pre">Q4_1</span></code>, <code class="docutils literal notranslate"><span class="pre">Q8_0</span></code>, <code class="docutils literal notranslate"><span class="pre">Q8_1</span></code>, etc. You can refer to <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/ggml/src/ggml-common.h">ggml-common.h</a> for more data types and their definition. Some of the GGUF dtypes and their corresponding quant schemes are shown in Table 1.</p>
<div class="pst-scrollable-table-container"><table class="table table-center" id="id3">
<caption><span class="caption-text">Some of GGUF dtypes and their corresponding quant schemes</span><a class="headerlink" href="#id3" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>GGUF dtype</p></th>
<th class="head"><p>quant scheme</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Q4_0</p></td>
<td><p>symmetric uint4 per-group quantization with group size 32</p></td>
</tr>
<tr class="row-odd"><td><p>Q4_1</p></td>
<td><p>asymmetric uint4 per-group quantization with group size 32</p></td>
</tr>
<tr class="row-even"><td><p>Q8_0</p></td>
<td><p>symmetric uint8 per-group quantization with group size 32</p></td>
</tr>
<tr class="row-odd"><td><p>Q8_1</p></td>
<td><p>asymmetric uint8 per-group quantization with group size 32</p></td>
</tr>
</tbody>
</table>
</div>
<p>As long as you find the GGUF data type that matches the quantization scheme of the quantized model in Quark, exporting to GGUF model is feasible. Thankfully, Quark supports a whole bunch of quantization schemes which match the majority of defined GGUF data types.</p>
<p>Let’s take <em>asymmetric int4 per-group</em> quantization with <em>group size 32</em> as an example, which is <code class="docutils literal notranslate"><span class="pre">Q4_1</span></code> in GGUF spec. Quantizer state for this quantization scheme are tensors for <em>weight</em>, <em>scale</em> and <em>zero_point</em> for each group. For example, for weight of shape <em>(N, 32)</em>, the shape of <em>scale</em> tensor and <em>zero_point</em> tensor are both <em>(N, 1)</em>. The definition of <code class="docutils literal notranslate"><span class="pre">Q4_1</span></code> in GGUF is as follows:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#define QK4_1 32</span>
<span class="k">typedef</span><span class="w"> </span><span class="k">struct</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">union</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">struct</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="n">ggml_half</span><span class="w"> </span><span class="n">d</span><span class="p">;</span><span class="w"> </span><span class="c1">// delta</span>
<span class="w">            </span><span class="n">ggml_half</span><span class="w"> </span><span class="n">m</span><span class="p">;</span><span class="w"> </span><span class="c1">// min</span>
<span class="w">        </span><span class="p">}</span><span class="w"> </span><span class="n">GGML_COMMON_AGGR</span><span class="p">;</span>
<span class="w">        </span><span class="n">ggml_half2</span><span class="w"> </span><span class="n">dm</span><span class="p">;</span>
<span class="w">    </span><span class="p">};</span>
<span class="w">    </span><span class="kt">uint8_t</span><span class="w"> </span><span class="n">qs</span><span class="p">[</span><span class="n">QK4_1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">2</span><span class="p">];</span><span class="w"> </span><span class="c1">// nibbles / quants</span>
<span class="p">}</span><span class="w"> </span><span class="n">block_q4_1</span><span class="p">;</span>
</pre></div>
</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">d</span></code> is scale. <code class="docutils literal notranslate"><span class="pre">m</span></code> is the minimum value of this block. According to this definition, you need to convert <em>weight</em> + <em>scale tensor</em> + <em>zero_point tensor</em> to <code class="docutils literal notranslate"><span class="pre">Q4_1</span></code> blocks. There’s one last question and we are done. In Quark, the storage is <em>weight</em> + <em>scale</em> + <em>zero_point</em>, however, in GGUF the storage is <em>weight</em> + <em>scale</em> + <em>min_val</em>. Are they equivalent to each other? The <em>quant</em> + <em>dequant</em> processes of each storage are shown in equation (1) and (2) respectively. <span class="math notranslate nohighlight">\(x\)</span> denotes float value. <span class="math notranslate nohighlight">\(\hat{x}\)</span> denotes the value after quant and dequant.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{x} &amp;= [clamp(\lfloor \frac{x}{s} \rceil + z, 0, max\_quant) - z] \times s \tag{1} \\
\hat{x} &amp;= clamp(\lfloor \frac{x - min\_val}{s} \rceil, 0, max\_quant) \times s + min\_val \tag{2} \\
\end{align}\end{split}\]</div>
<p>If you set <span class="math notranslate nohighlight">\(min\_val\)</span> to the minimum value of the block, then Equation (1) and (2) are not equivalent, because Equation (1) could guarantee that 0 is still 0 after the transformation, but Equation (2) couldn’t. Equation (2) could guarantee that the minimum value of the block will keep the same after the transformation but Equation (1) couldn’t.</p>
<p>However, if you set <span class="math notranslate nohighlight">\(min\_val\)</span> to <span class="math notranslate nohighlight">\(-s \times z\)</span>, they are equivalent. For <span class="math notranslate nohighlight">\(min\_val = -s \times z\)</span>, we get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\hat{x} &amp;= clamp(\lfloor \frac{x + s \times z}{s} \rceil, 0, max\_quant) \times s - s \times z \tag{3} \\
\hat{x} &amp;= clamp(\lfloor \frac{x}{s} + z \rceil, 0, max\_quant) \times s - s \times z \tag{4} \\
\hat{x} &amp;= clamp(\lfloor \frac{x}{s}\rceil + z, 0, max\_quant) \times s - s \times z \tag{5} \\
\hat{x} &amp;= [clamp(\lfloor \frac{x}{s} \rceil + z, 0, max\_quant) - z] \times s \tag{6} \\
\end{align}\end{split}\]</div>
<p>It’s exactly the same as Equation (1).</p>
<p>Note that the process mentioned above doesn’t involve any quantization algorithms. Quantization algorithms are agnostic to GGUF exporting, which means quantized models with ANY quantization algorithms can be exported to GGUF models. As long as the exported GGUF model matches the quant scheme involved.</p>
</section>
<section id="experiments">
<h2>Experiments<a class="headerlink" href="#experiments" title="Link to this heading">#</a></h2>
<p>The dataset used for evaluation is <code class="docutils literal notranslate"><span class="pre">wikitext2</span></code>. Download and extract the <a class="reference external" href="https://huggingface.co/datasets/ggml-org/ci/resolve/main/wikitext-2-raw-v1.zip">wikitext-2-raw-v1.zip file</a>. All the experiments are based on <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>’s commit <code class="docutils literal notranslate"><span class="pre">bdcb8f42221bc40c411150a009a3d3a30fa74722</span></code>.</p>
<p>First, use the script <a class="reference external" href="https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py">convert_hf_to_gguf.py</a> to convert HuggingFace model <code class="docutils literal notranslate"><span class="pre">Llama-2-7b</span></code> to GGUF model named <code class="docutils literal notranslate"><span class="pre">llama-2-7b-float.gguf</span></code>. Then, use the quantization feature of <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> to get a quantized model named <code class="docutils literal notranslate"><span class="pre">llama-2-7b-Q4_1.gguf</span></code> with the command</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>quantize<span class="w"> </span>Llama-2-7b-float.gguf<span class="w"> </span>Llama-2-7b-Q4_1.gguf<span class="w"> </span>Q4_1
</pre></div>
</div>
<p>Next, use Quark to quantize <code class="docutils literal notranslate"><span class="pre">Llama-2-7b</span></code> with a scheme of weight-only int4 asymmetric along with AWQ and export the quantized model to a GGUF model named <code class="docutils literal notranslate"><span class="pre">quark_exported_model.gguf</span></code>. Please refer to <a class="reference internal" href="../example_quark_torch_llm_ptq.html"><span class="doc">Language Model Post Training Quantization (PTQ) Using Quark</span></a> to get the command. Then, evaluate all the three models and get perplexities with the command below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>perplexity<span class="w"> </span>-m<span class="w"> </span>quark_exported_model.gguf<span class="w"> </span>-f<span class="w"> </span>&lt;path<span class="w"> </span>to<span class="w"> </span>wiki.test.raw&gt;
</pre></div>
</div>
<p>The results are shown in Table 2:</p>
<div class="pst-scrollable-table-container"><table class="table table-center" id="id4">
<caption><span class="caption-text">Experiment results</span><a class="headerlink" href="#id4" title="Link to this table">#</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>model</p></th>
<th class="head"><p>perplexity</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>llama-2-7b-float.gguf</p></td>
<td><p>5.7964 +/- 0.03236</p></td>
</tr>
<tr class="row-odd"><td><p>llama-2-7b-Q4_1.gguf</p></td>
<td><p>5.9994 +/- 0.03372</p></td>
</tr>
<tr class="row-even"><td><p>quark_exported_model.gguf</p></td>
<td><p>5.8952 +/- 0.03302</p></td>
</tr>
</tbody>
</table>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>There might be a discrepancy between the perplexity obtained from the GGUF model and that from Quark evaluation. There are two main reasons:</p>
<ol class="arabic simple">
<li><p>The implementation of perplexity calculation is a little different between <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> and Quark.</p></li>
<li><p>For the experiment settings above, the quantization process in Quark is a little different from that in <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>. In Quark, only weights are quantized and activations are kept in float32 without being quantized. However, in <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code>, activations are quantized to <code class="docutils literal notranslate"><span class="pre">Q8_1</span></code> implicitly when weights are in <code class="docutils literal notranslate"><span class="pre">Q4_1</span></code>.</p></li>
</ol>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You should choose quant schemes that match <code class="docutils literal notranslate"><span class="pre">llama.cpp</span></code> as much as possible.</p>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="quark_export_gguf.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">GGUF Exporting</p>
      </div>
    </a>
    <a class="right-next"
       href="quark_export_quark.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Quark Format</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gguf">What Is GGUF</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-quark-do-quantization">How Does Quark Do Quantization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-use-gguf-export-in-quark">How to Use GGUF Export in Quark</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-quantize-your-model">Step 1: Quantize Your Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-export-to-gguf">Step 2: Export to GGUF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-run-with-llama-cpp">Step 3: Run with llama.cpp</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-it-work">How Does It Work</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#experiments">Experiments</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>