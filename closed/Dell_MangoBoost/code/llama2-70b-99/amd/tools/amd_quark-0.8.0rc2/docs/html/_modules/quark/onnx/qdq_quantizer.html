
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.onnx.qdq_quantizer &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/onnx/qdq_quantizer';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.onnx.qdq_quantizer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.onnx.qdq_quantizer</h1><div class="highlight"><pre>
<span></span>#
# Modifications copyright(c) 2023 Advanced Micro Devices,Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#
# -------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
# --------------------------------------------------------------------------
import copy
import numpy as np

from quark.shares.utils.log import ScreenLogger, log_errors

import onnx
import onnx.numpy_helper
from onnx import TensorProto, ModelProto, NodeProto
from onnx import onnx_pb as onnx_proto
from onnxruntime.quantization.qdq_quantizer import QDQQuantizer as OrtQDQQuantizer
from onnxruntime.quantization.qdq_quantizer import (QDQQuantTensorType, QDQTensorQuantInfo)
from onnxruntime.quantization.quant_utils import (
    QuantType,
    QuantizationMode,
    DEQUANT_OP_NAME,
    QUANT_OP_NAME,
    QuantizedValue,
    QuantizedValueType,
    add_dequant_output_suffix,
    add_dequant_suffix,
    add_quant_input_suffix,
    add_quant_output_suffix,
    add_quant_suffix,
    find_by_name,
)

from .quant_utils import (
    VitisQuantFormat,
    __producer__,
    __version__,
    VitisQuantType,
    FIX_OP_NAME,
    FIX_OP_DEFAULT_ATTRS,
    BFPFIX_OP_NAME,
    BFPFIX_OP_DEFAULT_ATTRS,
    MXFIX_OP_NAME,
    MXFIX_OP_DEFAULT_ATTRS,
    VAI_DOMAIN,
    COP_DOMAIN,
    COP_QUANT_OP_NAME,
    COP_DEQUANT_OP_NAME,
    ONNX_WBIT_QTYPES_LIST,
    ONNX_FP_QTYPES_LIST,
    ONNX_BFP_QTYPES_LIST,
    get_annotate_tensors,
    get_qdq_to_remove,
    remove_nodes,
    modified_annotate_input,
    get_tensor_type_from_qType,
)
from .registry import (CreateQDQQuantizer, CreateNPUCnnQDQQuantizer, CreateNPUTransformerQDQQuantizer)
from .refine import adjust_quantize_info, align_quantize_info
from .simulate_dpu import simulate_transforms
from .onnx_quantizer import VitisONNXQuantizer

from typing import Any, List, Dict, Optional

logger = ScreenLogger(__name__)


<div class="viewcode-block" id="QDQQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.QDQQuantizer">[docs]</a>
class QDQQuantizer(OrtQDQQuantizer):  # type: ignore
    &quot;&quot;&quot;
    A class to perform quantization on an ONNX model using Quantize-Dequantize (QDQ) nodes.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization.
        reduce_range (bool): Whether to reduce the quantization range.
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights.
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        extra_options (Any, optional): Additional options for quantization.

    Inherits from:
        OrtQDQQuantizer: Base class for ONNX QDQ quantization.
    &quot;&quot;&quot;

    def __init__(
        self,
        model: ModelProto,
        per_channel: bool,
        reduce_range: bool,
        mode: QuantizationMode.QLinearOps,
        static: bool,
        weight_qType: Any,
        activation_qType: Any,
        tensors_range: Any,
        nodes_to_quantize: List[str],
        nodes_to_exclude: List[str],
        op_types_to_quantize: List[str],
        extra_options: Any = None,
    ):
        super().__init__(
            model,
            per_channel=per_channel,
            reduce_range=reduce_range,
            mode=mode,
            static=static,
            weight_qType=weight_qType,
            activation_qType=activation_qType,
            tensors_range=tensors_range,
            nodes_to_quantize=nodes_to_quantize,
            nodes_to_exclude=nodes_to_exclude,
            op_types_to_quantize=op_types_to_quantize,
            extra_options=extra_options,
        )
        self.int32_bias = True if extra_options is None or &quot;Int32Bias&quot; not in extra_options else extra_options[
            &quot;Int32Bias&quot;]

        # weights-only quantization switch
        self.weights_only = False if &quot;WeightsOnly&quot; not in extra_options else extra_options[&quot;WeightsOnly&quot;]

        # include-gptq quantization switch
        self.use_gptq = False if extra_options is None or &quot;UseGPTQ&quot; not in extra_options else extra_options[&quot;UseGPTQ&quot;]
        # If GPTQ is turned on, the quantizer will only quantize weights and leave the activations in floating-point for GPTQ.
        if self.use_gptq is True:
            self.weights_only = True

    def _is_tensor_quantizable(self, tensor_name: str) -&gt; bool:
        weight = find_by_name(tensor_name, self.model.initializer())
        if weight is not None:
            if weight.data_type in (onnx_proto.TensorProto.FLOAT, onnx_proto.TensorProto.FLOAT16):
                return True
        elif self.weights_only is True:
            return False
        elif tensor_name in self.value_infos:
            vi = self.value_infos[tensor_name]
            if vi.type.HasField(&quot;tensor_type&quot;) and vi.type.tensor_type.elem_type in (
                    TensorProto.FLOAT,
                    TensorProto.FLOAT16,
            ):
                return True
        else:
            logger.warning(
                f&quot;failed to infer the type of tensor: {tensor_name}. Skip to quantize it. Please check if it is expected.&quot;
            )

        return False

<div class="viewcode-block" id="QDQQuantizer.quantize_bias_tensor">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.QDQQuantizer.quantize_bias_tensor">[docs]</a>
    def quantize_bias_tensor(self, bias_name: str, input_name: str, weight_name: str, beta: float = 1.0) -&gt; None:
        weight = find_by_name(bias_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                if self.quantize_bias:
                    if self.int32_bias:
                        self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
                    else:
                        if self.per_channel:
                            self.quantize_weight_tensor_per_channel(bias_name, 0)
                        else:
                            self.quantize_weight_tensor(bias_name)
        else:
            logger.warning(f&quot;Expected {bias_name} to be a weight&quot;)</div>


    def quantize_model(self) -&gt; Any:
        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateQDQQuantizer(self, node)
                op_quantizer.quantize()

                if self.dedicated_qdq_pair:
                    for tensor_name in node.input:
                        if tensor_name not in self.tensor_to_its_receiving_nodes:
                            self.tensor_to_its_receiving_nodes[tensor_name] = []
                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)

        self._quantize_normal_tensors()
        self._quantize_sharing_param_tensors()
        if self.quantize_bias and self.int32_bias and not self.weights_only:
            self._quantize_bias_tensors()
        self.remove_nodes()

        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )

        if not self.add_qdq_pair_to_weight:
            self.model.clean_initializers()

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model

    def _add_qdq_pair_for_initializer(self, weight_proto: TensorProto, tensor_type: Any, axis: Any = None) -&gt; None:
        weight_name = weight_proto.name
        if axis is not None:
            if self.opset_version &lt; 13:
                raise ValueError(&quot;Per-Channel support with QDQ format requires onnx opset version 13 or above.&quot;)
            q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
                weight_name,
                # Quantization type is forced to be TensorProto.INT8.
                # when the expected value would be (see below)
                # self.weight_qType if tensor_type is QDQQuantTensorType.WEIGHT else self.activation_qType.
                # QLinearConv expects to have a unique value for all channels.
                # This code does not enforce that but it is necessarily the case when the
                # quantization is symmetric (as for INT8).
                onnx_proto.TensorProto.INT8,
                axis,
                keep_float_weight=self.add_qdq_pair_to_weight,
            )
        else:
            q_weight_name, zp_name, scale_name = self.quantize_initializer(
                weight_proto,
                self.weight_qType,
                keep_float_weight=self.add_qdq_pair_to_weight,
            )

        weight_dequant_output = add_dequant_output_suffix(weight_name)
        self.model.replace_input_of_all_nodes(weight_name, weight_dequant_output)
        if self.add_qdq_pair_to_weight:
            weight_quant_output = add_quant_output_suffix(weight_name)

            self._create_qdq_nodes(
                weight_name,
                weight_quant_output,
                add_quant_suffix(weight_name),
                weight_quant_output,
                weight_dequant_output,
                add_dequant_suffix(weight_name),
                scale_name,
                zp_name,
                axis,
            )
        else:
            dequant_node = onnx.helper.make_node(
                DEQUANT_OP_NAME,
                [q_weight_name, scale_name, zp_name],
                [weight_dequant_output],
                add_dequant_suffix(weight_name),
                axis=axis,
            )
            self.model.add_node(dequant_node)</div>



<div class="viewcode-block" id="QDQNPUTransformerQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer">[docs]</a>
class QDQNPUTransformerQuantizer(QDQQuantizer):
    &quot;&quot;&quot;
    A class to perform quantization on an ONNX model using Quantize-Dequantize (QDQ) nodes
    optimized for NPU (Neural Processing Unit) Transformers.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization.
        reduce_range (bool): Whether to reduce the quantization range.
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights.
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.

    Inherits from:
        QDQQuantizer: Base class for ONNX QDQ quantization.
    &quot;&quot;&quot;

    def __init__(
        self,
        model: ModelProto,
        per_channel: bool,
        reduce_range: bool,
        mode: QuantizationMode.QLinearOps,
        static: bool,
        weight_qType: Any,
        activation_qType: Any,
        tensors_range: Any,
        nodes_to_quantize: List[str],
        nodes_to_exclude: List[str],
        op_types_to_quantize: List[str],
        extra_options: Optional[Dict[str, Any]] = None,
    ):
        super().__init__(
            model,
            per_channel=per_channel,
            reduce_range=reduce_range,
            mode=mode,
            static=static,
            weight_qType=weight_qType,
            activation_qType=activation_qType,
            tensors_range=tensors_range,
            nodes_to_quantize=nodes_to_quantize,
            nodes_to_exclude=nodes_to_exclude,
            op_types_to_quantize=op_types_to_quantize,
            extra_options=extra_options,
        )
        self.int32_bias = True if extra_options is None or &quot;Int32Bias&quot; not in extra_options else extra_options[
            &quot;Int32Bias&quot;]

<div class="viewcode-block" id="QDQNPUTransformerQuantizer.quantize_bias_tensor">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer.quantize_bias_tensor">[docs]</a>
    def quantize_bias_tensor(self, bias_name: str, input_name: str, weight_name: str, beta: float = 1.0) -&gt; None:
        weight = find_by_name(bias_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                if self.quantize_bias:
                    if self.int32_bias:
                        self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
                    else:
                        if self.per_channel:
                            self.quantize_weight_tensor_per_channel(bias_name, 0)
                        else:
                            self.quantize_weight_tensor(bias_name)
        else:
            logger.warning(f&quot;Expected {bias_name} to be a weight&quot;)</div>


    def quantize_model(self) -&gt; Any:

        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateNPUTransformerQDQQuantizer(self, node)
                op_quantizer.quantize()

                if self.dedicated_qdq_pair:
                    for tensor_name in node.input:
                        if tensor_name not in self.tensor_to_its_receiving_nodes:
                            self.tensor_to_its_receiving_nodes[tensor_name] = []
                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)

        self.remove_nodes()
        self._quantize_normal_tensors()
        self._quantize_sharing_param_tensors()
        if self.quantize_bias and self.int32_bias and not self.weights_only:
            self._quantize_bias_tensors()

        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )

        if not self.add_qdq_pair_to_weight:
            self.model.clean_initializers()

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model</div>



<div class="viewcode-block" id="VitisQDQQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.VitisQDQQuantizer">[docs]</a>
class VitisQDQQuantizer(VitisONNXQuantizer):
    &quot;&quot;&quot;
    A class to perform Vitis-specific Quantize-Dequantize (QDQ) quantization on an ONNX model.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization.
        reduce_range (bool): Whether to reduce the quantization range.
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights.
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        calibrate_method (Any): The method used for calibration.
        quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
        extra_options (Any, optional): Additional options for quantization.

    Inherits from:
        VitisONNXQuantizer: Base class for Vitis-specific ONNX quantization.

    Attributes:
        tensors_to_quantize (Dict[Any, Any]): Dictionary of tensors to be quantized.
        bias_to_quantize (List[Any]): List of bias tensors to be quantized.
        nodes_to_remove (List[Any]): List of nodes to be removed during quantization.
        op_types_to_exclude_output_quantization (List[str]): List of op types to exclude from output quantization.
        quantize_bias (bool): Whether to quantize bias tensors.
        add_qdq_pair_to_weight (bool): Whether to add QDQ pairs to weights.
        dedicated_qdq_pair (bool): Whether to create dedicated QDQ pairs for each node.
        tensor_to_its_receiving_nodes (Dict[Any, Any]): Dictionary mapping tensors to their receiving nodes.
        qdq_op_type_per_channel_support_to_axis (Dict[str, int]): Dictionary mapping op types to channel axis for per-channel quantization.
        int32_bias (bool): Whether to quantize bias using int32.
        weights_only (bool): Whether to perform weights-only quantization.

    &quot;&quot;&quot;

    def __init__(
        self,
        model: ModelProto,
        per_channel: bool,
        reduce_range: bool,
        mode: QuantizationMode.QLinearOps,
        static: bool,
        weight_qType: Any,
        activation_qType: Any,
        tensors_range: Any,
        nodes_to_quantize: List[str],
        nodes_to_exclude: List[str],
        op_types_to_quantize: List[str],
        calibrate_method: Any,
        quantized_tensor_type: Dict[Any, Any] = {},
        extra_options: Any = None,
    ):
        &quot;&quot;&quot;
        Initializes the VitisQDQQuantizer with the provided configuration.

        Args:
            model (ModelProto): The ONNX model to be quantized.
            per_channel (bool): Whether to perform per-channel quantization.
            reduce_range (bool): Whether to reduce the quantization range.
            mode (QuantizationMode.QLinearOps): The quantization mode to be used.
            static (bool): Whether to use static quantization.
            weight_qType (Any): The quantization type for weights.
            activation_qType (Any): The quantization type for activations.
            tensors_range (Any): Dictionary specifying the min and max values for tensors.
            nodes_to_quantize (List[str]): List of node names to be quantized.
            nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
            op_types_to_quantize (List[str]): List of operation types to be quantized.
            calibrate_method (Any): The method used for calibration.
            quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
            extra_options (Any, optional): Additional options for quantization.
        &quot;&quot;&quot;
        self.calibrate_method = calibrate_method
        VitisONNXQuantizer.__init__(
            self,
            model,
            per_channel,
            reduce_range,
            mode,
            static,
            weight_qType,
            activation_qType,
            tensors_range,
            nodes_to_quantize,
            nodes_to_exclude,
            op_types_to_quantize,
            calibrate_method,
            quantized_tensor_type,
            extra_options,
        )
        self.tensors_to_quantize: Dict[Any, Any] = {}
        self.bias_to_quantize: List[Any] = []

        self.nodes_to_remove: List[Any] = []

        # Specific op types to exclude qdq quantization for their outputs.
        # In TRT, it&#39;s not recommended to quantize outputs for weighted ops such as Conv, Matmul, Gemm
        # because those ops may be followed by nodes that require high resolution inputs.
        # Adding QDQ for those ops&#39; output may end up with worse accuracy.
        # So, we don&#39;t recommend to add QDQ to node&#39;s output under such condition.
        self.op_types_to_exclude_output_quantization = ([] if extra_options is None
                                                        or &quot;OpTypesToExcludeOutputQuantization&quot; not in extra_options
                                                        else extra_options[&quot;OpTypesToExcludeOutputQuantization&quot;])

        # Some scenarios do not need the bias quantized. For example, in the case of Quantization Aware Training,
        # quantizing the bias is not needed. This is because in QAT, all model parameters are expected to be in
        # floating point format. To that end, we can use the FakeQuant operator for weights and activations that
        # can always have QDQ pairs (by using AddQDQPairToWeight). But for biases in a quantized model, we can&#39;t use
        # FakeQuant because it only ever appears before a DQ (since it is quantized as int32).
        self.quantize_bias = True if extra_options is None or &quot;QuantizeBias&quot; not in extra_options else extra_options[
            &quot;QuantizeBias&quot;]

        # We do quantization on Dequantizelinear&#39;s input to remove Quantizelinear for weight as an optimization.
        # In some cases, for example QDQ BERT model for TensorRT, QDQ should always appear as a pair.
        # Therefore, we need to disable this optimization and add qdq pair to weight.
        self.add_qdq_pair_to_weight = (False if extra_options is None or &quot;AddQDQPairToWeight&quot; not in extra_options else
                                       extra_options[&quot;AddQDQPairToWeight&quot;])

        # The default behavior is that multiple nodes can share a QDQ pair as their inputs.
        # In TRT, QDQ pair can&#39;t be shared between nodes, so it will create dedicated QDQ pairs for each node.
        self.dedicated_qdq_pair = (False if extra_options is None or &quot;DedicatedQDQPair&quot; not in extra_options else
                                   extra_options[&quot;DedicatedQDQPair&quot;])
        if self.dedicated_qdq_pair:
            self.tensor_to_its_receiving_nodes: Dict[Any, Any] = {}

        # Let user set channel axis for specific op type and it&#39;s effective only when per channel quantization is supported and per_channel is True.
        self.qdq_op_type_per_channel_support_to_axis = ({} if extra_options is None
                                                        or &quot;QDQOpTypePerChannelSupportToAxis&quot; not in extra_options else
                                                        extra_options[&quot;QDQOpTypePerChannelSupportToAxis&quot;])

        # We quantize Bias using Int32 by default except floating point type quantization
        if self.weight_qType in ONNX_FP_QTYPES_LIST + ONNX_BFP_QTYPES_LIST:
            self.int32_bias = False
        else:
            self.int32_bias = True
        if extra_options is not None and &quot;Int32Bias&quot; in extra_options:
            self.int32_bias = extra_options[&quot;Int32Bias&quot;]
        if self.int32_bias and (self.weight_qType in ONNX_BFP_QTYPES_LIST
                                or self.activation_qType in ONNX_BFP_QTYPES_LIST):
            self.int32_bias = False  # Cannot meet the requirement of bias_scale = input_scale * weight_scale
            logger.warning(&quot;Disabled Int32 Bias, because the quant type of activaion is BFP or MX&quot;)

        # weights-only quantization switch
        self.weights_only = False if &quot;WeightsOnly&quot; not in extra_options else extra_options[&quot;WeightsOnly&quot;]
        # include-gptq quantization switch
        self.use_gptq = False if extra_options is None or &quot;UseGPTQ&quot; not in extra_options else extra_options[&quot;UseGPTQ&quot;]
        # If GPTQ is turned on, the quantizer will only quantize weights and leave the activations in floating-point for GPTQ.
        if self.use_gptq is True:
            self.weights_only = True

    def _get_tensor_type(self, tensor_name: str) -&gt; Any:
        weight = find_by_name(tensor_name, self.model.initializer())
        if weight is not None:
            return weight.data_type
        elif tensor_name in self.value_infos:
            vi = self.value_infos[tensor_name]
            if vi.type.HasField(&quot;tensor_type&quot;):
                return vi.type.tensor_type.elem_type
        return None

    def _is_tensor_quantizable(self, tensor_name: str) -&gt; bool:
        weight = find_by_name(tensor_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                return True
        elif self.weights_only is True:
            return False
        elif tensor_name in self.value_infos.keys():
            vi = self.value_infos[tensor_name]
            if vi.type.HasField(&quot;tensor_type&quot;) and vi.type.tensor_type.elem_type == TensorProto.FLOAT:
                return True
        else:
            logger.warning(
                &quot;failed to infer the type of tensor: {}. Skip to quantize it. Please check if it is expected.&quot;.format(
                    tensor_name))

        return False

    def __quantize_tensor(self,
                          tensor_name: str,
                          quant_sharing_param: Any = None,
                          tensor_type: Any = QDQQuantTensorType.ACTIVATION) -&gt; None:
        if self._is_tensor_quantizable(tensor_name):
            if quant_sharing_param:
                data_type = self._get_tensor_type(tensor_name)
                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type,
                                                                           quant_para_provider=quant_sharing_param,
                                                                           data_type=data_type)
            elif tensor_name not in self.tensors_to_quantize:
                data_type = self._get_tensor_type(tensor_name)
                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=tensor_type, data_type=data_type)

    def quantize_activation_tensor(self, tensor_name: str, quant_sharing_param: Any = None) -&gt; Any:
        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.ACTIVATION)

    def quantize_weight_tensor(self, tensor_name: str, quant_sharing_param: Any = None) -&gt; Any:
        return self.__quantize_tensor(tensor_name, quant_sharing_param, QDQQuantTensorType.WEIGHT)

    def quantize_weight_tensor_per_channel(self, tensor_name: str, axis: Any) -&gt; None:

        weight = find_by_name(tensor_name, self.model.initializer())
        if weight:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                self.tensors_to_quantize[tensor_name] = QDQTensorQuantInfo(tensor_type=QDQQuantTensorType.WEIGHT,
                                                                           axis=axis,
                                                                           data_type=weight.data_type)
        else:
            logger.warning(
                &quot;only support per-channel quantization on weight. Tensor: {} is not quantized.&quot;.format(tensor_name))

    def quantize_bias_tensor(self, bias_name: str, input_name: str, weight_name: str, beta: float = 1.0) -&gt; None:
        weight = find_by_name(bias_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                if self.quantize_bias:
                    if self.int32_bias:
                        self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
                    else:
                        if self.per_channel:
                            self.quantize_weight_tensor_per_channel(bias_name, 0)
                        else:
                            self.quantize_weight_tensor(bias_name)
        else:
            logger.warning(f&quot;Expected {bias_name} to be a weight&quot;)

    def remove_node(self, node: NodeProto) -&gt; None:
        self.nodes_to_remove.append(node)

    def remove_nodes(self) -&gt; None:
        self.model.remove_nodes(self.nodes_to_remove)

    def quantize_model(self) -&gt; Any:
        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateQDQQuantizer(self, node)
                op_quantizer.quantize()

                if self.dedicated_qdq_pair:
                    for tensor_name in node.input:
                        if tensor_name not in self.tensor_to_its_receiving_nodes:
                            self.tensor_to_its_receiving_nodes[tensor_name] = []
                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)

        self._quantize_normal_tensors()
        self._quantize_sharing_param_tensors()
        if self.quantize_bias and self.int32_bias and not self.weights_only:
            self._quantize_bias_tensors()

        self.remove_nodes()
        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )

        if not self.add_qdq_pair_to_weight:
            self.model.clean_initializers()

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model

    def try_replacing_upstream_output(self, upstream_output_name: str, output_name: str) -&gt; bool:
        if (output_name in self.quantization_params.keys()
                and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
                and not self.model.is_graph_output(upstream_output_name)
                and not self.model.is_graph_input(upstream_output_name)):
            self.model.replace_output_of_all_nodes(upstream_output_name, output_name)
            if upstream_output_name in self.tensors_to_quantize:
                del self.tensors_to_quantize[upstream_output_name]
            return True
        return False

    def _create_qdq_nodes(self,
                          q_input: Any,
                          q_output: Any,
                          quant_node_name: str,
                          dq_input: Any,
                          dq_output: Any,
                          dequant_node_name: str,
                          scale_name: str,
                          zp_name: str,
                          axis: Any = None) -&gt; None:
        qlinear_node = onnx.helper.make_node(
            QUANT_OP_NAME,
            [q_input, scale_name, zp_name],
            [q_output],
            quant_node_name,
            axis=axis,
        )
        dequant_node = onnx.helper.make_node(
            DEQUANT_OP_NAME,
            [dq_input, scale_name, zp_name],
            [dq_output],
            dequant_node_name,
            axis=axis,
        )
        self.model.add_nodes([qlinear_node, dequant_node])

    def _add_qdq_pair_for_initializer(self, weight_proto: TensorProto, tensor_type: Any, axis: Any = None) -&gt; None:
        weight_name = weight_proto.name
        if axis is not None:
            if self.opset_version &lt; 13:
                raise ValueError(&quot;Per-Channel support with QDQ format requires onnx opset version 13 or above.&quot;)
            q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
                weight_name,
                onnx_proto.TensorProto.INT8,
                axis,
                self.calibrate_method,
                keep_float_weight=self.add_qdq_pair_to_weight)
        else:
            q_weight_name, zp_name, scale_name = self.quantize_initializer(
                weight_proto,
                self.weight_qType,
                self.calibrate_method,
                keep_float_weight=self.add_qdq_pair_to_weight,
            )

        weight_dequant_output = add_dequant_output_suffix(weight_name)
        self.model.replace_input_of_all_nodes(weight_name, weight_dequant_output)
        if self.add_qdq_pair_to_weight:
            weight_quant_output = add_quant_output_suffix(weight_name)

            self._create_qdq_nodes(
                weight_name,
                weight_quant_output,
                add_quant_suffix(weight_name),
                weight_quant_output,
                weight_dequant_output,
                add_dequant_suffix(weight_name),
                scale_name,
                zp_name,
                axis,
            )
        else:
            dequant_node = onnx.helper.make_node(
                DEQUANT_OP_NAME,
                [q_weight_name, scale_name, zp_name],
                [weight_dequant_output],
                add_dequant_suffix(weight_name),
                axis=axis,
            )
            self.model.add_node(dequant_node)

    def _add_qdq_pair_for_activation(self, tensor_name: str, scale_name: str, zp_name: str) -&gt; None:
        if (self.dedicated_qdq_pair and tensor_name in self.tensor_to_its_receiving_nodes
                and len(self.tensor_to_its_receiving_nodes[tensor_name]) &gt; 1):
            num_dedicated_qdq_pair = len(self.tensor_to_its_receiving_nodes[tensor_name])
            for i in range(num_dedicated_qdq_pair):
                postfix = f&quot;_{i + 1}&quot;
                tensor_name_quant_output_postfix = add_quant_output_suffix(tensor_name) + postfix
                tensor_name_dequant_output_postfix = add_dequant_output_suffix(tensor_name) + postfix
                quant_node_name_postfix = add_quant_suffix(tensor_name) + postfix
                dequant_node_name_postfix = add_dequant_suffix(tensor_name) + postfix
                self._create_qdq_nodes(
                    tensor_name,
                    tensor_name_quant_output_postfix,
                    quant_node_name_postfix,
                    tensor_name_quant_output_postfix,
                    tensor_name_dequant_output_postfix,
                    dequant_node_name_postfix,
                    scale_name,
                    zp_name,
                )

                node = self.tensor_to_its_receiving_nodes[tensor_name][i]
                self.model.replace_node_input(node, tensor_name, tensor_name_dequant_output_postfix)
                if i == 0:
                    quantized_value = QuantizedValue(
                        tensor_name,
                        tensor_name_dequant_output_postfix,
                        scale_name,
                        zp_name,
                        QuantizedValueType.Input,
                    )
                    self.quantized_value_map[tensor_name] = quantized_value
        else:
            q_input = tensor_name
            dq_output = add_dequant_output_suffix(tensor_name)
            if self.model.is_graph_output(tensor_name):
                q_input = add_quant_input_suffix(tensor_name)
                dq_output = tensor_name
                self.model.replace_output_of_all_nodes(tensor_name, q_input)
            else:
                self.model.replace_input_of_all_nodes(tensor_name, dq_output)

            self._create_qdq_nodes(
                q_input,
                add_quant_output_suffix(tensor_name),
                add_quant_suffix(tensor_name),
                add_quant_output_suffix(tensor_name),
                dq_output,
                add_dequant_suffix(tensor_name),
                scale_name,
                zp_name,
            )

            quantized_value = QuantizedValue(
                tensor_name,
                dq_output,
                scale_name,
                zp_name,
                QuantizedValueType.Input,
            )
            self.quantized_value_map[tensor_name] = quantized_value

    def _quantize_normal_tensors(self) -&gt; None:
        for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
            if tensor_name in self.quantized_value_map.keys():
                continue

            if not tensor_info.is_shared:
                # Quantize the input
                initializer = find_by_name(tensor_name, self.model.initializer())
                if initializer:
                    self._add_qdq_pair_for_initializer(initializer, tensor_info.tensor_type, tensor_info.axis)
                else:
                    used_scale, used_zp = self.find_quant_scale_zp(tensor_name)
                    data_found, scale_name, zp_name, _, _ = self._get_quantization_params(
                        tensor_name, used_scale, used_zp)

                    if not data_found:
                        raise ValueError(
                            f&quot;Quantization parameters are not specified for param {tensor_name}. &quot;
                            &quot;In static mode quantization params for inputs and outputs of nodes to be quantized are required.&quot;
                        )

                    self._add_qdq_pair_for_activation(tensor_name, scale_name, zp_name)

                del self.tensors_to_quantize[tensor_name]

    def _quantize_sharing_param_tensors(self) -&gt; None:
        while self.tensors_to_quantize:
            for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
                tensor_provider_name = tensor_info.quant_para_provider
                if tensor_provider_name in self.quantized_value_map:
                    del self.tensors_to_quantize[tensor_name]

                    quantized_value = self.quantized_value_map[tensor_provider_name]
                    # Quantize the input
                    initializer = find_by_name(tensor_name, self.model.initializer())
                    if initializer is not None:
                        raise ValueError(&quot;Quantization parameter shared mode is not supported for weight yet&quot;)
                    self._add_qdq_pair_for_activation(tensor_name, quantized_value.scale_name, quantized_value.zp_name)

    def _quantize_bias_tensors(self) -&gt; None:
        for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
            if bias_name in self.quantized_value_map.keys():
                continue
            # Quantize the input
            self.quantize_bias_static(bias_name, input_name, weight_name, beta)
            self.model.remove_initializer(find_by_name(bias_name, self.model.initializer()))
            quant_value = self.quantized_value_map[bias_name]
            inputs = [quant_value.q_name, quant_value.scale_name, quant_value.zp_name]
            node_name = add_dequant_suffix(bias_name)
            if quant_value.axis is not None:
                dequant_node = onnx.helper.make_node(
                    &quot;DequantizeLinear&quot;,
                    inputs,
                    [bias_name],
                    node_name,
                    axis=quant_value.axis,
                )
            else:
                dequant_node = onnx.helper.make_node(
                    &quot;DequantizeLinear&quot;,
                    inputs,
                    [bias_name],
                    node_name,
                )
            self.model.add_node(dequant_node)

    def is_tensor_quantized(self, tensor_name: str) -&gt; bool:
        return tensor_name in self.tensors_to_quantize or tensor_name in self.bias_to_quantize</div>



<div class="viewcode-block" id="VitisQDQNPUCNNQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.VitisQDQNPUCNNQuantizer">[docs]</a>
class VitisQDQNPUCNNQuantizer(VitisQDQQuantizer):
    &quot;&quot;&quot;
    A class to perform Vitis-specific Quantize-Dequantize (QDQ) quantization for NPU (Neural Processing Unit) on CNN models.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization (must be False for NPU).
        reduce_range (bool): Whether to reduce the quantization range (must be False for NPU).
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights (must be QuantType.QInt8 for NPU).
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        calibrate_method (Any): The method used for calibration.
        quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
        extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.

    Inherits from:
        VitisQDQQuantizer: Base class for Vitis-specific QDQ quantization.

    Attributes:
        tensors_to_quantize (Dict[Any, Any]): Dictionary of tensors to be quantized.
        is_weight_symmetric (bool): Whether to enforce symmetric quantization for weights.
        is_activation_symmetric (bool): Whether to enforce symmetric quantization for activations.

    &quot;&quot;&quot;

    @log_errors
    def __init__(
        self,
        model: ModelProto,
        per_channel: bool,
        reduce_range: bool,
        mode: QuantizationMode.QLinearOps,
        static: bool,
        weight_qType: Any,
        activation_qType: Any,
        tensors_range: Any,
        nodes_to_quantize: List[str],
        nodes_to_exclude: List[str],
        op_types_to_quantize: List[str],
        calibrate_method: Any,
        quantized_tensor_type: Dict[Any, Any] = {},
        extra_options: Optional[Dict[str, Any]] = None,
    ):
        &quot;&quot;&quot;
        Initializes the VitisQDQNPUCNNQuantizer with the provided configuration.

        Args:
            model (ModelProto): The ONNX model to be quantized.
            per_channel (bool): Whether to perform per-channel quantization (must be False for NPU).
            reduce_range (bool): Whether to reduce the quantization range (must be False for NPU).
            mode (QuantizationMode.QLinearOps): The quantization mode to be used.
            static (bool): Whether to use static quantization.
            weight_qType (Any): The quantization type for weights (must be QuantType.QInt8 for NPU).
            activation_qType (Any): The quantization type for activations.
            tensors_range (Any): Dictionary specifying the min and max values for tensors.
            nodes_to_quantize (List[str]): List of node names to be quantized.
            nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
            op_types_to_quantize (List[str]): List of operation types to be quantized.
            calibrate_method (Any): The method used for calibration.
            quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
            extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.
        &quot;&quot;&quot;
        self.calibrate_method = calibrate_method
        VitisQDQQuantizer.__init__(
            self,
            model,
            False,
            False,
            mode,
            static,
            weight_qType,
            activation_qType,
            tensors_range,
            nodes_to_quantize,
            nodes_to_exclude,
            op_types_to_quantize,
            calibrate_method,
            quantized_tensor_type,
            extra_options,
        )
        self.tensors_to_quantize = {}

        if per_channel:
            raise ValueError(
                &quot;Only per-tensor quantization is supported when enable_dpu=True, `per_channel` must be set to False.&quot;)

        if reduce_range:
            raise ValueError(&quot;reduce_range is not supported when enable_dpu=True, `reduce_range` must be set to False.&quot;)

        if weight_qType != QuantType.QInt8:
            raise ValueError(&quot;Only QuantType.QInt8 weight_type is supported when enable_dpu=True.&quot;)

        # If using nable_dpu, QDQ should always set WeightSymmetric as True.
        if &quot;WeightSymmetric&quot; in self.extra_options and not self.extra_options[&quot;WeightSymmetric&quot;]:
            raise ValueError(&quot;When enable_dpu=True, WeightSymmetric must be set to true.&quot;)
        self.is_weight_symmetric = True

        # If using enable_dpu, QDQ should always always set ActivationSymmetric as True.
        if &quot;ActivationSymmetric&quot; in self.extra_options and not self.extra_options[&quot;ActivationSymmetric&quot;]:
            raise ValueError(&quot;When enable_dpu=True, ActivationSymmetric must be set to true.&quot;)
        self.is_activation_symmetric = True

    def quantize_model(self) -&gt; Any:
        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateNPUCnnQDQQuantizer(self, node)
                op_quantizer.quantize()

                if self.dedicated_qdq_pair:
                    for tensor_name in node.input:
                        if tensor_name not in self.tensor_to_its_receiving_nodes:
                            self.tensor_to_its_receiving_nodes[tensor_name] = []
                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)

        self._quantize_normal_tensors()

        self._quantize_sharing_param_tensors()
        self.remove_nodes()
        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )
        if &quot;SimulateDPU&quot; not in self.extra_options or self.extra_options[&quot;SimulateDPU&quot;] is True:
            self._simulate_transforms()

        if &quot;NPULimitationCheck&quot; not in self.extra_options or self.extra_options[&quot;NPULimitationCheck&quot;] is True:
            self._quantize_refine()
        if not self.add_qdq_pair_to_weight:
            self.model.clean_initializers()

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model

    def _add_qdq_pair_for_initializer(self, weight_proto: TensorProto, tensor_type: Any, axis: Any = None) -&gt; None:
        weight_name = weight_proto.name
        q_weight_name, zp_name, scale_name = self.quantize_initializer(
            weight_proto,
            self.weight_qType,
            self.calibrate_method,
            keep_float_weight=self.add_qdq_pair_to_weight,
        )
        weight_dequant_output = add_dequant_output_suffix(weight_name)
        self.model.replace_input_of_all_nodes(weight_name, weight_dequant_output)
        if self.add_qdq_pair_to_weight:
            weight_quant_output = add_quant_output_suffix(weight_name)

            self._create_qdq_nodes(
                weight_name,
                weight_quant_output,
                add_quant_suffix(weight_name),
                weight_quant_output,
                weight_dequant_output,
                add_dequant_suffix(weight_name),
                scale_name,
                zp_name,
                axis,
            )
        else:
            dequant_node = onnx.helper.make_node(
                DEQUANT_OP_NAME,
                [q_weight_name, scale_name, zp_name],
                [weight_dequant_output],
                add_dequant_suffix(weight_name),
                axis=axis,
            )

            self.model.add_node(dequant_node)

    def quantize_bias_tensor(self, bias_name: str, input_name: str, weight_name: str, beta: float = 1.0) -&gt; None:
        weight = find_by_name(bias_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                # Use int8 quantization for bias as well as weights.
                self.quantize_weight_tensor(bias_name)
        else:
            logger.warning(&quot;Expected {} to be a weight&quot;.format(bias_name))

    def _quantize_refine(self) -&gt; None:
        max_loop_num = 5
        if &quot;MaxLoopNum&quot; in self.extra_options:
            max_loop_num = self.extra_options[&quot;MaxLoopNum&quot;]

        adjust_shift_cut = True
        if &quot;AdjustShiftCut&quot; in self.extra_options:
            adjust_shift_cut = self.extra_options[&quot;AdjustShiftCut&quot;]
        adjust_shift_bias = True
        if &quot;AdjustShiftBias&quot; in self.extra_options:
            adjust_shift_bias = self.extra_options[&quot;AdjustShiftBias&quot;]
        adjust_shift_read = True
        if &quot;AdjustShiftRead&quot; in self.extra_options:
            adjust_shift_read = self.extra_options[&quot;AdjustShiftRead&quot;]
        adjust_shift_write = True
        if &quot;AdjustShiftWrite&quot; in self.extra_options:
            adjust_shift_write = self.extra_options[&quot;AdjustShiftWrite&quot;]
        adjust_hard_sigmoid = True
        if &quot;AdjustHardSigmoid&quot; in self.extra_options:
            adjust_hard_sigmoid = self.extra_options[&quot;AdjustHardSigmoid&quot;]
        adjust_shift_swish = True
        if &quot;AdjustShiftSwish&quot; in self.extra_options:
            adjust_shift_swish = self.extra_options[&quot;AdjustShiftSwish&quot;]
        align_concat = True
        if &quot;AlignConcat&quot; in self.extra_options:
            align_concat = self.extra_options[&quot;AlignConcat&quot;]
        align_pool = True
        if &quot;AlignPool&quot; in self.extra_options:
            align_pool = self.extra_options[&quot;AlignPool&quot;]
        align_pad = True
        if &quot;AlignPad&quot; in self.extra_options:
            align_pad = self.extra_options[&quot;AlignPad&quot;]
        align_slice = True
        if &quot;AlignSlice&quot; in self.extra_options:
            align_slice = self.extra_options[&quot;AlignSlice&quot;]

        self.model = adjust_quantize_info(
            self.model,
            max_loop_num=max_loop_num,
            adjust_shift_cut=adjust_shift_cut,
            adjust_shift_bias=adjust_shift_bias,
            adjust_shift_read=adjust_shift_read,
            adjust_shift_write=adjust_shift_write,
            adjust_hard_sigmoid=adjust_hard_sigmoid,
            adjust_shift_swish=adjust_shift_swish,
            align_concat=align_concat,
            align_pool=align_pool,
            align_pad=align_pad,
            align_slice=align_slice,
        )

    def _simulate_transforms(self) -&gt; None:
        convert_leaky_relu_to_dpu_version = True
        if &quot;ConvertLeakyReluToDPUVersion&quot; in self.extra_options:
            convert_leaky_relu_to_dpu_version = self.extra_options[&quot;ConvertLeakyReluToDPUVersion&quot;]
        convert_sigmoid_to_hard_sigmoid = True
        if &quot;ConvertSigmoidToHardSigmoid&quot; in self.extra_options:
            convert_sigmoid_to_hard_sigmoid = self.extra_options[&quot;ConvertSigmoidToHardSigmoid&quot;]
        convert_hard_sigmoid_to_dpu_version = True
        if &quot;ConvertHardSigmoidToDPUVersion&quot; in self.extra_options:
            convert_hard_sigmoid_to_dpu_version = self.extra_options[&quot;ConvertHardSigmoidToDPUVersion&quot;]
        convert_avg_pool_to_dpu_version = True
        if &quot;ConvertAvgPoolToDPUVersion&quot; in self.extra_options:
            convert_avg_pool_to_dpu_version = self.extra_options[&quot;ConvertAvgPoolToDPUVersion&quot;]
        convert_reduce_mean_to_dpu_version = True
        if &quot;ConvertReduceMeanToDPUVersion&quot; in self.extra_options:
            convert_reduce_mean_to_dpu_version = self.extra_options[&quot;ConvertReduceMeanToDPUVersion&quot;]
        convert_softmax_to_dpu_version = False
        if &quot;ConvertSoftmaxToDPUVersion&quot; in self.extra_options:
            convert_softmax_to_dpu_version = self.extra_options[&quot;ConvertSoftmaxToDPUVersion&quot;]
        convert_instance_norm_to_dpu_version = False
        if &quot;ConvertInstanceNormToDPUVersion&quot; in self.extra_options:
            convert_instance_norm_to_dpu_version = self.extra_options[&quot;ConvertInstanceNormToDPUVersion&quot;]

        self.model.model, self.nodes_to_exclude = simulate_transforms(
            self.model.model,
            self.should_quantize_node,
            self.nodes_to_quantize,
            self.nodes_to_exclude,
            convert_leaky_relu_to_dpu_version=convert_leaky_relu_to_dpu_version,
            convert_sigmoid_to_hard_sigmoid=convert_sigmoid_to_hard_sigmoid,
            convert_hard_sigmoid_to_dpu_version=convert_hard_sigmoid_to_dpu_version,
            convert_avg_pool_to_dpu_version=convert_avg_pool_to_dpu_version,
            convert_reduce_mean_to_dpu_version=convert_reduce_mean_to_dpu_version,
            convert_softmax_to_dpu_version=convert_softmax_to_dpu_version,
            convert_instance_norm_to_dpu_version=convert_instance_norm_to_dpu_version,
        )</div>



<div class="viewcode-block" id="VitisExtendedQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.VitisExtendedQuantizer">[docs]</a>
class VitisExtendedQuantizer(VitisQDQQuantizer):
    &quot;&quot;&quot;
    A class to perform extended Vitis-specific Quantize-Dequantize (QDQ) quantization.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization.
        reduce_range (bool): Whether to reduce the quantization range.
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        quant_format (Any): The format for quantization.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights.
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        calibrate_method (Any): The method used for calibration.
        quantized_tensor_type (Dict[Any, Any]): Dictionary specifying quantized tensor types.
        extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.

    Inherits from:
        VitisQDQQuantizer: Base class for Vitis-specific QDQ quantization.

    Attributes:
        tensors_to_quantize (Dict[Any, Any]): Dictionary of tensors to be quantized.
        quant_format (Any): The format for quantization.
        add_qdq_pair_to_weight (bool): Whether to add QDQ pair to weight (and bias).
        fold_relu (bool): Whether to fold ReLU layers.

    &quot;&quot;&quot;

    def __init__(
        self,
        model: ModelProto,
        per_channel: bool,
        reduce_range: bool,
        mode: QuantizationMode.QLinearOps,
        quant_format: Any,
        static: bool,
        weight_qType: Any,
        activation_qType: Any,
        tensors_range: Any,
        nodes_to_quantize: List[str],
        nodes_to_exclude: List[str],
        op_types_to_quantize: List[str],
        calibrate_method: Any,
        quantized_tensor_type: Dict[Any, Any],
        extra_options: Optional[Dict[str, Any]] = None,
    ):
        &quot;&quot;&quot;
        Initializes the VitisExtendedQuantizer with the provided configuration.

        Args:
            model (ModelProto): The ONNX model to be quantized.
            per_channel (bool): Whether to perform per-channel quantization.
            reduce_range (bool): Whether to reduce the quantization range.
            mode (QuantizationMode.QLinearOps): The quantization mode to be used.
            quant_format (Any): The format for quantization.
            static (bool): Whether to use static quantization.
            weight_qType (Any): The quantization type for weights.
            activation_qType (Any): The quantization type for activations.
            tensors_range (Any): Dictionary specifying the min and max values for tensors.
            nodes_to_quantize (List[str]): List of node names to be quantized.
            nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
            op_types_to_quantize (List[str]): List of operation types to be quantized.
            calibrate_method (Any): The method used for calibration.
            quantized_tensor_type (Dict[Any, Any]): Dictionary specifying quantized tensor types.
            extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.
        &quot;&quot;&quot;
        self.calibrate_method = calibrate_method
        VitisQDQQuantizer.__init__(
            self,
            model,
            per_channel,
            reduce_range,
            mode,
            static,
            weight_qType,
            activation_qType,
            tensors_range,
            nodes_to_quantize,
            nodes_to_exclude,
            op_types_to_quantize,
            calibrate_method,
            quantized_tensor_type,
            extra_options,
        )
        self.tensors_to_quantize = {}

        self.quant_format = quant_format
        assert self.quant_format == VitisQuantFormat.QDQ

        # We add Q/DQ pair to weight (and bias) for float16 and bfloat16 by default,
        # which is aimed to avoid failure of data persistence check.
        # For Interger quantization type, we fold Q to support fast finetune.
        if self.weight_qType in ONNX_FP_QTYPES_LIST:
            self.add_qdq_pair_to_weight = True
        else:
            self.add_qdq_pair_to_weight = False
        if extra_options is not None and &quot;AddQDQPairToWeight&quot; in extra_options:
            self.add_qdq_pair_to_weight = extra_options[&quot;AddQDQPairToWeight&quot;]
        self.quantized_tensor_type = quantized_tensor_type
        self.fold_relu = extra_options.get(&quot;FoldRelu&quot;, False) if extra_options is not None else False

        self.fn_name_w, self.fn_attrs_w = self._fn_name_and_attrs(weight_qType)
        self.fn_name_a, self.fn_attrs_a = self._fn_name_and_attrs(activation_qType)

    def quantize_model(self) -&gt; Any:
        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateNPUCnnQDQQuantizer(self, node)
                op_quantizer.quantize()

                if self.dedicated_qdq_pair:
                    for tensor_name in node.input:
                        if tensor_name not in self.tensor_to_its_receiving_nodes:
                            self.tensor_to_its_receiving_nodes[tensor_name] = []
                        self.tensor_to_its_receiving_nodes[tensor_name].append(node)

        self._quantize_normal_tensors()
        self._quantize_sharing_param_tensors()
        if self.quantize_bias and self.int32_bias and not self.weights_only:
            self._quantize_bias_tensors()

        self.remove_nodes()
        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )
        if &quot;SimulateDPU&quot; not in self.extra_options or self.extra_options[&quot;SimulateDPU&quot;] is True:
            self._simulate_transforms()

        if &quot;NPULimitationCheck&quot; not in self.extra_options or self.extra_options[&quot;NPULimitationCheck&quot;] is True:
            self._quantize_refine()

        self.model.clean_initializers()

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model

    def try_replacing_upstream_output(self, upstream_output_name: str, output_name: str) -&gt; bool:
        # TODO : Understand the principle here and fix the issue caused by QDQRemovableActivation.
        # As showed at onnxruntime/quantization/operators/activation.py, if activation uses asymmetric,
        # the QDQRemovableActivation remove nodes, which caused the graph broken.
        if (self.fold_relu and output_name in self.quantization_params
                and len(self.model.input_name_to_nodes()[upstream_output_name]) == 1
                and not self.model.is_graph_output(upstream_output_name)
                and not self.model.is_graph_input(upstream_output_name)):
            self.model.replace_output_of_all_nodes(upstream_output_name, output_name)
            if upstream_output_name in self.tensors_to_quantize:
                del self.tensors_to_quantize[upstream_output_name]
            return True
        return False

    &#39;&#39;&#39;
    def _create_fn_nodes(self,
                         q_input: Any,
                         dq_output: Any,
                         dequant_node_name: str,
                         scale_name: str,
                         zp_name: str,
                         axis: Any = None) -&gt; None:
        &quot;&quot;&quot;
        create fix_neuron node
        &quot;&quot;&quot;
        fix_neuron_node = onnx.helper.make_node(
            FIX_OP_NAME,
            [q_input, scale_name, zp_name],
            [dq_output],
            dequant_node_name,
            axis=axis,
            domain=VAI_DOMAIN,
        )
        bit_width = onnx.helper.make_attribute(&quot;bit_width&quot;, &quot;8&quot;)
        fix_neuron_node.attribute.append(bit_width)

        scale = find_by_name(scale_name, self.model.initializer())
        scale = scale.float_data[0]
        pos = int(np.rint(-np.log2(scale)))
        pos_attr = onnx.helper.make_attribute(&quot;pos&quot;, str(pos))
        fix_neuron_node.attribute.append(pos_attr)

        self.model.add_nodes([fix_neuron_node])
    &#39;&#39;&#39;

    def _fn_name_and_attrs(self, qType: Any) -&gt; tuple[str, Dict[str, Any]]:
        if qType == VitisQuantType.QBFP:
            fn_name = BFPFIX_OP_NAME
            fn_attrs = copy.deepcopy(BFPFIX_OP_DEFAULT_ATTRS)
            # Get attributes for BFPFixNeuron
            if self.extra_options is not None and &quot;BFPAttributes&quot; in self.extra_options:
                fn_attrs.update(self.extra_options[&quot;BFPAttributes&quot;])
        elif qType == VitisQuantType.QMX:
            fn_name = MXFIX_OP_NAME
            fn_attrs = copy.deepcopy(MXFIX_OP_DEFAULT_ATTRS)
            # Get attributes for MXFixNeuron
            if self.extra_options is not None and &quot;MXAttributes&quot; in self.extra_options:
                fn_attrs.update(self.extra_options[&quot;MXAttributes&quot;])
        else:
            fn_name = FIX_OP_NAME
            fn_attrs = {
                **FIX_OP_DEFAULT_ATTRS,
            }
        return fn_name, fn_attrs

    def _create_fn_nodes(self, q_input: Any, dq_output: Any, dequant_node_name: str, scale_name: str, zp_name: str,
                         fn_name: str, fn_attrs: Any) -&gt; None:
        &quot;&quot;&quot;
        create fix_neuron node
        &quot;&quot;&quot;
        fix_neuron_node = onnx.helper.make_node(
            fn_name,
            [q_input],
            [dq_output],
            dequant_node_name,
            domain=COP_DOMAIN,
        )

        for k, v in fn_attrs.items():
            fix_neuron_node.attribute.append(onnx.helper.make_attribute(k, v))

        self.model.add_nodes([fix_neuron_node])

    def _create_pof2qdq_nodes(self,
                              q_input: Any,
                              q_output: Any,
                              quant_node_name: str,
                              dq_input: Any,
                              dq_output: Any,
                              dequant_node_name: str,
                              scale_name: str,
                              zp_name: str,
                              axis: Any = None) -&gt; None:
        qlinear_node = onnx.helper.make_node(
            QUANT_OP_NAME,
            [q_input, scale_name, zp_name],
            [q_output],
            quant_node_name,
            axis=axis,
            domain=VAI_DOMAIN,
        )
        dequant_node = onnx.helper.make_node(
            DEQUANT_OP_NAME,
            [dq_input, scale_name, zp_name],
            [dq_output],
            dequant_node_name,
            axis=axis,
            domain=VAI_DOMAIN,
        )
        bit_width = onnx.helper.make_attribute(&quot;bit_width&quot;, &quot;8&quot;)

        scale = find_by_name(scale_name, self.model.initializer())
        scale = scale.float_data[0]
        pos = int(np.rint(-np.log2(scale)))
        pos_attr = onnx.helper.make_attribute(&quot;pos&quot;, str(pos))

        qlinear_node.attribute.append(bit_width)
        qlinear_node.attribute.append(pos_attr)
        dequant_node.attribute.append(bit_width)
        dequant_node.attribute.append(pos_attr)
        self.model.add_nodes([qlinear_node, dequant_node])

    def _create_customqdq_nodes(self,
                                q_input: Any,
                                q_output: Any,
                                quant_node_name: str,
                                dq_input: Any,
                                dq_output: Any,
                                dequant_node_name: str,
                                scale_name: str,
                                zp_name: str,
                                axis: Any = None) -&gt; None:
        qlinear_node = onnx.helper.make_node(
            COP_QUANT_OP_NAME,
            [q_input, scale_name, zp_name],
            [q_output],
            quant_node_name,
            axis=axis,
            domain=COP_DOMAIN,
        )
        dequant_node = onnx.helper.make_node(
            COP_DEQUANT_OP_NAME,
            [dq_input, scale_name, zp_name],
            [dq_output],
            dequant_node_name,
            axis=axis,
            domain=COP_DOMAIN,
        )
        self.model.add_nodes([qlinear_node, dequant_node])

    def _add_fn_pair_for_weight(self, weight_proto: TensorProto, axis: Any = None, zp_type: Any = None) -&gt; None:
        weight_name = weight_proto.name

        if zp_type is not None:
            fn_name, fn_attrs = self._fn_name_and_attrs(zp_type)
            zp_type = get_tensor_type_from_qType(zp_type)
        else:
            fn_name, fn_attrs = self.fn_name_w, self.fn_attrs_w
            zp_type = self.weight_qType

        for key in fn_attrs.keys():
            if key == &quot;axis&quot; and len(weight_proto.dims) == 1:
                fn_attrs[key] = 0  # For scalar, the axis should always be 0
            if key == &quot;convert_to_bfloat_before_bfp&quot;:
                fn_attrs[key] = 0  # Initializer is a constant, no conversion required

        if axis is not None:
            if self.opset_version &lt; 13:
                raise ValueError(&quot;Per-Channel support with QDQ format requires onnx opset version 13 or above.&quot;)
            q_weight_name, zp_name, scale_name = self.quantize_weight_per_channel(
                weight_name, zp_type, axis, self.calibrate_method, keep_float_weight=self.add_qdq_pair_to_weight)
        else:
            q_weight_name, zp_name, scale_name = self.quantize_initializer(
                weight_proto,
                zp_type,
                self.calibrate_method,
                keep_float_weight=self.add_qdq_pair_to_weight,
            )

        weight_dequant_output = add_dequant_output_suffix(weight_name)
        self.model.replace_input_of_all_nodes(weight_name, weight_dequant_output)
        if zp_type in ONNX_BFP_QTYPES_LIST:
            self._create_fn_nodes(
                weight_name,
                weight_dequant_output,
                add_dequant_suffix(weight_name),
                scale_name,
                zp_name,
                fn_name,
                fn_attrs,
            )
        elif self.add_qdq_pair_to_weight:
            weight_quant_output = add_quant_output_suffix(weight_name)
            if zp_type in ONNX_WBIT_QTYPES_LIST or self.use_qdq_vitis_custom_ops:
                self._create_customqdq_nodes(
                    weight_name,
                    weight_quant_output,
                    add_quant_suffix(weight_name),
                    weight_quant_output,
                    weight_dequant_output,
                    add_dequant_suffix(weight_name),
                    scale_name,
                    zp_name,
                    axis,
                )
            else:
                self._create_pof2qdq_nodes(
                    weight_name,
                    weight_quant_output,
                    add_quant_suffix(weight_name),
                    weight_quant_output,
                    weight_dequant_output,
                    add_dequant_suffix(weight_name),
                    scale_name,
                    zp_name,
                    axis,
                )
        else:
            if zp_type in ONNX_WBIT_QTYPES_LIST or self.use_qdq_vitis_custom_ops:
                dequant_node = onnx.helper.make_node(
                    COP_DEQUANT_OP_NAME,
                    [q_weight_name, scale_name, zp_name],
                    [weight_dequant_output],
                    add_dequant_suffix(weight_name),
                    axis=axis,
                    domain=COP_DOMAIN,
                )
                self.model.add_node(dequant_node)
            else:
                dequant_node = onnx.helper.make_node(
                    DEQUANT_OP_NAME,
                    [q_weight_name, scale_name, zp_name],
                    [weight_dequant_output],
                    add_dequant_suffix(weight_name),
                    axis=axis,
                    domain=VAI_DOMAIN,
                )
                bit_width = onnx.helper.make_attribute(&quot;bit_width&quot;, &quot;8&quot;)
                dequant_node.attribute.append(bit_width)
                self.model.add_node(dequant_node)

    def _add_fn_pair_for_activation(self, tensor_name: str, scale_name: str, zp_name: str, zp_type: Any = None) -&gt; Any:
        if zp_type is not None:
            fn_name, fn_attrs = self._fn_name_and_attrs(zp_type)
            zp_type = get_tensor_type_from_qType(zp_type)
        else:
            fn_name, fn_attrs = self.fn_name_a, self.fn_attrs_a
            zp_type = self.activation_qType
        if (self.dedicated_qdq_pair and tensor_name in self.tensor_to_its_receiving_nodes
                and len(self.tensor_to_its_receiving_nodes[tensor_name]) &gt; 1):
            num_dedicated_qdq_pair = len(self.tensor_to_its_receiving_nodes[tensor_name])
            for i in range(num_dedicated_qdq_pair):
                postfix = f&quot;_{i + 1}&quot;
                tensor_name_quant_output_postfix = add_quant_output_suffix(tensor_name) + postfix
                tensor_name_dequant_output_postfix = add_dequant_output_suffix(tensor_name) + postfix
                quant_node_name_postfix = add_quant_suffix(tensor_name) + postfix
                dequant_node_name_postfix = add_dequant_suffix(tensor_name) + postfix

                if zp_type in ONNX_BFP_QTYPES_LIST:
                    self._create_fn_nodes(tensor_name, tensor_name_dequant_output_postfix, dequant_node_name_postfix,
                                          scale_name, zp_name, fn_name, fn_attrs)
                else:
                    if zp_type in ONNX_WBIT_QTYPES_LIST or self.use_qdq_vitis_custom_ops:
                        self._create_customqdq_nodes(
                            tensor_name,
                            tensor_name_quant_output_postfix,
                            quant_node_name_postfix,
                            tensor_name_quant_output_postfix,
                            tensor_name_dequant_output_postfix,
                            dequant_node_name_postfix,
                            scale_name,
                            zp_name,
                        )
                    else:
                        self._create_pof2qdq_nodes(
                            tensor_name,
                            tensor_name_quant_output_postfix,
                            quant_node_name_postfix,
                            tensor_name_quant_output_postfix,
                            tensor_name_dequant_output_postfix,
                            dequant_node_name_postfix,
                            scale_name,
                            zp_name,
                        )

                node = self.tensor_to_its_receiving_nodes[tensor_name][i]
                self.model.replace_node_input(node, tensor_name, tensor_name_dequant_output_postfix)
                if i == 0:
                    quantized_value = QuantizedValue(
                        tensor_name,
                        tensor_name_dequant_output_postfix,
                        scale_name,
                        zp_name,
                        QuantizedValueType.Input,
                    )
                    self.quantized_value_map[tensor_name] = quantized_value
        else:
            q_input = tensor_name
            dq_output = add_dequant_output_suffix(tensor_name)
            if self.model.is_graph_output(tensor_name):
                q_input = add_quant_input_suffix(tensor_name)
                dq_output = tensor_name
                self.model.replace_output_of_all_nodes(tensor_name, q_input)
            else:
                self.model.replace_input_of_all_nodes(tensor_name, dq_output)

            if zp_type in ONNX_BFP_QTYPES_LIST:
                self._create_fn_nodes(q_input, dq_output, add_dequant_suffix(tensor_name), scale_name, zp_name, fn_name,
                                      fn_attrs)
            else:
                if zp_type in ONNX_WBIT_QTYPES_LIST or self.use_qdq_vitis_custom_ops:
                    self._create_customqdq_nodes(
                        q_input,
                        add_quant_output_suffix(tensor_name),
                        add_quant_suffix(tensor_name),
                        add_quant_output_suffix(tensor_name),
                        dq_output,
                        add_dequant_suffix(tensor_name),
                        scale_name,
                        zp_name,
                    )
                else:
                    self._create_pof2qdq_nodes(
                        q_input,
                        add_quant_output_suffix(tensor_name),
                        add_quant_suffix(tensor_name),
                        add_quant_output_suffix(tensor_name),
                        dq_output,
                        add_dequant_suffix(tensor_name),
                        scale_name,
                        zp_name,
                    )

            quantized_value = QuantizedValue(
                tensor_name,
                dq_output,
                scale_name,
                zp_name,
                QuantizedValueType.Input,
            )
            self.quantized_value_map[tensor_name] = quantized_value

    def _quantize_normal_tensors(self) -&gt; None:
        for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():

            if tensor_name in self.quantized_value_map.keys():
                continue

            if not tensor_info.is_shared:
                # This is for tensor-wise mixed precision
                zp_type = None
                if tensor_name in self.quantized_tensor_type:
                    zp_type = self.quantized_tensor_type[tensor_name]
                # Quantize the input
                initializer = find_by_name(tensor_name, self.model.initializer())
                if initializer:
                    if self.weight_qType == TensorProto.BFLOAT16:
                        weight = onnx.numpy_helper.to_array(initializer)
                        # clip weight to the range of BFLOAT16 [1.17549435e-38, 3.38953139e38]
                        if np.max(np.abs(weight)) &gt; 3.38953139e38 or np.min(np.abs(weight)) &lt; 1.17549435e-38:
                            original_weight = weight
                            weight = (np.sign(original_weight) *
                                      np.clip(np.abs(original_weight), 1.17549435e-38, 3.38953139e38)).astype(
                                          original_weight.dtype)
                            logger.info(
                                f&quot;The original weight of {tensor_name}: {original_weight} has been clipped to new weight: {weight} because it is out of BFLOAT16 boundary.&quot;
                            )
                        initializer_new = onnx.numpy_helper.from_array(weight, name=initializer.name)
                        initializer.CopyFrom(initializer_new)
                    self._add_fn_pair_for_weight(initializer, tensor_info.axis, zp_type)
                else:
                    if (zp_type is None and self.activation_qType in ONNX_BFP_QTYPES_LIST) or (
                            zp_type is not None and zp_type in [VitisQuantType.QBFP, VitisQuantType.QMX]):
                        self._add_fn_pair_for_activation(tensor_name, &#39;&#39;, &#39;&#39;,
                                                         zp_type)  # BFP doesn&#39;t need scale and zero point
                        del self.tensors_to_quantize[tensor_name]
                        continue
                    used_scale, used_zp = self.find_quant_scale_zp(tensor_name)
                    data_found, scale_name, zp_name, _, _ = self._get_quantization_params(
                        tensor_name, used_scale, used_zp, zp_type)

                    if not data_found:
                        raise ValueError(
                            f&quot;Quantization parameters are not specified for param {tensor_name}. &quot;
                            &quot;In static mode quantization params for inputs and outputs of nodes to be quantized are required.&quot;
                        )

                    self._add_fn_pair_for_activation(tensor_name, scale_name, zp_name, zp_type)

                del self.tensors_to_quantize[tensor_name]

    def _quantize_sharing_param_tensors(self) -&gt; None:
        while self.tensors_to_quantize:
            for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():
                tensor_provider_name = tensor_info.quant_para_provider
                if tensor_provider_name in self.quantized_value_map:
                    del self.tensors_to_quantize[tensor_name]

                    quantized_value = self.quantized_value_map[tensor_provider_name]
                    # Quantize the input
                    initializer = find_by_name(tensor_name, self.model.initializer())
                    if initializer is not None:
                        raise ValueError(&quot;Quantization parameter shared mode is not supported for weight yet&quot;)
                    self._add_fn_pair_for_activation(tensor_name, quantized_value.scale_name, quantized_value.zp_name)

    def _quantize_bias_tensors(self) -&gt; None:
        for bias_name, input_name, weight_name, beta in self.bias_to_quantize:
            if bias_name in self.quantized_value_map.keys():
                continue
            # Quantize the input
            self.quantize_bias_static(bias_name, input_name, weight_name, beta)
            self.model.remove_initializer(find_by_name(bias_name, self.model.initializer()))
            quant_value = self.quantized_value_map[bias_name]
            inputs = [quant_value.q_name, quant_value.scale_name, quant_value.zp_name]
            node_name = add_dequant_suffix(bias_name)

            # Keep the QDQ type of bias consistent with the weights
            if self.weight_qType in ONNX_WBIT_QTYPES_LIST or self.use_qdq_vitis_custom_ops:
                if quant_value.axis is not None:
                    dequant_node = onnx.helper.make_node(
                        COP_DEQUANT_OP_NAME,
                        inputs,
                        [bias_name],
                        node_name,
                        axis=quant_value.axis,
                        domain=COP_DOMAIN,
                    )
                else:
                    dequant_node = onnx.helper.make_node(
                        COP_DEQUANT_OP_NAME,
                        inputs,
                        [bias_name],
                        node_name,
                        domain=COP_DOMAIN,
                    )
                self.model.add_node(dequant_node)
                continue

            if quant_value.axis is not None:
                dequant_node = onnx.helper.make_node(
                    DEQUANT_OP_NAME,
                    inputs,
                    [bias_name],
                    node_name,
                    axis=quant_value.axis,
                    domain=VAI_DOMAIN,
                )
            else:
                dequant_node = onnx.helper.make_node(
                    DEQUANT_OP_NAME,
                    inputs,
                    [bias_name],
                    node_name,
                    domain=VAI_DOMAIN,
                )
            bit_width = onnx.helper.make_attribute(&quot;bit_width&quot;, &quot;8&quot;)
            dequant_node.attribute.append(bit_width)
            self.model.add_node(dequant_node)

    def quantize_bias_tensor(self, bias_name: str, input_name: str, weight_name: str, beta: float = 1.0) -&gt; None:
        weight = find_by_name(bias_name, self.model.initializer())
        if weight is not None:
            if weight.data_type == onnx_proto.TensorProto.FLOAT:
                if self.quantize_bias:
                    if self.int32_bias:
                        self.bias_to_quantize.append((bias_name, input_name, weight_name, beta))
                    else:
                        if self.per_channel:
                            self.quantize_weight_tensor_per_channel(bias_name, 0)
                        else:
                            self.quantize_weight_tensor(bias_name)
        else:
            logger.warning(f&quot;Expected {bias_name} to be a weight&quot;)

    def _adjust_model_scale(self) -&gt; None:
        for node in self.model.model.graph.node:
            if node.op_type == &quot;DequantizeLinear&quot; or node.op_type == &quot;QuantizeLinear&quot;:
                pos = None
                for attr in node.attribute:
                    if attr.name == &quot;pos&quot;:
                        pos = int(attr.s)
                if pos is None:
                    continue
                new_scale = float(np.power(2., -pos))
                for i in self.model.model.graph.initializer:
                    if i.name == node.input[1]:
                        if i.float_data[0] != new_scale:
                            i.float_data[0] = new_scale

    def _quantize_refine(self) -&gt; None:
        max_loop_num = 5
        if &quot;MaxLoopNum&quot; in self.extra_options:
            max_loop_num = self.extra_options[&quot;MaxLoopNum&quot;]

        align_concat = False
        if &quot;AlignConcat&quot; in self.extra_options:
            align_concat = self.extra_options[&quot;AlignConcat&quot;]
        align_pool = False
        if &quot;AlignPool&quot; in self.extra_options:
            align_pool = self.extra_options[&quot;AlignPool&quot;]
        align_pad = False
        if &quot;AlignPad&quot; in self.extra_options:
            align_pad = self.extra_options[&quot;AlignPad&quot;]
        align_slice = False
        if &quot;AlignSlice&quot; in self.extra_options:
            align_slice = self.extra_options[&quot;AlignSlice&quot;]
        align_transpose = False
        if &quot;AlignTranspose&quot; in self.extra_options:
            align_transpose = self.extra_options[&quot;AlignTranspose&quot;]
        align_reshape = False
        if &quot;AlignReshape&quot; in self.extra_options:
            align_reshape = self.extra_options[&quot;AlignReshape&quot;]
        adjust_bias_scale = True
        if &quot;AdjustBiasScale&quot; in self.extra_options:
            adjust_bias_scale = self.extra_options[&quot;AdjustBiasScale&quot;]

        self.model = align_quantize_info(
            self.model,
            max_loop_num=max_loop_num,
            align_concat=align_concat,
            align_pool=align_pool,
            align_pad=align_pad,
            align_slice=align_slice,
            align_transpose=align_transpose,
            align_reshape=align_reshape,
            adjust_bias_scale=adjust_bias_scale,
        )

        if self.weight_qType in [TensorProto.INT8, TensorProto.UINT8] and self.activation_qType in [
                TensorProto.INT8, TensorProto.UINT8
        ] and self.use_qdq_vitis_custom_ops:
            self._adjust_model_scale()

    def _simulate_transforms(self) -&gt; None:
        convert_leaky_relu_to_dpu_version = False
        if &quot;ConvertLeakyReluToDPUVersion&quot; in self.extra_options:
            convert_leaky_relu_to_dpu_version = self.extra_options[&quot;ConvertLeakyReluToDPUVersion&quot;]
        convert_sigmoid_to_hard_sigmoid = False
        if &quot;ConvertSigmoidToHardSigmoid&quot; in self.extra_options:
            convert_sigmoid_to_hard_sigmoid = self.extra_options[&quot;ConvertSigmoidToHardSigmoid&quot;]
        convert_hard_sigmoid_to_dpu_version = False
        if &quot;ConvertHardSigmoidToDPUVersion&quot; in self.extra_options:
            convert_hard_sigmoid_to_dpu_version = self.extra_options[&quot;ConvertHardSigmoidToDPUVersion&quot;]
        convert_avg_pool_to_dpu_version = False
        if &quot;ConvertAvgPoolToDPUVersion&quot; in self.extra_options:
            convert_avg_pool_to_dpu_version = self.extra_options[&quot;ConvertAvgPoolToDPUVersion&quot;]
        convert_reduce_mean_to_dpu_version = False
        if &quot;ConvertReduceMeanToDPUVersion&quot; in self.extra_options:
            convert_reduce_mean_to_dpu_version = self.extra_options[&quot;ConvertReduceMeanToDPUVersion&quot;]
        convert_softmax_to_dpu_version = False
        if &quot;ConvertSoftmaxToDPUVersion&quot; in self.extra_options:
            convert_softmax_to_dpu_version = self.extra_options[&quot;ConvertSoftmaxToDPUVersion&quot;]
        convert_instance_norm_to_dpu_version = False
        if &quot;ConvertInstanceNormToDPUVersion&quot; in self.extra_options:
            convert_instance_norm_to_dpu_version = self.extra_options[&quot;ConvertInstanceNormToDPUVersion&quot;]

        self.model.model, self.nodes_to_exclude = simulate_transforms(
            self.model.model,
            self.should_quantize_node,
            self.nodes_to_quantize,
            self.nodes_to_exclude,
            convert_leaky_relu_to_dpu_version=convert_leaky_relu_to_dpu_version,
            convert_sigmoid_to_hard_sigmoid=convert_sigmoid_to_hard_sigmoid,
            convert_hard_sigmoid_to_dpu_version=convert_hard_sigmoid_to_dpu_version,
            convert_avg_pool_to_dpu_version=convert_avg_pool_to_dpu_version,
            convert_reduce_mean_to_dpu_version=convert_reduce_mean_to_dpu_version,
            convert_softmax_to_dpu_version=convert_softmax_to_dpu_version,
            convert_instance_norm_to_dpu_version=convert_instance_norm_to_dpu_version,
        )</div>



<div class="viewcode-block" id="VitisBFPQuantizer">
<a class="viewcode-back" href="../../../autoapi/quark/onnx/qdq_quantizer/index.html#quark.onnx.qdq_quantizer.VitisBFPQuantizer">[docs]</a>
class VitisBFPQuantizer(VitisQDQQuantizer):
    &quot;&quot;&quot;
    A class to perform Vitis-specific Block Floating Point (BFP) Quantization-Dequantization (QDQ) quantization.

    Args:
        model (ModelProto): The ONNX model to be quantized.
        per_channel (bool): Whether to perform per-channel quantization.
        reduce_range (bool): Whether to reduce the quantization range.
        mode (QuantizationMode.QLinearOps): The quantization mode to be used.
        quant_format (Any): The format for quantization.
        static (bool): Whether to use static quantization.
        weight_qType (Any): The quantization type for weights.
        activation_qType (Any): The quantization type for activations.
        tensors_range (Any): Dictionary specifying the min and max values for tensors.
        nodes_to_quantize (List[str]): List of node names to be quantized.
        nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
        op_types_to_quantize (List[str]): List of operation types to be quantized.
        calibrate_method (Any): The method used for calibration.
        quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
        extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.

    Inherits from:
        VitisQDQQuantizer: Base class for Vitis-specific QDQ quantization.

    Attributes:
        int32_bias (bool): Whether to quantize bias as int32.
        is_activation_symmetric (bool): Whether to use symmetric quantization for activations.
        quant_format (Any): The format for quantization.
        fn_type: (string): The op type of the fix neuron.
        fn_attrs (Dict[str, Any]): Attributes for BFP/MX fix neuron.
    &quot;&quot;&quot;

    def __init__(self,
                 model: ModelProto,
                 per_channel: bool,
                 reduce_range: bool,
                 mode: QuantizationMode.QLinearOps,
                 quant_format: Any,
                 static: bool,
                 weight_qType: Any,
                 activation_qType: Any,
                 tensors_range: Any,
                 nodes_to_quantize: List[str],
                 nodes_to_exclude: List[str],
                 op_types_to_quantize: List[str],
                 calibrate_method: Any,
                 quantized_tensor_type: Dict[Any, Any] = {},
                 extra_options: Optional[Dict[str, Any]] = None):
        &quot;&quot;&quot;
        Initializes the VitisBFPQuantizer with the provided configuration.

        Args:
            model (ModelProto): The ONNX model to be quantized.
            per_channel (bool): Whether to perform per-channel quantization.
            reduce_range (bool): Whether to reduce the quantization range.
            mode (QuantizationMode.QLinearOps): The quantization mode to be used.
            static (bool): Whether to use static quantization.
            weight_qType (Any): The quantization type for weights.
            activation_qType (Any): The quantization type for activations.
            tensors_range (Any): Dictionary specifying the min and max values for tensors.
            nodes_to_quantize (List[str]): List of node names to be quantized.
            nodes_to_exclude (List[str]): List of node names to be excluded from quantization.
            op_types_to_quantize (List[str]): List of operation types to be quantized.
            calibrate_method (Any): The method used for calibration.
            quantized_tensor_type (Dict[Any, Any], optional): Dictionary specifying quantized tensor types.
            extra_options (Optional[Dict[str, Any]], optional): Additional options for quantization.
        &quot;&quot;&quot;
        super().__init__(model, per_channel, reduce_range, mode, static, weight_qType, activation_qType, tensors_range,
                         nodes_to_quantize, nodes_to_exclude, op_types_to_quantize, calibrate_method,
                         quantized_tensor_type, extra_options)

        self.int32_bias = False
        if extra_options is not None and &quot;Int32Bias&quot; in extra_options and extra_options[&quot;Int32Bias&quot;]:
            self.int32_bias = extra_options[&quot;Int32Bias&quot;]
            logger.warning(&quot;Will not quantize Bias since do not support Int32Bias in BFP/MX mode&quot;)

        self.is_activation_symmetric = True
        if self.extra_options is not None and &quot;ActivationSymmetric&quot; in self.extra_options and not self.extra_options[
                &quot;ActivationSymmetric&quot;]:
            self.is_activation_symmetric = self.extra_options[&quot;ActivationSymmetric&quot;]
            logger.warning(&quot;Setting ActivationSymmetric to False has no effect on BFP/MX mode&quot;)

        self.quant_format = quant_format
        assert self.quant_format in [VitisQuantFormat.BFPFixNeuron, VitisQuantFormat.MXFixNeuron]

        self.fn_name = &quot;&quot;
        self.fn_attrs = {}
        if self.quant_format == VitisQuantFormat.BFPFixNeuron:
            self.fn_name = BFPFIX_OP_NAME
            self.fn_attrs = copy.deepcopy(BFPFIX_OP_DEFAULT_ATTRS)
            # Get attributes for BFPFixNeuron
            if extra_options is not None and &quot;BFPAttributes&quot; in extra_options:
                self.fn_attrs.update(extra_options[&quot;BFPAttributes&quot;])
        else:
            self.fn_name = MXFIX_OP_NAME
            self.fn_attrs = copy.deepcopy(MXFIX_OP_DEFAULT_ATTRS)
            # Get attributes for MXFixNeuron
            if extra_options is not None and &quot;MXAttributes&quot; in extra_options:
                self.fn_attrs.update(extra_options[&quot;MXAttributes&quot;])

    def _create_fn_nodes(self,
                         q_input: Any,
                         dq_output: Any,
                         dequant_node_name: str,
                         axis: Any = None,
                         convert_to: Any = None) -&gt; None:
        &quot;&quot;&quot;
        create fix_neuron node
        &quot;&quot;&quot;
        fix_neuron_node = onnx.helper.make_node(
            self.fn_name,
            [q_input],
            [dq_output],
            dequant_node_name,
            domain=COP_DOMAIN,
        )

        for k, v in self.fn_attrs.items():
            if k == &quot;axis&quot; and axis is not None:
                v = axis
            elif k == &quot;convert_to_bfloat_before_bfp&quot; and convert_to is not None:
                v = convert_to
            fix_neuron_node.attribute.append(onnx.helper.make_attribute(k, v))

        self.model.add_nodes([fix_neuron_node])

    def _add_fn_pair_for_weight(self, weight_proto: TensorProto) -&gt; None:
        weight_name = weight_proto.name
        dq_output = add_dequant_output_suffix(weight_name)
        self.model.replace_input_of_all_nodes(weight_name, dq_output)
        axis = 0 if len(weight_proto.dims) == 1 else None  # For scalar, the axis should be 0
        convert_to = 0  # Initializer is a constant, no conversion required
        self._create_fn_nodes(weight_name, dq_output, add_dequant_suffix(weight_name), axis, convert_to)

    def _add_fn_pair_for_activation(self, tensor_name: str) -&gt; None:
        q_input = tensor_name
        dq_output = add_dequant_output_suffix(tensor_name)
        if self.model.is_graph_output(tensor_name):
            q_input = add_quant_input_suffix(tensor_name)
            dq_output = tensor_name
            self.model.replace_output_of_all_nodes(tensor_name, q_input)
        else:
            self.model.replace_input_of_all_nodes(tensor_name, dq_output)

        self._create_fn_nodes(q_input, dq_output, add_dequant_suffix(tensor_name))

    def _quantize_normal_tensors(self) -&gt; None:
        for tensor_name, tensor_info in self.tensors_to_quantize.copy().items():

            if tensor_name in self.quantized_value_map.keys():
                continue

            if not tensor_info.is_shared:
                # Quantize the input
                initializer = find_by_name(tensor_name, self.model.initializer())
                if initializer:
                    self._add_fn_pair_for_weight(initializer)
                else:
                    self._add_fn_pair_for_activation(tensor_name)

                del self.tensors_to_quantize[tensor_name]

    def quantize_model(self) -&gt; Any:
        annotate_tensors = get_annotate_tensors(self.model.model)

        for node in self.model.nodes():
            if self.should_quantize_node(node):
                op_quantizer = CreateQDQQuantizer(self, node)
                op_quantizer.quantize()

        self._quantize_normal_tensors()
        # Do not support Int32 Bias in BFP mode
        # if self.quantize_bias and self.int32_bias:
        # self._quantize_bias_tensors()

        self.remove_nodes()
        dq_nodes_to_remove, q_nodes_to_remove, input_node_mapping = get_qdq_to_remove(
            self.model.model, annotate_tensors)
        pruned_model = copy.deepcopy(self.model)
        modified_annotate_input(pruned_model.model, input_node_mapping)
        pruned_model.model = remove_nodes(pruned_model.model, dq_nodes_to_remove)
        pruned_model.model = remove_nodes(pruned_model.model, q_nodes_to_remove)
        try:
            pruned_model.topological_sort()
            logger.info(&quot;Remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu).&quot;)
            self.model.model = pruned_model.model
        except Exception as e:
            logger.warning(
                f&quot;Unable to remove QuantizeLinear &amp; DequantizeLinear on certain operations(such as conv-relu). Exception: {e}&quot;
            )

        self.model.model.producer_name = __producer__
        self.model.model.producer_version = __version__

        return self.model.model</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>