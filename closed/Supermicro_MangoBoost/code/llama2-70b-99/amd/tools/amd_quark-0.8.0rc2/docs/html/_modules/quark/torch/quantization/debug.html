
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.quantization.debug &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/quantization/debug';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.quantization.debug</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.quantization.debug</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2024, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#

import numpy as np
from functools import partial
from quark.torch.quantization.tensor_quantize import FakeQuantizeBase
from quark.shares.utils.log import ScreenLogger
from pathlib import Path
import json

from typing import Tuple, Dict, Optional, List, Union, Any, Iterable, Iterator, Collection
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch
from quark.torch.quantization.nn.modules.mixin import QuantMixin
import torch.nn as nn
from contextlib import contextmanager
import os
import multiprocessing
from quark.shares.utils.import_utils import is_matplotlib_available
from transformers.feature_extraction_utils import BatchFeature

if is_matplotlib_available():
    import matplotlib.pyplot as plt

logger = ScreenLogger(__name__)

SAVE_ACTIVATIONS_HISTOGRAM = os.environ.get(&quot;QUARK_DEBUG_ACT_HIST&quot;, None) == &quot;1&quot;
DEBUG_INPUT_PICKLE = os.environ.get(&quot;QUARK_DEBUG_INPUT_PICKLE&quot;, None)


<div class="viewcode-block" id="weight_stats_hook">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.weight_stats_hook">[docs]</a>
def weight_stats_hook(module: FakeQuantizeBase, args: Tuple[Any, ...], output: torch.Tensor, module_name: str,
                      log_dir: str, n_bins: int, stats: Dict[str, Any]) -&gt; None:
    &quot;&quot;&quot;
    Hook to collect statistics on the weight and bias quantization. This hook should only be attached to `FakeQuantizeBase` layers corresponding to weight and bias quantization.

    This hook should be attached with https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook.

    Args:
        module (FakeQuantizeBase): The torch.nn.Module this hook is being attached to.
        args (Tuple[Any, ...]): The module inputs, as specified in `torch.nn.Module.register_forward_hook` documentation.
        output (torch.Tensor): The module output, as specified in `torch.nn.Module.register_forward_hook` documentation.
        module_name (str): The module name, set with `functools.partial`. This is useful to access the module name from within the hook.
        log_dir (str): The directory the weight statistics will be saved to, set with `functools.partial`.
        n_bins (int): The number of bins inthe histograms of values that are saved for visualization.
        stats (Dict[str, Any]): The dictionary used to store statistics on the weight and bias quantization. It can be set using an empty handle using `functools.partial`. Passing a dictionary is useful to access outside of the hook its content that was modified from within the hook.
    &quot;&quot;&quot;
    if module_name not in stats:
        if &quot;_weight_quantizer&quot; in module_name:
            quantizer_type = &quot;weight&quot;
        elif &quot;_bias_quantizer&quot; in module_name:
            quantizer_type = &quot;bias&quot;

        stats[module_name] = {&quot;quantizer_type&quot;: quantizer_type, &quot;module_name&quot;: module_name}

    with torch.no_grad():
        input_tensor = args[0].to(torch.float32)
        stats[module_name][&quot;shape&quot;] = input_tensor.shape
        quantized_tensor = output.to(torch.float32)

        if module.fake_quant_enabled[0] == 1 and &quot;l1_error&quot; not in stats[module_name]:
            # Maximum entropy would be with an uniform distribution, all the quantization bins filled equally.
            # We should also compute the entropy of the quantized weight distribution, but it is a bit expensive with QDQ so not doing it for now.
            reference_entropy = -torch.sum(1 / n_bins * torch.log(torch.full((n_bins, ), 1 / n_bins))).item()

            l1_error = (input_tensor - quantized_tensor).abs().max().item()

            stats[module_name][&quot;l1_error&quot;] = l1_error
            tensor_stats = {&quot;l1_error&quot;: l1_error, &quot;theorical_q_entropy&quot;: reference_entropy}

            short_module_name = module_name.replace(&quot;_weight_quantizer&quot;, &quot;weight&quot;).replace(&quot;_bias_quantizer&quot;, &quot;bias&quot;)
            with open(Path(log_dir, short_module_name + &quot;_stats.json&quot;), &#39;w&#39;) as fp:
                json.dump(tensor_stats, fp)

        if &quot;histogram&quot; not in stats[module_name]:
            histogram = torch.histogram(input_tensor.flatten().cpu(), bins=100)
            stats[module_name][&quot;histogram&quot;] = (histogram[0].numpy(), histogram[1].numpy())</div>



<div class="viewcode-block" id="activation_stats_hook">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.activation_stats_hook">[docs]</a>
def activation_stats_hook(module: FakeQuantizeBase, args: Tuple[Any, ...], output: torch.Tensor, module_name: str,
                          stats: Dict[str, Any]) -&gt; None:
    &quot;&quot;&quot;
    Hook to collect statistics on the activation quantization. This hook should only be attached to `FakeQuantizeBase` layers corresponding to input/output quantization.
    &quot;&quot;&quot;
    if module.fake_quant_enabled[0] == 0:
        quantizer_enabled = False
    else:
        quantizer_enabled = True

    assert isinstance(args, tuple)
    assert isinstance(args[0], torch.Tensor)
    input_tensor = args[0].to(torch.float32)

    if not quantizer_enabled:
        stats[module_name][&quot;ref_input_tensor&quot;] = input_tensor
        stats[module_name][&quot;ref_output_tensor&quot;] = input_tensor  # For non-quantized models, QDQ is a no-op.

        if SAVE_ACTIVATIONS_HISTOGRAM:
            if &quot;input_ref_histogram&quot; not in stats[module_name]:
                histogram = torch.histogram(input_tensor.flatten().cpu(), bins=100)
                tuple_histogram = (histogram[0].numpy(), histogram[1].numpy())
                stats[module_name][&quot;input_ref_histogram&quot;] = tuple_histogram

            if &quot;input_ref_histogram_absmean_ch0&quot; not in stats[module_name]:
                histogram = torch.histogram(input_tensor.abs().mean(dim=-2).flatten().cpu(), bins=100)
                tuple_histogram = (histogram[0].numpy(), histogram[1].numpy())
                stats[module_name][&quot;input_ref_histogram_absmean_ch0&quot;] = tuple_histogram

            if &quot;input_ref_histogram_absmean_ch1&quot; not in stats[module_name]:
                histogram = torch.histogram(input_tensor.abs().mean(dim=-1).flatten().cpu(), bins=100)
                tuple_histogram = (histogram[0].numpy(), histogram[1].numpy())
                stats[module_name][&quot;input_ref_histogram_absmean_ch1&quot;] = tuple_histogram
    else:
        ref_input_tensor = stats[module_name][&quot;ref_input_tensor&quot;]
        ref_output_tensor = stats[module_name][&quot;ref_output_tensor&quot;]
        quantized_tensor = output.to(torch.float32)

        def reldiff(x: torch.Tensor, ref: torch.Tensor, eps: float = 1e-12) -&gt; float:
            # Compute relative difference in high precision.
            ref = ref.to(torch.float32)
            x = x.to(torch.float32)

            reldiff = (x - ref).abs() / (ref.abs() + eps)
            assert torch.all(torch.isfinite(reldiff))
            return reldiff.mean().item()

        # Compare input tensor of FakeQuantizeBase to reference input tensor (non-quantized model).
        stats[module_name][&quot;l1_ref_input&quot;].append(reldiff(input_tensor, ref_input_tensor))

        # Compare output tensor of FakeQuantizeBase to reference output tensor (non-quantized model).
        stats[module_name][&quot;l1_ref_output&quot;].append(reldiff(quantized_tensor, ref_output_tensor))

        # Compare output tensor of FakeQuantizeBase to its input tensor.
        stats[module_name][&quot;l1_io_error&quot;].append(reldiff(quantized_tensor, input_tensor))

        if SAVE_ACTIVATIONS_HISTOGRAM and &quot;input_histogram&quot; not in stats[module_name]:
            histogram = torch.histogram(input_tensor.flatten().cpu(), bins=100)
            stats[module_name][&quot;input_histogram&quot;] = (histogram[0].numpy(), histogram[1].numpy())

        if SAVE_ACTIVATIONS_HISTOGRAM and &quot;input_qdq_histogram&quot; not in stats[module_name]:
            histogram = torch.histogram(quantized_tensor.flatten().cpu(), bins=100)
            stats[module_name][&quot;input_qdq_histogram&quot;] = (histogram[0].numpy(), histogram[1].numpy())</div>



<div class="viewcode-block" id="distribution_plot">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.distribution_plot">[docs]</a>
def distribution_plot(
        histogram: Tuple[np.ndarray, np.ndarray],  # type: ignore[type-arg]
        save_path: Union[str, Path],
        title: str) -&gt; None:
    &quot;&quot;&quot;
    Plots and saves a bar plot using the bins and distribution from `histogram`. This is useful to save a given layer distribution, error, etc.
    &quot;&quot;&quot;
    hist, bins = histogram
    width = bins[1] - bins[0]
    center = (bins[:-1] + bins[1:]) / 2
    plt.clf()
    plt.bar(center, hist, edgecolor=&#39;black&#39;, fill=True, align=&#39;center&#39;, width=width, linewidth=0.5)
    plt.axvline(bins.min(), color=&quot;r&quot;, label=&quot;min tensor value&quot;)
    plt.axvline(bins.max(), color=&quot;r&quot;, label=&quot;max tensor value&quot;)
    plt.legend()
    plt.title(title, fontsize=14)
    plt.grid()
    plt.tight_layout()
    plt.savefig(save_path, dpi=300)</div>



<div class="viewcode-block" id="barplot">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.barplot">[docs]</a>
def barplot(labels: Collection[str], values: Iterable[float], name: str, log_dir: Union[str, Path]) -&gt; None:
    &quot;&quot;&quot;
    Plots and saves a bar plot summary of values, each value having a label. This is useful to plot a summary of e.g. quantization error over many layers.
    &quot;&quot;&quot;
    x_range = range(len(labels))
    plt.clf()
    plt.figure(figsize=(2 + int(0.2 * len(x_range)), 5))  # Dynamic x axis size to avoid having its ticks overlap.
    plt.bar(x_range, values, edgecolor=&#39;black&#39;)
    plt.xticks(x_range, labels, rotation=65, fontsize=9, ha=&#39;right&#39;, rotation_mode=&#39;anchor&#39;)
    plt.xlim([-1, len(x_range)])
    plt.grid()
    plt.savefig(Path(log_dir, f&quot;{name}.png&quot;), dpi=300, bbox_inches=&quot;tight&quot;)</div>



<div class="viewcode-block" id="save_distribution_histogram">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.save_distribution_histogram">[docs]</a>
def save_distribution_histogram(module_name: str, tensor_stats: Dict[str, Any], log_dir: str) -&gt; None:
    &quot;&quot;&quot;
    Saves bar plots of activations. Utility function to be used by multiprocessing.
    &quot;&quot;&quot;
    input_shape_str = str(tuple(tensor_stats[&quot;ref_input_tensor&quot;].shape))

    quantizer_type = tensor_stats[&quot;quantizer_type&quot;]

    module_name = module_name.replace(&quot;_input_quantizer&quot;, &quot;&quot;).replace(&quot;_output_quantizer&quot;, &quot;&quot;)

    # Plot the histogram of values of the reference inputs at the point the FakeQuantize layer is inserted (input or output of a module).
    histogram_file = Path(log_dir, module_name + f&quot;{quantizer_type}_ref_histogram.png&quot;)
    distribution_plot(tensor_stats[&quot;input_ref_histogram&quot;],
                      histogram_file,
                      title=module_name + f&quot;\nref input histogram (tensor={input_shape_str})&quot;)

    # Plot the histogram of values of the reference inputs at the point the FakeQuantize layer is inserted, absmean reduced on the -2 dimension.
    histogram_file = Path(log_dir, module_name + f&quot;{quantizer_type}_ref_histogram_absmean_ch0.png&quot;)
    reduced_shape_str = str(
        tuple(tensor_stats[&#39;ref_input_tensor&#39;].shape[:-2] + tensor_stats[&#39;ref_input_tensor&#39;].shape[-1:]))
    distribution_plot(
        tensor_stats[&quot;input_ref_histogram_absmean_ch0&quot;],
        histogram_file,
        title=module_name +
        f&quot;\nref input histogram, absmean ch0 (tensor={input_shape_str})\nreduction over -2 dim (tensor={reduced_shape_str})&quot;
    )

    # Plot the histogram of values of the reference inputs at the point the FakeQuantize layer is inserted, absmean reduced on the -1 dimension.
    histogram_file = Path(log_dir, module_name + f&quot;{quantizer_type}_ref_histogram_absmean_ch1.png&quot;)
    reduced_shape_str = str(tuple(tensor_stats[&#39;ref_input_tensor&#39;].shape[:-1]))
    distribution_plot(
        tensor_stats[&quot;input_ref_histogram_absmean_ch1&quot;],
        histogram_file,
        title=module_name +
        f&quot;\nref input histogram, absmean ch1 (tensor={input_shape_str})\nreduction over -1 dim (tensor={reduced_shape_str})&quot;
    )

    # Plot the histogram of values of the activation inputs to the FakeQuantize layer.
    histogram_file = Path(log_dir, module_name + f&quot;{quantizer_type}_histogram.png&quot;)
    distribution_plot(tensor_stats[&quot;input_histogram&quot;],
                      histogram_file,
                      title=module_name + f&quot;\ninput histogram (tensor={input_shape_str})&quot;)

    # Plot the histogram of values of the activation outputs of the FakeQuantize layer (after QDQ).
    histogram_file = Path(log_dir, module_name + f&quot;{quantizer_type}_qdq_histogram.png&quot;)
    distribution_plot(tensor_stats[&quot;input_qdq_histogram&quot;],
                      histogram_file,
                      title=module_name + f&quot;\nqdq input histogram (tensor={input_shape_str})&quot;)</div>



<div class="viewcode-block" id="summarize_weight">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.summarize_weight">[docs]</a>
def summarize_weight(stats: Dict[str, Any], log_dir: Path) -&gt; None:
    &quot;&quot;&quot;
    Saves a histogram of the distribution of the weight tensor for each weight tracked. Saves as well a summary plot of the L1 quantization error over all the different weight tensors.
    &quot;&quot;&quot;
    l1_errors_weights = {}

    distribution_plot_args = []

    # Plot the non-quantized weight distribution per layer.
    for module_name, tensor_stats in tqdm(stats.items(), desc=&quot;plot weight distribution&quot;):
        if tensor_stats[&quot;quantizer_type&quot;] in [&quot;weight&quot;, &quot;bias&quot;]:
            module_name_short = module_name.replace(&quot;._weight_quantizer&quot;, &quot;_w&quot;).replace(&quot;._bias_quantizer&quot;, &quot;_b&quot;)

            l1_errors_weights[module_name_short] = tensor_stats[&quot;l1_error&quot;]

            module_name = module_name.replace(&quot;._weight_quantizer&quot;, &quot;&quot;).replace(&quot;._bias_quantizer&quot;, &quot;&quot;)
            histogram_file = Path(log_dir, module_name + &quot;.weight.png&quot;)
            shape_str = str(tuple(tensor_stats[&quot;shape&quot;]))
            distribution_plot_args.append(
                (tensor_stats[&quot;histogram&quot;], histogram_file, module_name + f&quot;\n weight histogram (tensor= {shape_str})&quot;))

    start_method = multiprocessing.get_start_method()
    multiprocessing.set_start_method(&#39;spawn&#39;, force=True)
    pool = multiprocessing.Pool(processes=32)

    for _ in tqdm(pool.starmap(distribution_plot, distribution_plot_args), total=len(distribution_plot_args)):
        pass

    pool.close()
    pool.join()

    if start_method is not None:
        multiprocessing.set_start_method(start_method, force=True)

    # Plot the summary of weight quantization error over each layers.
    barplot(l1_errors_weights.keys(), l1_errors_weights.values(), name=&quot;summary_weight_error&quot;, log_dir=log_dir)</div>



<div class="viewcode-block" id="summarize_activation">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.summarize_activation">[docs]</a>
def summarize_activation(stats: Dict[str, Any], log_dir: Path) -&gt; None:
    &quot;&quot;&quot;
    Saves a summary over all activations of the error between the quantized / non-quantized model.
    &quot;&quot;&quot;
    l1_errors_ref_input = {}
    l1_errors_ref_output = {}
    l1_io_error = {}

    # Average the activation metrics over all the input samples used in the statistics collection.
    for name, tensor_stats in stats.items():
        if tensor_stats[&quot;quantizer_type&quot;] in [&quot;input&quot;, &quot;output&quot;]:
            l1_errors_ref_input[name] = np.mean(tensor_stats[&quot;l1_ref_input&quot;])
            l1_io_error[name] = np.mean(tensor_stats[&quot;l1_io_error&quot;])
            l1_errors_ref_output[name] = np.mean(tensor_stats[&quot;l1_ref_output&quot;])

    # Plot the summary of relative error of input tensor of FakeQuantizeBase compared to reference input tensor (non-quantized model).
    labels = [
        key.replace(&quot;._input_quantizer&quot;, &quot;_i&quot;).replace(&quot;._output_quantizer&quot;, &quot;_o&quot;)
        for key in l1_errors_ref_input.keys()
    ]
    barplot(labels, l1_errors_ref_input.values(), name=&quot;summary_ref_input_error&quot;, log_dir=log_dir)

    # Plot the summary of relative error of output tensor of FakeQuantizeBase compared to reference output tensor (non-quantized model).
    labels = [
        key.replace(&quot;._input_quantizer&quot;, &quot;_i&quot;).replace(&quot;._output_quantizer&quot;, &quot;_o&quot;)
        for key in l1_errors_ref_output.keys()
    ]
    barplot(labels, l1_errors_ref_output.values(), name=&quot;summary_ref_output_error&quot;, log_dir=log_dir)

    # Plot the summary of relative error of output tensor of FakeQuantizeBase compared to its input tensor.
    labels = [key.replace(&quot;._input_quantizer&quot;, &quot;_i&quot;).replace(&quot;._output_quantizer&quot;, &quot;_o&quot;) for key in l1_io_error.keys()]
    barplot(labels, l1_io_error.values(), name=&quot;summary_io_quantization_error&quot;, log_dir=log_dir)

    if SAVE_ACTIVATIONS_HISTOGRAM:
        save_distribution_args = [(module_name, tensor_stats, log_dir) for module_name, tensor_stats in stats.items()
                                  if tensor_stats[&quot;quantizer_type&quot;] in [&quot;input&quot;, &quot;output&quot;]]

        start_method = multiprocessing.get_start_method()
        multiprocessing.set_start_method(&#39;spawn&#39;, force=True)
        pool = multiprocessing.Pool(processes=32)

        for _ in tqdm(pool.starmap(save_distribution_histogram, save_distribution_args),
                      total=len(save_distribution_args)):
            pass

        pool.close()
        pool.join()

        if start_method is not None:
            multiprocessing.set_start_method(start_method, force=True)</div>



@contextmanager
<div class="viewcode-block" id="insert_stats_hooks">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.insert_stats_hooks">[docs]</a>
def insert_stats_hooks(model: nn.Module, stats: Dict[str, Any], log_dir: Path) -&gt; Iterator[None]:
    &quot;&quot;&quot;
    Inserts the hooks to track statistics about quantization error.
    &quot;&quot;&quot;
    hooks = []
    for name, module in model.named_modules():
        if isinstance(module, FakeQuantizeBase):
            if &quot;weight_quantizer&quot; in name or &quot;bias_quantizer&quot; in name:
                hook = module.register_forward_hook(
                    partial(weight_stats_hook, module_name=name, log_dir=log_dir, n_bins=256, stats=stats))
                hooks.append(hook)
            elif &quot;input_quantizer&quot; in name or &quot;output_quantizer&quot; in name:
                hook = module.register_forward_hook(partial(activation_stats_hook, module_name=name, stats=stats))
                hooks.append(hook)

    yield

    for hook in hooks:
        hook.remove()</div>



<div class="viewcode-block" id="collect_quantization_statistics">
<a class="viewcode-back" href="../../../../autoapi/quark/torch/quantization/debug/index.html#quark.torch.quantization.debug.collect_quantization_statistics">[docs]</a>
def collect_quantization_statistics(model: nn.Module, dataloader: Optional[Union[DataLoader[torch.Tensor],
                                                                                 DataLoader[List[Dict[str,
                                                                                                      torch.Tensor]]],
                                                                                 DataLoader[Dict[str, torch.Tensor]],
                                                                                 DataLoader[List[BatchFeature]]]],
                                    stats: Dict[str, Any], log_dir: Path) -&gt; None:
    &quot;&quot;&quot;
    Collects (through the hooks attached to the model) statistics on the operators inputs/outputs to compute quantization error metrics, as well as on the weights.

    Moreover, this function writes to disk statistics, distribution and summary bar charts for the quantization of weights and activations.
    &quot;&quot;&quot;
    if not is_matplotlib_available():
        raise ImportError(
            &quot;The package `matplotlib` is required to collect quantization error statistics and plot them. Please install `matplotlib` (example: `pip install matplotlib`).&quot;
        )

    if DEBUG_INPUT_PICKLE is None:
        if SAVE_ACTIVATIONS_HISTOGRAM:
            logger.warning(
                &quot;The histograms of activations / activation errors are saved only for the first item in the dataloader. Please make sure that this input is meaningful, and bear in mind that this item was used as well for calibration. In order to use specific inputs to collect activation quantization statistics, please specify the environment variable `QUARK_DEBUG_INPUT_PICKLE` to a file containing the reference tensor or dict inputs saved with `torch.save`.&quot;
            )

        input_iterable: Optional[Iterable[Any]] = dataloader
    else:
        input_dict = torch.load(DEBUG_INPUT_PICKLE, weights_only=True)
        input_iterable = [input_dict]

    if input_iterable is not None:
        for module_name, module in model.named_modules():
            if isinstance(module, FakeQuantizeBase) and (&quot;input_quantizer&quot; in module_name
                                                         or &quot;output_quantizer&quot; in module_name):
                if &quot;input_quantizer&quot; in module_name:
                    quantizer_type = &quot;input&quot;
                else:
                    quantizer_type = &quot;output&quot;

                if module_name not in stats:
                    stats[module_name] = {
                        &quot;l1_ref_input&quot;: [],
                        &quot;l1_io_error&quot;: [],
                        &quot;l1_ref_output&quot;: [],
                        &quot;quantizer_type&quot;: quantizer_type
                    }
                else:
                    stats[module_name][&quot;l1_ref_input&quot;] = []
                    stats[module_name][&quot;l1_io_error&quot;] = []
                    stats[module_name][&quot;l1_ref_output&quot;] = []
                    stats[module_name][&quot;quantizer_type&quot;] = quantizer_type

        for data in tqdm(input_iterable, desc=&quot;Debug forward&quot;):
            for module in model.modules():
                if isinstance(module, FakeQuantizeBase):
                    module.disable_fake_quant()

            with torch.no_grad():
                if isinstance(data, (dict, BatchFeature)):
                    _ = model(**data)
                else:
                    _ = model(data)

            for module in model.modules():
                if isinstance(module, FakeQuantizeBase):
                    module.enable_fake_quant()

            with torch.no_grad():
                if isinstance(data, (dict, BatchFeature)):
                    _ = model(**data)
                else:
                    _ = model(data)
    else:
        for module in model.modules():
            if isinstance(module, FakeQuantizeBase):
                module.enable_fake_quant()

        for module in tqdm(model.modules()):
            if isinstance(module, QuantMixin):
                if module._weight_quantizer is not None:
                    module.get_quant_weight(module.weight)
                if module._bias_quantizer is not None:
                    module.get_quant_bias(module.bias)

    summarize_weight(stats, log_dir)
    if input_iterable is not None:
        summarize_activation(stats, log_dir)</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>