defaults: []

# benchmark details
benchmark_name: llama2-70b-interactive
scenario: server
test_mode: performance
server_version: sync

env_config:
  VLLM_LOGGING_LEVEL: "ERROR"
  HARNESS_GC_LIMIT: 100000

# configuration related to the LLM model.
llm_config:
  model: /model/llama2-70b-chat-hf/fp8_quantized
  tensor_parallel_size: 1
  num_scheduler_steps: 8
  quantization: fp8
  max_model_len: 2048
  swap_space: 0
  gpu_memory_utilization: 1.0
  max_seq_len_to_capture: 1
  enforce_eager: True
  disable_custom_all_reduce: True
  max_num_batched_tokens: 40960
  max_num_seqs: 600
  enable_chunked_prefill: False
  block_size: 32
  enable_prefix_caching: False

# configuration related to the sampling params
sampling_params:
  temperature: 0.0
  min_tokens: 1
  max_tokens: 1024
  ignore_eos: False
  detokenize: False

# configuration related to the harness tests.
harness_config:
  dataset_path: /data/processed-openorca/open_orca_gpt4_tokenized_llama.sampled_24576.pkl
  mlperf_conf_path: /app/mlperf_inference/mlperf.conf
  user_conf_path: /lab-mlperf-inference/code/user_mi300x_interactive.conf
  target_qps: -1 # 30
  total_sample_count: 24576
  output_log_dir: /app/logs
  enable_log_trace: False
  warmup_duration: 0
  data_parallel_size: 8
  warm_up_sample_count_per_server: 50
  enable_batcher: False
  batcher_threshold: 0.1
  gpu_batch_size: 128
  schedule_algo: shortest_queue_with_tokens
  load_balance_token_weight: 0.01
  load_balance_window_size: 12