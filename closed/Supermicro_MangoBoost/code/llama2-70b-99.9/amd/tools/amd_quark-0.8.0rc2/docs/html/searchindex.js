Search.setIndex({"alltitles": {"1. Image Classification Task PTQ/QAT Results": [[319, "image-classification-task-ptq-qat-results"]], "1. Install AMD Quark": [[302, "install-amd-quark"]], "1. Install Quark:": [[338, "install-quark"]], "1. PPL in LM-Evaluation-Harness vs PPL Feature in Table": [[310, "ppl-in-lm-evaluation-harness-vs-ppl-feature-in-table"]], "1. Quantizing to Other Precisions": [[301, "quantizing-to-other-precisions"]], "1. Retrieve dataset from LM-Eval-Harness": [[312, "retrieve-dataset-from-lm-eval-harness"]], "1.1 Quantizing Float32 Models to Int16 or Int32": [[301, "quantizing-float32-models-to-int16-or-int32"]], "1.2 Quantizing Float32 Models to Float16 or BFloat16": [[301, "quantizing-float32-models-to-float16-or-bfloat16"]], "1.3 Quantizing Float32 Models to BFP16": [[301, "quantizing-float32-models-to-bfp16"]], "1.4 Quantizing Float32 Models to Mixed Data Formats": [[301, "quantizing-float32-models-to-mixed-data-formats"]], "2. Export Pretrained Model-Of-Interest to ONNX": [[312, "export-pretrained-model-of-interest-to-onnx"]], "2. ONNX Exported Models": [[310, "onnx-exported-models"]], "2. Object Detection Task PTQ/QAT Results": [[319, "object-detection-task-ptq-qat-results"]], "2. Quantizing Float16 Models": [[301, "quantizing-float16-models"]], "2. Set the model": [[302, "set-the-model"]], "2. Set the model:": [[338, "set-the-model"]], "3. Retrieve OGA references for Pretrained ONNX Model": [[312, "retrieve-oga-references-for-pretrained-onnx-model"]], "3. Set the quantization configuration": [[302, "set-the-quantization-configuration"]], "3. Set the quantization configuration:": [[338, "set-the-quantization-configuration"]], "3. Support for VLMs": [[310, "support-for-vlms"]], "4. Do quantization": [[338, "do-quantization"]], "4. Get Baseline Evaluation Scores on Pretrained ONNX Model": [[312, "get-baseline-evaluation-scores-on-pretrained-onnx-model"]], "4. LLM_Eval_Harness vs LLM_Eval_Haness_Offline": [[310, "llm-eval-harness-vs-llm-eval-haness-offline"]], "4. Set up the calibration data (this is required for weight only and dynamic quantization as well)": [[302, "set-up-the-calibration-data-this-is-required-for-weight-only-and-dynamic-quantization-as-well"]], "5. Apply the quantization": [[302, "apply-the-quantization"]], "5. Evaluate an optimized ONNX Model": [[312, "evaluate-an-optimized-onnx-model"]], "5. Export Evaluation Results": [[310, "export-evaluation-results"]], "6. Compare Scores from Step 4 and Step 5": [[312, "compare-scores-from-step-4-and-step-5"]], "AMD Quark APIs for ONNX": [[288, null]], "AMD Quark for ONNX": [[258, null], [290, "amd-quark-for-onnx"]], "AMD Quark for ONNX: Streamlined Quantization for ONNX models": [[249, "amd-quark-for-onnx-streamlined-quantization-for-onnx-models"]], "AMD Quark for PyTorch": [[304, null]], "AMD Quark for PyTorch: Flexible and Efficient Quantization for PyTorch Models": [[249, "amd-quark-for-pytorch-flexible-and-efficient-quantization-for-pytorch-models"]], "AMD Quark for Pytorch": [[331, "amd-quark-for-pytorch"]], "Accelerate with GPUs": [[285, null]], "Accessing ONNX Examples": [[289, null]], "Accessing PyTorch Examples": [[330, null]], "Accuracy Improvement Algorithms": [[256, null]], "Activation/weight smoothing (SmoothQuant)": [[337, null]], "AdaQuant": [[252, "adaquant"], [252, "id2"]], "AdaRound": [[252, "adaround"], [252, "id1"]], "Adding Calibration Datasets": [[260, null], [305, null]], "Apply Quantization Algorithms": [[336, "apply-quantization-algorithms"]], "Arguments": [[252, "arguments"], [253, "arguments"], [254, "arguments"], [255, "arguments"]], "Attributes": [[41, "attributes"], [228, "attributes"]], "Auto Search for Best Practice of RyzenAI ONNX Model Quantization": [[279, null], [280, null]], "Auto search for RyzenAI quantization": [[284, "auto-search-for-ryzenai-quantization"]], "Automatic Mixed Precision based on Sensitivity Analysis": [[298, "automatic-mixed-precision-based-on-sensitivity-analysis"]], "Automatic Search for Model Quantization": [[299, null]], "BF16 quantization in AMD Quark for ONNX": [[294, "bf16-quantization-in-amd-quark-for-onnx"]], "BFP16 (Block floating point) Quantization": [[259, null], [338, null]], "BFP16 Models Inference": [[285, "bfp16-models-inference"]], "BFP16 Quantization": [[266, "bfp16-quantization"]], "BFP16 Quantization with ADAQUANT": [[266, "bfp16-quantization-with-adaquant"]], "Basic Example": [[258, "basic-example"], [304, "basic-example"]], "Basic Usage": [[248, null]], "Benchmark": [[309, "benchmark"]], "Benefits of AdaRound and AdaQuant": [[252, "benefits-of-adaround-and-adaquant"]], "Benefits of BFP16 Quantization": [[259, "benefits-of-bfp16-quantization"], [295, "benefits-of-bfp16-quantization"]], "Benefits of Mixed Precision Quantization": [[298, "benefits-of-mixed-precision-quantization"]], "Best Practice for Ryzen AI in AMD Quark ONNX": [[292, null]], "Best Practice for Ryzen AI in Quark ONNX": [[284, null], [286, null], [287, null]], "Best Practices for Post-Training Quantization (PTQ)": [[336, null]], "Block Floating Point (BFP) Example": [[266, null]], "Brevitas Integration": [[308, null], [308, "id1"]], "Bridge from Quark to llama.cpp": [[320, null]], "C++ Compilation Issues": [[331, "c-compilation-issues"]], "Calibration": [[285, "calibration"]], "Calibration Data Path for AMD Quark Quantizer": [[260, "calibration-data-path-for-amd-quark-quantizer"]], "Calibration Methods": [[261, null], [306, null]], "Calibration and Export": [[309, "calibration-and-export"]], "Call the Auto Search Process": [[279, "call-the-auto-search-process"], [280, "call-the-auto-search-process"]], "Class DataReader for AMD Quark Quantizer": [[260, "class-datareader-for-amd-quark-quantizer"]], "Classes": [[1, "classes"], [3, "classes"], [4, "classes"], [5, "classes"], [6, "classes"], [7, "classes"], [8, "classes"], [9, "classes"], [13, "classes"], [14, "classes"], [15, "classes"], [16, "classes"], [17, "classes"], [21, "classes"], [22, "classes"], [25, "classes"], [26, "classes"], [27, "classes"], [31, "classes"], [32, "classes"], [33, "classes"], [34, "classes"], [39, "classes"], [50, "classes"], [51, "classes"], [53, "classes"], [54, "classes"], [55, "classes"], [56, "classes"], [57, "classes"], [59, "classes"], [63, "classes"], [66, "classes"], [71, "classes"], [76, "classes"], [111, "classes"], [118, "classes"], [121, "classes"], [122, "classes"], [123, "classes"], [124, "classes"], [127, "classes"], [128, "classes"], [130, "classes"], [131, "classes"], [132, "classes"], [134, "classes"], [135, "classes"], [136, "classes"], [139, "classes"], [140, "classes"], [143, "classes"], [144, "classes"], [149, "classes"], [150, "classes"], [151, "classes"], [156, "classes"], [159, "classes"], [160, "classes"], [162, "classes"], [177, "classes"], [178, "classes"], [181, "classes"], [182, "classes"], [183, "classes"], [186, "classes"], [188, "classes"], [192, "classes"], [193, "classes"], [194, "classes"], [195, "classes"], [198, "classes"], [199, "classes"], [202, "classes"], [206, "classes"], [209, "classes"], [213, "classes"], [215, "classes"], [217, "classes"], [222, "classes"], [229, "classes"], [232, "classes"], [233, "classes"], [234, "classes"], [235, "classes"], [236, "classes"], [237, "classes"], [240, "classes"], [241, "classes"], [242, "classes"], [243, "classes"]], "Components": [[299, "components"]], "Conclusion": [[299, "conclusion"], [302, "conclusion"], [308, "conclusion"]], "Configuring ONNX Quantization": [[300, null]], "Configuring PyTorch Quantization": [[341, null]], "Considerations for Quantization": [[251, "considerations-for-quantization"]], "Context: Uniform Integer Quantization": [[303, "context-uniform-integer-quantization"]], "Convert a A8W8 NPU Model to a A8W8 CPU Model": [[293, "convert-a-a8w8-npu-model-to-a-a8w8-cpu-model"]], "Convert a Float16 Model to a BFP16 Model": [[293, "convert-a-float16-model-to-a-bfp16-model"]], "Convert a Float16 Model to a BFloat16 Model": [[293, "convert-a-float16-model-to-a-bfloat16-model"]], "Convert a Float16 Model to a Float32 Model": [[293, "convert-a-float16-model-to-a-float32-model"]], "Convert a Float32 Model to a BFP16 Model": [[293, "convert-a-float32-model-to-a-bfp16-model"]], "Convert a Float32 Model to a BFloat16 Model": [[293, "convert-a-float32-model-to-a-bfloat16-model"]], "Convert a NCHW input Model to a NHWC Model": [[293, "convert-a-nchw-input-model-to-a-nhwc-model"]], "Convert a U16U8 Quantized Model to a U8U8 Model": [[293, "convert-a-u16u8-quantized-model-to-a-u8u8-model"]], "Customized Configurations": [[300, "customized-configurations"]], "Dataloader with Dataset as torch.Tensor": [[305, "dataloader-with-dataset-as-torch-tensor"]], "Dataloader with Dict[str, torch.Tensor]": [[305, "dataloader-with-dict-str-torch-tensor"]], "Dataloader with List[Dict[str, torch.Tensor]] or List[torch.Tensor]": [[305, "dataloader-with-list-dict-str-torch-tensor-or-list-torch-tensor"]], "Dataset Files": [[309, "dataset-files"]], "Debugging quantization degradation in AMD Quark": [[307, null]], "Detailed Code": [[265, "detailed-code"]], "Diffusion Model Quantization using Quark": [[309, null]], "Dumping the Simulation Results": [[291, "dumping-the-simulation-results"]], "Dynamic Quantization": [[289, null]], "Dynamic Quantization for Llama-2-7b": [[272, null]], "Dynamic Quantization for OPT-125M": [[273, null]], "Entropy Calibration Method": [[261, "entropy-calibration-method"]], "Environment Issues": [[331, "environment-issues"]], "Environment Setup": [[285, "environment-setup"]], "Evaluate Accuracy Between Two Image Folders": [[293, "evaluate-accuracy-between-two-image-folders"]], "Evaluating the Quantized Model": [[291, "evaluating-the-quantized-model"]], "Evaluation": [[266, "evaluation"], [267, "evaluation"], [268, "evaluation"], [269, "evaluation"], [270, "evaluation"], [271, "evaluation"], [272, "evaluation"], [273, "evaluation"], [274, "evaluation"], [275, "evaluation"], [276, "evaluation"], [277, "evaluation"], [278, "evaluation"], [281, "evaluation"], [282, "evaluation"], [283, "evaluation"]], "Example": [[253, "example"], [254, "example"], [255, "example"], [259, "example"], [297, "example"]], "Example 1": [[318, "example-1"]], "Example 2": [[318, "example-2"]], "Example 3": [[318, "example-3"]], "Example Code:": [[260, "example-code"], [260, "id1"]], "Example of GGUF Exporting": [[322, "example-of-gguf-exporting"]], "Example of HF Format Exporting": [[323, "example-of-hf-format-exporting"]], "Example of HF Format Importing": [[323, "example-of-hf-format-importing"]], "Example of Loading in Eager Mode": [[335, "example-of-loading-in-eager-mode"]], "Example of Loading in FX-graph Mode": [[335, "example-of-loading-in-fx-graph-mode"]], "Example of Onnx Exporting": [[325, "example-of-onnx-exporting"]], "Example of Quark Format Exporting": [[326, "example-of-quark-format-exporting"]], "Example of Quark Format Importing": [[326, "example-of-quark-format-importing"]], "Example of Saving in Eager Mode": [[335, "example-of-saving-in-eager-mode"]], "Example of Saving in FX-graph Mode": [[335, "example-of-saving-in-fx-graph-mode"]], "Examples": [[252, "examples"], [295, "examples"], [296, "examples"]], "Exclude Outlier Layers": [[336, "exclude-outlier-layers"]], "Expanding with more models": [[340, "expanding-with-more-models"]], "Experiment Results": [[319, "experiment-results"]], "Experiment results": [[320, "id4"]], "Experiments": [[320, "experiments"]], "Exporting PyTorch Models to ONNX": [[291, "exporting-pytorch-models-to-onnx"]], "Exporting Quantized Models": [[321, null]], "Exporting Using ONNX Runtime Gen AI Model Builder": [[324, null], [324, "id1"]], "Exporting to HuggingFace Format": [[323, "exporting-to-huggingface-format"]], "Extension Feature Design": [[308, "extension-feature-design"]], "Extensions for PyTorch": [[327, null]], "Fake Quantization": [[251, "fake-quantization"]], "Fast Finetune": [[252, "fast-finetune"], [285, "fast-finetune"]], "Fine-Grained User Guide": [[319, "fine-grained-user-guide"]], "Flow Diagram": [[299, "flow-diagram"]], "For Multi-Input Models": [[265, "for-multi-input-models"]], "For Multi-Input Models:": [[260, "for-multi-input-models"]], "For Single-Input Models": [[265, "for-single-input-models"]], "For Single-Input Models:": [[260, "for-single-input-models"]], "Frequently Asked Questions (FAQ)": [[290, null], [331, null]], "Full List of Quantization Configuration Features": [[257, null]], "Functions": [[1, "functions"], [3, "functions"], [5, "functions"], [9, "functions"], [11, "functions"], [18, "functions"], [20, "functions"], [22, "functions"], [32, "functions"], [36, "functions"], [41, "functions"], [53, "functions"], [55, "functions"], [59, "functions"], [61, "functions"], [72, "functions"], [74, "functions"], [78, "functions"], [86, "functions"], [91, "functions"], [92, "functions"], [93, "functions"], [99, "functions"], [106, "functions"], [107, "functions"], [108, "functions"], [110, "functions"], [114, "functions"], [119, "functions"], [120, "functions"], [138, "functions"], [140, "functions"], [141, "functions"], [144, "functions"], [147, "functions"], [150, "functions"], [154, "functions"], [169, "functions"], [180, "functions"], [188, "functions"], [190, "functions"], [198, "functions"], [199, "functions"], [203, "functions"], [205, "functions"], [211, "functions"], [212, "functions"], [218, "functions"], [219, "functions"], [220, "functions"], [221, "functions"], [225, "functions"], [226, "functions"], [228, "functions"], [229, "functions"], [230, "functions"], [235, "functions"], [238, "functions"], [244, "functions"]], "GGUF Exporting": [[322, null]], "Get example code and script": [[319, "get-example-code-and-script"]], "Getting Started": [[310, "getting-started"]], "Guidelines": [[278, "guidelines"]], "Hardware Mapping": [[303, "hardware-mapping"]], "How BFP16 works in Quark": [[338, "how-bfp16-works-in-quark"]], "How Does It Work": [[320, "how-does-it-work"]], "How Does Quark Do Quantization": [[320, "how-does-quark-do-quantization"]], "How are These Two-Level Scales Obtained?": [[303, "how-are-these-two-level-scales-obtained"]], "How are the scales calculated?": [[302, "how-are-the-scales-calculated"]], "How are the scales used?": [[302, "how-are-the-scales-used"]], "How does SmoothQuant work?": [[337, "how-does-smoothquant-work"]], "How is the tensor turned into blocks?": [[302, "how-is-the-tensor-turned-into-blocks"]], "How to Enable AdaRound / AdaQuant in AMD Quark?": [[252, "how-to-enable-adaround-adaquant-in-amd-quark"]], "How to Enable MX Quantization in AMD Quark for ONNX?": [[297, "how-to-enable-mx-quantization-in-amd-quark-for-onnx"]], "How to Enable Mixed Precision in AMD Quark for ONNX?": [[298, "how-to-enable-mixed-precision-in-amd-quark-for-onnx"]], "How to Further Improve the Accuracy for BF16 Quantization?": [[294, "how-to-further-improve-the-accuracy-for-bf16-quantization"]], "How to Further Improve the Accuracy of a MX Quantized Model?": [[297, "how-to-further-improve-the-accuracy-of-a-mx-quantized-model"]], "How to Further Improve the Accuracy of a MX9 Quantized Model?": [[296, "how-to-further-improve-the-accuracy-of-a-mx9-quantized-model"]], "How to Use GGUF Export in Quark": [[320, "how-to-use-gguf-export-in-quark"]], "How to enable BFP16 quantization in AMD Quark for ONNX?": [[259, "how-to-enable-bfp16-quantization-in-amd-quark-for-onnx"]], "How to enable BFP16 quantization in Quark for ONNX?": [[295, "how-to-enable-bfp16-quantization-in-quark-for-onnx"]], "How to enable MX9 quantization in AMD Quark for ONNX?": [[296, "how-to-enable-mx9-quantization-in-amd-quark-for-onnx"]], "How to further improve the accuracy of a BFP16 quantized model in AMD Quark for ONNX?": [[259, "how-to-further-improve-the-accuracy-of-a-bfp16-quantized-model-in-amd-quark-for-onnx"]], "How to further improve the accuracy of a BFP16 quantized model in Quark for ONNX?": [[295, "how-to-further-improve-the-accuracy-of-a-bfp16-quantized-model-in-quark-for-onnx"]], "How to use BFP16 in Quark": [[338, "how-to-use-bfp16-in-quark"]], "How to use MX in AMD Quark": [[302, "how-to-use-mx-in-amd-quark"]], "HuggingFace Format": [[323, null]], "Image Classification": [[289, null]], "Important Details": [[310, "important-details"]], "Importing HuggingFace Format": [[323, "importing-huggingface-format"]], "Improving Model Accuracy": [[289, null]], "Inference": [[287, "inference"]], "Install from ZIP": [[250, "install-from-zip"]], "Installation": [[250, "installation"], [316, "installation"]], "Installation Guide": [[250, null]], "Installation Verification": [[250, "installation-verification"]], "Integer Quantization": [[251, "integer-quantization"]], "Integration with AMD Pytorch-light (APL)": [[318, null]], "Introduction": [[278, "introduction"], [294, null], [295, null], [296, null], [302, "introduction"], [318, "introduction"], [320, "introduction"], [338, "introduction"], [340, "introduction"]], "Key Concepts": [[259, "key-concepts"], [295, "key-concepts"]], "Key Features": [[249, "key-features"], [249, "id1"]], "LM-Evaluation Harness (Offline)": [[312, null]], "LM-Evaluation Harness Evaluations": [[311, null]], "LM-Evaluation-Harness on ONNX Models": [[311, "lm-evaluation-harness-on-onnx-models"]], "LM-Evaluation-Harness on Torch Models": [[311, "lm-evaluation-harness-on-torch-models"]], "Language Model Evaluations in Quark": [[310, null]], "Language Model Optimization": [[328, null]], "Language Model Post Training Quantization (PTQ) Using Quark": [[316, null]], "Language Model QAT Using Quark": [[317, null]], "Language Models": [[289, null]], "License": [[270, "license"], [276, "license"], [278, "license"], [294, "license"], [295, "license"], [296, "license"], [297, "license"], [298, "license"], [299, "license"]], "Load SafeTensor and Run with a prompt": [[309, "load-safetensor-and-run-with-a-prompt"]], "Load SafeTensor and Test": [[309, "load-safetensor-and-test"]], "Load and Run": [[309, "load-and-run"]], "Load and Test": [[309, "load-and-test"]], "Loading": [[335, "loading"]], "MSE Calibration Method": [[261, "mse-calibration-method"]], "Microscaling (MX)": [[297, null]], "Microscaling (MX) Example": [[267, null]], "MinMax Calibration Method": [[261, "minmax-calibration-method"]], "Mixed Precision": [[298, null]], "Mixed Precision Quantization in AMD Quark for ONNX": [[298, "mixed-precision-quantization-in-amd-quark-for-onnx"]], "Model Issues": [[290, "model-issues"]], "Model Preparation": [[324, "model-preparation"]], "Model Quantization": [[251, "model-quantization"], [275, "model-quantization"]], "Model Quantization Preparation": [[279, "model-quantization-preparation"], [280, "model-quantization-preparation"]], "ModelQuantizer": [[308, "modelquantizer"]], "Module Contents": [[1, "module-contents"], [3, "module-contents"], [4, "module-contents"], [5, "module-contents"], [6, "module-contents"], [7, "module-contents"], [8, "module-contents"], [9, "module-contents"], [11, "module-contents"], [13, "module-contents"], [14, "module-contents"], [15, "module-contents"], [16, "module-contents"], [17, "module-contents"], [18, "module-contents"], [20, "module-contents"], [21, "module-contents"], [22, "module-contents"], [25, "module-contents"], [26, "module-contents"], [27, "module-contents"], [31, "module-contents"], [32, "module-contents"], [33, "module-contents"], [34, "module-contents"], [36, "module-contents"], [39, "module-contents"], [50, "module-contents"], [51, "module-contents"], [53, "module-contents"], [54, "module-contents"], [55, "module-contents"], [56, "module-contents"], [57, "module-contents"], [61, "module-contents"], [63, "module-contents"], [66, "module-contents"], [71, "module-contents"], [72, "module-contents"], [74, "module-contents"], [76, "module-contents"], [78, "module-contents"], [86, "module-contents"], [91, "module-contents"], [92, "module-contents"], [93, "module-contents"], [99, "module-contents"], [106, "module-contents"], [107, "module-contents"], [108, "module-contents"], [110, "module-contents"], [111, "module-contents"], [114, "module-contents"], [118, "module-contents"], [119, "module-contents"], [120, "module-contents"], [121, "module-contents"], [122, "module-contents"], [124, "module-contents"], [127, "module-contents"], [128, "module-contents"], [131, "module-contents"], [135, "module-contents"], [136, "module-contents"], [138, "module-contents"], [139, "module-contents"], [140, "module-contents"], [141, "module-contents"], [143, "module-contents"], [144, "module-contents"], [147, "module-contents"], [149, "module-contents"], [150, "module-contents"], [151, "module-contents"], [154, "module-contents"], [156, "module-contents"], [159, "module-contents"], [162, "module-contents"], [169, "module-contents"], [177, "module-contents"], [178, "module-contents"], [180, "module-contents"], [181, "module-contents"], [182, "module-contents"], [183, "module-contents"], [186, "module-contents"], [190, "module-contents"], [193, "module-contents"], [194, "module-contents"], [198, "module-contents"], [199, "module-contents"], [202, "module-contents"], [203, "module-contents"], [205, "module-contents"], [206, "module-contents"], [209, "module-contents"], [211, "module-contents"], [212, "module-contents"], [213, "module-contents"], [215, "module-contents"], [217, "module-contents"], [218, "module-contents"], [219, "module-contents"], [220, "module-contents"], [221, "module-contents"], [222, "module-contents"], [225, "module-contents"], [226, "module-contents"], [228, "module-contents"], [230, "module-contents"], [233, "module-contents"], [234, "module-contents"], [235, "module-contents"], [236, "module-contents"], [237, "module-contents"], [238, "module-contents"], [240, "module-contents"], [241, "module-contents"], [242, "module-contents"], [243, "module-contents"], [244, "module-contents"]], "More Quantization Default Configurations": [[300, "more-quantization-default-configurations"]], "New Features (Version 0.1.0)": [[342, "new-features-version-0-1-0"]], "New Features (Version 0.2.0)": [[342, "new-features-version-0-2-0"]], "New Features (Version 0.5.0)": [[342, "new-features-version-0-5-0"]], "New Features (Version 0.5.1)": [[342, "new-features-version-0-5-1"]], "New Features (Version 0.6.0)": [[342, "new-features-version-0-6-0"]], "New Features (Version 0.7)": [[342, "new-features-version-0-7"]], "New Features (Version 0.8)": [[342, "new-features-version-0-8"]], "NonOverflow Calibration Method": [[261, "nonoverflow-calibration-method"]], "Notes": [[303, "notes"]], "ONNX Examples in AMD Quark for This Release": [[289, "onnx-examples-in-amd-quark-for-this-release"]], "ONNX Exporting": [[325, null]], "ONNX Runtime Gen AI (OGA) Installation": [[324, "onnx-runtime-gen-ai-oga-installation"]], "Older Versions": [[250, "older-versions"]], "Optional Utilities": [[291, null]], "Other Arguments": [[311, "other-arguments"], [313, "other-arguments"], [314, "other-arguments"]], "Overview": [[299, "overview"], [308, "overview"]], "PPL on Torch Models": [[313, "ppl-on-torch-models"]], "PTQ": [[319, "ptq"]], "Package Contents": [[41, "package-contents"], [59, "package-contents"], [123, "package-contents"], [130, "package-contents"], [132, "package-contents"], [134, "package-contents"], [160, "package-contents"], [188, "package-contents"], [192, "package-contents"], [195, "package-contents"], [229, "package-contents"], [232, "package-contents"]], "Parameter Explanation": [[341, "id1"], [341, "id2"], [341, "id3"], [341, "id4"], [341, "id5"], [341, "id6"]], "Percentile Calibration Method": [[261, "percentile-calibration-method"]], "Perplexity Evaluations": [[313, null]], "Perplexity on ONNX Models": [[313, "perplexity-on-onnx-models"]], "Pip Requirements": [[269, "pip-requirements"], [271, "pip-requirements"], [274, "pip-requirements"], [281, "pip-requirements"], [284, "pip-requirements"], [286, "pip-requirements"], [287, "pip-requirements"], [292, "pip-requirements"]], "Pip requirements": [[266, "pip-requirements"], [267, "pip-requirements"], [268, "pip-requirements"], [270, "pip-requirements"], [272, "pip-requirements"], [273, "pip-requirements"], [275, "pip-requirements"], [276, "pip-requirements"], [277, "pip-requirements"], [278, "pip-requirements"], [282, "pip-requirements"], [283, "pip-requirements"]], "Pre-processing on the Float Model": [[291, "pre-processing-on-the-float-model"]], "Preparation": [[315, "preparation"], [316, "preparation"], [317, "preparation"], [324, "preparation"]], "Prepare Calibration Data": [[284, "prepare-calibration-data"], [286, "prepare-calibration-data"], [287, "prepare-calibration-data"], [292, "prepare-calibration-data"]], "Prepare Data": [[269, "prepare-data"], [271, "prepare-data"]], "Prepare Model": [[269, "prepare-model"], [271, "prepare-model"], [274, "prepare-model"], [281, "prepare-model"]], "Prepare data": [[266, "prepare-data"], [267, "prepare-data"], [268, "prepare-data"], [270, "prepare-data"], [277, "prepare-data"]], "Prepare data and model": [[275, "prepare-data-and-model"]], "Prepare model": [[266, "prepare-model"], [267, "prepare-model"], [268, "prepare-model"], [270, "prepare-model"], [272, "prepare-model"], [273, "prepare-model"], [276, "prepare-model"], [277, "prepare-model"], [278, "prepare-model"], [282, "prepare-model"], [283, "prepare-model"], [284, "prepare-model"], [286, "prepare-model"], [287, "prepare-model"], [292, "prepare-model"]], "Prepare model from Torch to ONNX (Optional)": [[287, "prepare-model-from-torch-to-onnx-optional"]], "Prerequisites": [[250, "prerequisites"]], "Print Names and Quantity of A16W8 and A8W8 Conv for Mixed-Precision Models": [[293, "print-names-and-quantity-of-a16w8-and-a8w8-conv-for-mixed-precision-models"]], "Pruning": [[315, null]], "Pruning Scripts": [[315, "pruning-scripts"]], "PyTorch Examples in Quark for This Release": [[330, null]], "QAT": [[319, "qat"]], "QAT Scripts": [[317, "qat-scripts"]], "QuaRot": [[254, null]], "Quantizating Llama-2-7b model using MatMulNBits quantizer": [[282, null]], "Quantizating a model with GPTQ": [[274, null]], "Quantization": [[272, "quantization"], [273, "quantization"], [276, "quantization"], [282, "quantization"], [283, "quantization"], [286, "quantization"], [287, "quantization"], [292, "quantization"]], "Quantization & Export Scripts": [[309, "quantization-export-scripts"], [316, "quantization-export-scripts"]], "Quantization Issues": [[290, "quantization-issues"]], "Quantization Results": [[266, "quantization-results"]], "Quantization Schemes": [[262, null], [332, null]], "Quantization Strategies": [[263, null], [333, null]], "Quantization Symmetry": [[264, null], [334, null]], "Quantization Using AdaQuant and AdaRound": [[252, null]], "Quantization With CLE": [[271, "quantization-with-cle"]], "Quantization With GPTQ": [[274, "quantization-with-gptq"]], "Quantization With Mixed Precision": [[277, "quantization-with-mixed-precision"]], "Quantization With Smooth Quant": [[281, "quantization-with-smooth-quant"]], "Quantization Without CLE": [[271, "quantization-without-cle"]], "Quantization Without GPTQ": [[274, "quantization-without-gptq"]], "Quantization Without Mixed Precision": [[277, "quantization-without-mixed-precision"]], "Quantization Without Smooth Quant": [[281, "quantization-without-smooth-quant"]], "Quantization using Mixed Precision": [[277, null]], "Quantization using SmoothQuant": [[281, null]], "Quantization with ADAQUANT": [[268, "quantization-with-adaquant"]], "Quantization with ADAROUND": [[269, "quantization-with-adaround"]], "Quantization with AMD Quark": [[251, null]], "Quantization with HQQ": [[282, "quantization-with-hqq"]], "Quantization with MX Formats": [[267, "quantization-with-mx-formats"]], "Quantization with QuaRot": [[278, "quantization-with-quarot"]], "Quantization with QuaRot and SmoothQuant": [[278, "quantization-with-quarot-and-smoothquant"]], "Quantization with auto_search": [[270, "quantization-with-auto-search"]], "Quantization without ADAQUANT": [[268, "quantization-without-adaquant"]], "Quantization without ADAROUND": [[269, "quantization-without-adaround"]], "Quantization without QuaRot": [[278, "quantization-without-quarot"]], "QuantizationConfig": [[308, "quantizationconfig"]], "Quantize Controlnet and Export SafeTensors (unet-only)": [[309, "quantize-controlnet-and-export-safetensors-unet-only"]], "Quantize Diffusion and Export ONNX (entire pipeline)": [[309, "quantize-diffusion-and-export-onnx-entire-pipeline"]], "Quantize a ONNX Model Using Random Input": [[293, "quantize-a-onnx-model-using-random-input"]], "Quantizing Using CrossLayerEqualization (CLE)": [[253, null]], "Quantizing a ResNet50-v1-12 Model": [[275, null]], "Quantizing an OPT-125M Model": [[276, null]], "Quantizing with Rotation and SmoothQuant": [[340, null]], "Quark APIs for PyTorch": [[329, null]], "Quark Format": [[326, null]], "Quark Format Exporting": [[326, "quark-format-exporting"]], "Quark Format Importing": [[326, "quark-format-importing"]], "Quark ONNX Example for CrossLayerEqualization (CLE)": [[271, null]], "Quark ONNX Quantization Example": [[268, null], [269, null], [270, null], [278, null], [283, null]], "Quark UINT4 Quantization with AWQ": [[324, "quark-uint4-quantization-with-awq"]], "Quick Start": [[319, "quick-start"]], "Recipe 1: Evaluation of Llama Float16 Model without Quantization": [[316, "recipe-1-evaluation-of-llama-float16-model-without-quantization"]], "Recipe 1: Evaluation of Llama2 Float16 Model without Pruning": [[315, "recipe-1-evaluation-of-llama2-float16-model-without-pruning"]], "Recipe 1: Evaluation of Original LLM": [[317, "recipe-1-evaluation-of-original-llm"]], "Recipe 2: FP8 (OCP fp8_e4m3) Quantization & Json_SafeTensors_Export with KV Cache": [[316, "recipe-2-fp8-ocp-fp8-e4m3-quantization-json-safetensors-export-with-kv-cache"]], "Recipe 2: Pruning Model and Saved to Safetensors": [[315, "recipe-2-pruning-model-and-saved-to-safetensors"]], "Recipe 2: QAT Finetuning and Export to Safetensors": [[317, "recipe-2-qat-finetuning-and-export-to-safetensors"]], "Recipe 3: INT Weight-Only Quantization & Json_SafeTensors_Export with AWQ": [[316, "recipe-3-int-weight-only-quantization-json-safetensors-export-with-awq"]], "Recipe 3: Reload and Evaluate Finetuned Model": [[317, "recipe-3-reload-and-evaluate-finetuned-model"]], "Recipe 4: INT Static Quantization & Json_SafeTensors_Export (on CPU)": [[316, "recipe-4-int-static-quantization-json-safetensors-export-on-cpu"]], "Recipe 5: Quantization & GGUF_Export with AWQ (W_uint4 A_float16 per_group asymmetric)": [[316, "recipe-5-quantization-gguf-export-with-awq-w-uint4-a-float16-per-group-asymmetric"]], "Recipe 6: MX Quantization": [[316, "recipe-6-mx-quantization"]], "Recipe 7: BFP16 Quantization": [[316, "recipe-7-bfp16-quantization"]], "Recipe 8: MX6 Quantization": [[316, "recipe-8-mx6-quantization"]], "Recipes": [[311, "recipes"], [313, "recipes"], [314, "recipes"]], "Release Notes": [[342, null]], "Replace inf and -inf Values in ONNX Model Weights": [[293, "replace-inf-and-inf-values-in-onnx-model-weights"]], "Results": [[340, "results"]], "Rotation-based quantization with QuaRot": [[339, null]], "Rouge & Meteor Evaluations": [[314, null]], "Rouge/Meteor on ONNX Models": [[314, "rouge-meteor-on-onnx-models"]], "Rouge/Meteor on Torch Models": [[314, "rouge-meteor-on-torch-models"]], "Run Diffusion Model Without Quantization": [[309, "run-diffusion-model-without-quantization"]], "Ryzen AI Quantization": [[289, null]], "Save & Load Quantized Models": [[335, null]], "Saving": [[335, "saving"]], "Search Config Settings": [[279, "search-config-settings"], [280, "search-config-settings"]], "SmoothQuant (SQ)": [[255, null]], "Some of GGUF dtypes and their corresponding quant schemes": [[320, "id3"]], "Step 1: Configuring QuantizationSpec for torch.Tensors": [[341, "step-1-configuring-quantizationspec-for-torch-tensors"]], "Step 1: Quantize Your Model": [[320, "step-1-quantize-your-model"]], "Step 2: Establishing QuantizationConfig for nn.Module": [[341, "step-2-establishing-quantizationconfig-for-nn-module"]], "Step 2: Export to GGUF": [[320, "step-2-export-to-gguf"]], "Step 3: Run with llama.cpp": [[320, "step-3-run-with-llama-cpp"]], "Step 3: [Optional] Setting AlgoConfig for the model": [[341, "step-3-optional-setting-algoconfig-for-the-model"]], "Step 4: Setting up the overall Config for the model.": [[341, "step-4-setting-up-the-overall-config-for-the-model"]], "Step-by-Step Integration": [[308, "step-by-step-integration"]], "Step-by-Step Process": [[312, "step-by-step-process"]], "Submodules": [[30, "submodules"], [35, "submodules"], [52, "submodules"], [59, "submodules"], [60, "submodules"], [100, "submodules"], [113, "submodules"], [133, "submodules"], [152, "submodules"], [160, "submodules"], [184, "submodules"], [195, "submodules"], [201, "submodules"], [229, "submodules"]], "Summary Table": [[301, "summary-table"], [301, "summary-table-1"]], "Supported Data Types": [[301, "supported-data-types"]], "Supported Data and Op Types": [[301, null]], "Supported Features": [[258, "supported-features"], [304, "supported-features"]], "Supported Models": [[315, "supported-models"], [316, "supported-models"], [316, "id1"], [317, "supported-models"]], "Supported Op Type": [[301, "supported-op-type"]], "Supported Tasks": [[312, "supported-tasks"]], "TQT": [[319, "tqt"]], "Third-party Dependencies": [[309, "third-party-dependencies"]], "Tips:": [[291, "tips"]], "Tools": [[293, null]], "Try Different Quantization Schemes": [[336, "try-different-quantization-schemes"]], "Try QAT": [[336, "try-qat"]], "Tutorial: Generating AWQ Configuration Automatically (Experimental)": [[316, "tutorial-generating-awq-configuration-automatically-experimental"]], "Tutorial: Running a Model Not on the Supported List": [[316, "tutorial-running-a-model-not-on-the-supported-list"]], "Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)": [[303, null]], "Two-level Quantization: MX6 and MX9 Data Types": [[303, "two-level-quantization-mx6-and-mx9-data-types"]], "Upgrades of AdaRound / AdaQuant in AMD Quark for ONNX": [[252, "upgrades-of-adaround-adaquant-in-amd-quark-for-onnx"]], "Usage": [[299, "usage"], [316, "usage"]], "User Guide": [[310, "user-guide"]], "Using MX (Microscaling)": [[302, null]], "Using ONNX Model Inference and Saving Input Data in NPY Format": [[265, null]], "Using Random Data for AMD Quark Quantizer": [[260, "using-random-data-for-amd-quark-quantizer"]], "Using SmoothQuant in quark.torch": [[337, "using-smoothquant-in-quark-torch"]], "Vision Model Quantization Using Quark FX Graph Mode": [[319, null]], "Weights-Only Quantization": [[289, null]], "Welcome to AMD Quark Documentation!": [[249, null]], "What Does This Mean?": [[251, "what-does-this-mean"]], "What Happens Internally in Quark When We Quantize Something?": [[251, "what-happens-internally-in-quark-when-we-quantize-something"]], "What Is GGUF": [[320, "what-is-gguf"]], "What Is Quantization?": [[251, "what-is-quantization"]], "What is BFP16 Quantization?": [[295, "what-is-bfp16-quantization"]], "What is MX Quantization?": [[297, "what-is-mx-quantization"]], "What is Microexponents Quantization?": [[296, "what-is-microexponents-quantization"]], "What is Mixed Precision Quantization?": [[298, "what-is-mixed-precision-quantization"]], "When Are Values Actually Converted into Their Quantized Data Types?": [[251, "when-are-values-actually-converted-into-their-quantized-data-types"]], "quark": [[0, null]], "quark.onnx": [[35, null]], "quark.onnx.auto_search": [[1, null]], "quark.onnx.bias_correction": [[2, null]], "quark.onnx.calibrate": [[3, null]], "quark.onnx.cpu_quantizer": [[4, null]], "quark.onnx.equalization": [[5, null]], "quark.onnx.finetuning": [[19, null]], "quark.onnx.finetuning.create_torch": [[12, null]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, null]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, null]], "quark.onnx.finetuning.create_torch.create_model": [[8, null]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, null]], "quark.onnx.finetuning.create_torch.create_model_test": [[10, null]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, null]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, null]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, null]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, null]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, null]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, null]], "quark.onnx.finetuning.fast_finetune": [[18, null]], "quark.onnx.finetuning.onnx_evaluate": [[20, null]], "quark.onnx.finetuning.onnx_subgraph": [[21, null]], "quark.onnx.finetuning.torch_utils": [[22, null]], "quark.onnx.finetuning.torch_utils_test": [[23, null]], "quark.onnx.finetuning.train_torch": [[24, null]], "quark.onnx.finetuning.train_torch.train_model": [[25, null]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, null]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, null]], "quark.onnx.gptq": [[29, null]], "quark.onnx.gptq.gptq": [[28, null]], "quark.onnx.graph_transformations": [[30, null]], "quark.onnx.graph_transformations.model_transformer": [[31, null]], "quark.onnx.graph_transformations.model_transformer_test": [[32, null]], "quark.onnx.graph_transformations.transforms": [[33, null]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, null]], "quark.onnx.mprecision": [[37, null]], "quark.onnx.mprecision.auto_mixprecision": [[36, null]], "quark.onnx.mprecision.mixed_bfp": [[38, null]], "quark.onnx.onnx_quantizer": [[39, null]], "quark.onnx.operators": [[42, null]], "quark.onnx.operators.custom_ops": [[41, null]], "quark.onnx.operators.custom_ops.build_vai_custom_op": [[40, null]], "quark.onnx.operators.vai_ops": [[45, null]], "quark.onnx.operators.vai_ops.concat": [[43, null]], "quark.onnx.operators.vai_ops.hardsigmoid": [[44, null]], "quark.onnx.operators.vai_ops.layernorm": [[46, null]], "quark.onnx.operators.vai_ops.prelu": [[47, null]], "quark.onnx.operators.vai_ops.qdq_ops": [[48, null]], "quark.onnx.operators.vai_ops.softmax": [[49, null]], "quark.onnx.optimizations": [[52, null]], "quark.onnx.optimizations.convert_transforms": [[50, null]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, null]], "quark.onnx.optimize": [[53, null]], "quark.onnx.qdq_quantizer": [[54, null]], "quark.onnx.quant_utils": [[55, null]], "quark.onnx.quantization": [[60, null]], "quark.onnx.quantization.api": [[56, null]], "quark.onnx.quantization.config": [[59, null]], "quark.onnx.quantization.config.config": [[57, null]], "quark.onnx.quantization.config.custom_config": [[58, null]], "quark.onnx.quantize": [[61, null]], "quark.onnx.quantizers": [[65, null]], "quark.onnx.quantizers.bfp_quantizer": [[62, null]], "quark.onnx.quantizers.cpu_quantizer": [[63, null]], "quark.onnx.quantizers.extended_quantizer": [[64, null]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, null]], "quark.onnx.quantizers.npu_cnn_quantizer": [[67, null]], "quark.onnx.quantizers.npu_transformer_quantizer": [[68, null]], "quark.onnx.quantizers.onnx_quantizer": [[69, null]], "quark.onnx.quantizers.qdq_quantizer": [[70, null]], "quark.onnx.quarot": [[71, null]], "quark.onnx.refine": [[72, null]], "quark.onnx.registry": [[73, null]], "quark.onnx.simulate_dpu": [[74, null]], "quark.onnx.simulate_dpu_softmax": [[75, null]], "quark.onnx.smooth_quant": [[76, null]], "quark.onnx.tools": [[100, null]], "quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu": [[77, null]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, null]], "quark.onnx.tools.convert_dynamic_to_fixed": [[79, null]], "quark.onnx.tools.convert_fp16_to_bf16": [[80, null]], "quark.onnx.tools.convert_fp16_to_bfp16": [[81, null]], "quark.onnx.tools.convert_fp16_to_fp32": [[82, null]], "quark.onnx.tools.convert_fp32_to_bf16": [[83, null]], "quark.onnx.tools.convert_fp32_to_bfp16": [[84, null]], "quark.onnx.tools.convert_fp32_to_fp16": [[85, null]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, null]], "quark.onnx.tools.convert_nchw_to_nhwc": [[87, null]], "quark.onnx.tools.convert_onnx_to_onnxtxt": [[88, null]], "quark.onnx.tools.convert_onnxtxt_to_onnx": [[89, null]], "quark.onnx.tools.convert_opset_version": [[90, null]], "quark.onnx.tools.convert_qdq_to_qop": [[91, null]], "quark.onnx.tools.convert_quant_to_float": [[92, null]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, null]], "quark.onnx.tools.convert_s8s8_to_u8s8": [[94, null]], "quark.onnx.tools.convert_shared_initializer_to_unique": [[95, null]], "quark.onnx.tools.convert_u16s8_to_s16s8": [[96, null]], "quark.onnx.tools.convert_u16u8_to_u8u8": [[97, null]], "quark.onnx.tools.evaluate": [[98, null]], "quark.onnx.tools.float16": [[99, null]], "quark.onnx.tools.insert_clip_bfloat16_qdq": [[101, null]], "quark.onnx.tools.print_a16w8_a8w8_nodes": [[102, null]], "quark.onnx.tools.random_quantize": [[103, null]], "quark.onnx.tools.remove_bf16_cast": [[104, null]], "quark.onnx.tools.remove_initializer_from_input": [[105, null]], "quark.onnx.tools.remove_qdq": [[106, null]], "quark.onnx.tools.remove_qdq_between_ops": [[107, null]], "quark.onnx.tools.remove_qdq_mul_add": [[108, null]], "quark.onnx.tools.replace_bfloat16_qdq_cast": [[109, null]], "quark.onnx.tools.replace_inf_weights": [[110, null]], "quark.onnx.tools.save_tensor_hist": [[111, null]], "quark.onnx.tools.save_weights_hist": [[112, null]], "quark.onnx.utils": [[113, null]], "quark.onnx.utils.model_utils": [[114, null]], "quark.shares": [[115, null]], "quark.shares.utils": [[117, null]], "quark.shares.utils.import_utils": [[116, null]], "quark.shares.utils.log": [[118, null]], "quark.shares.utils.testing_utils": [[119, null]], "quark.torch": [[188, null]], "quark.torch.algorithm": [[133, null]], "quark.torch.algorithm.api": [[120, null]], "quark.torch.algorithm.awq": [[123, null]], "quark.torch.algorithm.awq.auto_smooth": [[121, null]], "quark.torch.algorithm.awq.awq": [[122, null]], "quark.torch.algorithm.awq.modules": [[125, null]], "quark.torch.algorithm.awq.modules.act": [[124, null]], "quark.torch.algorithm.awq.scale": [[126, null]], "quark.torch.algorithm.awq.smooth": [[127, null]], "quark.torch.algorithm.blockwise_tuning": [[130, null]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, null]], "quark.torch.algorithm.blockwise_tuning.blockwise_utils": [[129, null]], "quark.torch.algorithm.gptq": [[132, null]], "quark.torch.algorithm.gptq.gptq": [[131, null]], "quark.torch.algorithm.osscar": [[134, null]], "quark.torch.algorithm.osscar.osscar": [[135, null]], "quark.torch.algorithm.processor": [[136, null]], "quark.torch.algorithm.quarot": [[137, null]], "quark.torch.algorithm.quarot.monkeypatch": [[138, null]], "quark.torch.algorithm.quarot.quarot": [[139, null]], "quark.torch.algorithm.quarot.utils": [[140, null]], "quark.torch.algorithm.rotation": [[142, null]], "quark.torch.algorithm.rotation.hadamard": [[141, null]], "quark.torch.algorithm.rotation.rotation": [[143, null]], "quark.torch.algorithm.rotation.rotation_utils": [[144, null]], "quark.torch.algorithm.utils": [[146, null]], "quark.torch.algorithm.utils.auto_config": [[145, null]], "quark.torch.algorithm.utils.module": [[147, null]], "quark.torch.algorithm.utils.prepare": [[148, null]], "quark.torch.algorithm.utils.utils": [[149, null]], "quark.torch.export": [[160, null]], "quark.torch.export.api": [[150, null]], "quark.torch.export.config": [[152, null]], "quark.torch.export.config.config": [[151, null]], "quark.torch.export.constants": [[153, null]], "quark.torch.export.gguf_export": [[157, null]], "quark.torch.export.gguf_export.api": [[154, null]], "quark.torch.export.gguf_export.gguf_model_converter": [[155, null]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, null]], "quark.torch.export.gguf_export.tensor_convert": [[158, null]], "quark.torch.export.gguf_export.utils": [[159, null]], "quark.torch.export.json_export": [[167, null]], "quark.torch.export.json_export.builder": [[161, null]], "quark.torch.export.json_export.builder.llm_info": [[162, null]], "quark.torch.export.json_export.builder.llm_info_builder": [[163, null]], "quark.torch.export.json_export.builder.native_model_info_builder": [[164, null]], "quark.torch.export.json_export.converter": [[165, null]], "quark.torch.export.json_export.converter.llm_info_converter": [[166, null]], "quark.torch.export.json_export.utils": [[168, null]], "quark.torch.export.json_export.utils.utils": [[169, null]], "quark.torch.export.main_export": [[170, null]], "quark.torch.export.main_export.model_post_process": [[171, null]], "quark.torch.export.main_export.quant_config_parser": [[172, null]], "quark.torch.export.main_import": [[173, null]], "quark.torch.export.main_import.pretrained_config": [[174, null]], "quark.torch.export.nn": [[175, null]], "quark.torch.export.nn.modules": [[176, null]], "quark.torch.export.nn.modules.qparamslinear": [[177, null]], "quark.torch.export.nn.modules.realquantizer": [[178, null]], "quark.torch.export.onnx": [[179, null]], "quark.torch.export.utils": [[180, null]], "quark.torch.extensions": [[187, null]], "quark.torch.extensions.brevitas": [[184, null]], "quark.torch.extensions.brevitas.algos": [[181, null]], "quark.torch.extensions.brevitas.api": [[182, null]], "quark.torch.extensions.brevitas.config": [[183, null]], "quark.torch.extensions.brevitas.mapping": [[185, null]], "quark.torch.extensions.brevitas.verification": [[186, null]], "quark.torch.kernel": [[192, null]], "quark.torch.kernel.hw_emulation": [[191, null]], "quark.torch.kernel.hw_emulation.extensions": [[189, null]], "quark.torch.kernel.hw_emulation.hw_emulation_interface": [[190, null]], "quark.torch.pruning": [[195, null]], "quark.torch.pruning.api": [[193, null]], "quark.torch.pruning.config": [[194, null]], "quark.torch.pruning.model_transformation": [[196, null]], "quark.torch.pruning.utils": [[197, null]], "quark.torch.quantization": [[229, null]], "quark.torch.quantization.api": [[198, null]], "quark.torch.quantization.config": [[201, null]], "quark.torch.quantization.config.config": [[199, null]], "quark.torch.quantization.config.config_verification": [[200, null]], "quark.torch.quantization.config.type": [[202, null]], "quark.torch.quantization.config.utils": [[203, null]], "quark.torch.quantization.constants": [[204, null]], "quark.torch.quantization.debug": [[205, null]], "quark.torch.quantization.graph": [[208, null]], "quark.torch.quantization.graph.fx": [[207, null]], "quark.torch.quantization.graph.fx.base": [[206, null]], "quark.torch.quantization.graph.optimization": [[210, null]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, null]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, null]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, null]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, null]], "quark.torch.quantization.graph.optimization.post_quant": [[214, null]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, null]], "quark.torch.quantization.graph.optimization.pre_quant": [[216, null]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, null]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, null]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, null]], "quark.torch.quantization.graph.optimization.utils": [[223, null]], "quark.torch.quantization.graph.processor": [[224, null]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, null]], "quark.torch.quantization.graph.processor.processor": [[226, null]], "quark.torch.quantization.graph.processor.processor_utils": [[227, null]], "quark.torch.quantization.graph.torch_utils": [[228, null]], "quark.torch.quantization.model_transformation": [[230, null]], "quark.torch.quantization.nn": [[231, null]], "quark.torch.quantization.nn.modules": [[232, null]], "quark.torch.quantization.nn.modules.mixin": [[233, null]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, null]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, null]], "quark.torch.quantization.nn.modules.quantize_embed": [[236, null]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, null]], "quark.torch.quantization.nn.utils": [[238, null]], "quark.torch.quantization.observer": [[239, null]], "quark.torch.quantization.observer.lsq_observer": [[240, null]], "quark.torch.quantization.observer.observer": [[241, null]], "quark.torch.quantization.observer.tqt_observer": [[242, null]], "quark.torch.quantization.tensor_quantize": [[243, null]], "quark.torch.quantization.utils": [[244, null]], "quark.torch.utils": [[245, null]], "quark.torch.utils.pack": [[246, null]], "quark.version": [[247, null]]}, "docnames": ["autoapi/quark/index", "autoapi/quark/onnx/auto_search/index", "autoapi/quark/onnx/bias_correction/index", "autoapi/quark/onnx/calibrate/index", "autoapi/quark/onnx/cpu_quantizer/index", "autoapi/quark/onnx/equalization/index", "autoapi/quark/onnx/finetuning/create_torch/base_fn_quantizers/index", "autoapi/quark/onnx/finetuning/create_torch/base_qdq_quantizers/index", "autoapi/quark/onnx/finetuning/create_torch/create_model/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_ops/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_test/index", "autoapi/quark/onnx/finetuning/create_torch/create_model_utils/index", "autoapi/quark/onnx/finetuning/create_torch/index", "autoapi/quark/onnx/finetuning/create_torch/quant_base_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_conv_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_gemm_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_matmul_ops/index", "autoapi/quark/onnx/finetuning/create_torch/quant_norm_ops/index", "autoapi/quark/onnx/finetuning/fast_finetune/index", "autoapi/quark/onnx/finetuning/index", "autoapi/quark/onnx/finetuning/onnx_evaluate/index", "autoapi/quark/onnx/finetuning/onnx_subgraph/index", "autoapi/quark/onnx/finetuning/torch_utils/index", "autoapi/quark/onnx/finetuning/torch_utils_test/index", "autoapi/quark/onnx/finetuning/train_torch/index", "autoapi/quark/onnx/finetuning/train_torch/train_model/index", "autoapi/quark/onnx/finetuning/train_torch/train_model_loss/index", "autoapi/quark/onnx/finetuning/train_torch/train_model_param/index", "autoapi/quark/onnx/gptq/gptq/index", "autoapi/quark/onnx/gptq/index", "autoapi/quark/onnx/graph_transformations/index", "autoapi/quark/onnx/graph_transformations/model_transformer/index", "autoapi/quark/onnx/graph_transformations/model_transformer_test/index", "autoapi/quark/onnx/graph_transformations/transforms/index", "autoapi/quark/onnx/graph_transformations/transforms_pipeline/index", "autoapi/quark/onnx/index", "autoapi/quark/onnx/mprecision/auto_mixprecision/index", "autoapi/quark/onnx/mprecision/index", "autoapi/quark/onnx/mprecision/mixed_bfp/index", "autoapi/quark/onnx/onnx_quantizer/index", "autoapi/quark/onnx/operators/custom_ops/build_vai_custom_op/index", "autoapi/quark/onnx/operators/custom_ops/index", "autoapi/quark/onnx/operators/index", "autoapi/quark/onnx/operators/vai_ops/concat/index", "autoapi/quark/onnx/operators/vai_ops/hardsigmoid/index", "autoapi/quark/onnx/operators/vai_ops/index", "autoapi/quark/onnx/operators/vai_ops/layernorm/index", "autoapi/quark/onnx/operators/vai_ops/prelu/index", "autoapi/quark/onnx/operators/vai_ops/qdq_ops/index", "autoapi/quark/onnx/operators/vai_ops/softmax/index", "autoapi/quark/onnx/optimizations/convert_transforms/index", "autoapi/quark/onnx/optimizations/convert_transforms_pipeline/index", "autoapi/quark/onnx/optimizations/index", "autoapi/quark/onnx/optimize/index", "autoapi/quark/onnx/qdq_quantizer/index", "autoapi/quark/onnx/quant_utils/index", "autoapi/quark/onnx/quantization/api/index", "autoapi/quark/onnx/quantization/config/config/index", "autoapi/quark/onnx/quantization/config/custom_config/index", "autoapi/quark/onnx/quantization/config/index", "autoapi/quark/onnx/quantization/index", "autoapi/quark/onnx/quantize/index", "autoapi/quark/onnx/quantizers/bfp_quantizer/index", "autoapi/quark/onnx/quantizers/cpu_quantizer/index", "autoapi/quark/onnx/quantizers/extended_quantizer/index", "autoapi/quark/onnx/quantizers/index", "autoapi/quark/onnx/quantizers/matmul_nbits_quantizer/index", "autoapi/quark/onnx/quantizers/npu_cnn_quantizer/index", "autoapi/quark/onnx/quantizers/npu_transformer_quantizer/index", "autoapi/quark/onnx/quantizers/onnx_quantizer/index", "autoapi/quark/onnx/quantizers/qdq_quantizer/index", "autoapi/quark/onnx/quarot/index", "autoapi/quark/onnx/refine/index", "autoapi/quark/onnx/registry/index", "autoapi/quark/onnx/simulate_dpu/index", "autoapi/quark/onnx/simulate_dpu_softmax/index", "autoapi/quark/onnx/smooth_quant/index", "autoapi/quark/onnx/tools/convert_a8w8_npu_to_a8w8_cpu/index", "autoapi/quark/onnx/tools/convert_customqdq_to_qdq/index", "autoapi/quark/onnx/tools/convert_dynamic_to_fixed/index", "autoapi/quark/onnx/tools/convert_fp16_to_bf16/index", "autoapi/quark/onnx/tools/convert_fp16_to_bfp16/index", "autoapi/quark/onnx/tools/convert_fp16_to_fp32/index", "autoapi/quark/onnx/tools/convert_fp32_to_bf16/index", "autoapi/quark/onnx/tools/convert_fp32_to_bfp16/index", "autoapi/quark/onnx/tools/convert_fp32_to_fp16/index", "autoapi/quark/onnx/tools/convert_lstm_to_customlstm/index", "autoapi/quark/onnx/tools/convert_nchw_to_nhwc/index", "autoapi/quark/onnx/tools/convert_onnx_to_onnxtxt/index", "autoapi/quark/onnx/tools/convert_onnxtxt_to_onnx/index", "autoapi/quark/onnx/tools/convert_opset_version/index", "autoapi/quark/onnx/tools/convert_qdq_to_qop/index", "autoapi/quark/onnx/tools/convert_quant_to_float/index", "autoapi/quark/onnx/tools/convert_resize_fs_to_pof2s/index", "autoapi/quark/onnx/tools/convert_s8s8_to_u8s8/index", "autoapi/quark/onnx/tools/convert_shared_initializer_to_unique/index", "autoapi/quark/onnx/tools/convert_u16s8_to_s16s8/index", "autoapi/quark/onnx/tools/convert_u16u8_to_u8u8/index", "autoapi/quark/onnx/tools/evaluate/index", "autoapi/quark/onnx/tools/float16/index", "autoapi/quark/onnx/tools/index", "autoapi/quark/onnx/tools/insert_clip_bfloat16_qdq/index", "autoapi/quark/onnx/tools/print_a16w8_a8w8_nodes/index", "autoapi/quark/onnx/tools/random_quantize/index", "autoapi/quark/onnx/tools/remove_bf16_cast/index", "autoapi/quark/onnx/tools/remove_initializer_from_input/index", "autoapi/quark/onnx/tools/remove_qdq/index", "autoapi/quark/onnx/tools/remove_qdq_between_ops/index", "autoapi/quark/onnx/tools/remove_qdq_mul_add/index", "autoapi/quark/onnx/tools/replace_bfloat16_qdq_cast/index", "autoapi/quark/onnx/tools/replace_inf_weights/index", "autoapi/quark/onnx/tools/save_tensor_hist/index", "autoapi/quark/onnx/tools/save_weights_hist/index", "autoapi/quark/onnx/utils/index", "autoapi/quark/onnx/utils/model_utils/index", "autoapi/quark/shares/index", "autoapi/quark/shares/utils/import_utils/index", "autoapi/quark/shares/utils/index", "autoapi/quark/shares/utils/log/index", "autoapi/quark/shares/utils/testing_utils/index", "autoapi/quark/torch/algorithm/api/index", "autoapi/quark/torch/algorithm/awq/auto_smooth/index", "autoapi/quark/torch/algorithm/awq/awq/index", "autoapi/quark/torch/algorithm/awq/index", "autoapi/quark/torch/algorithm/awq/modules/act/index", "autoapi/quark/torch/algorithm/awq/modules/index", "autoapi/quark/torch/algorithm/awq/scale/index", "autoapi/quark/torch/algorithm/awq/smooth/index", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_tuning/index", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_utils/index", "autoapi/quark/torch/algorithm/blockwise_tuning/index", "autoapi/quark/torch/algorithm/gptq/gptq/index", "autoapi/quark/torch/algorithm/gptq/index", "autoapi/quark/torch/algorithm/index", "autoapi/quark/torch/algorithm/osscar/index", "autoapi/quark/torch/algorithm/osscar/osscar/index", "autoapi/quark/torch/algorithm/processor/index", "autoapi/quark/torch/algorithm/quarot/index", "autoapi/quark/torch/algorithm/quarot/monkeypatch/index", "autoapi/quark/torch/algorithm/quarot/quarot/index", "autoapi/quark/torch/algorithm/quarot/utils/index", "autoapi/quark/torch/algorithm/rotation/hadamard/index", "autoapi/quark/torch/algorithm/rotation/index", "autoapi/quark/torch/algorithm/rotation/rotation/index", "autoapi/quark/torch/algorithm/rotation/rotation_utils/index", "autoapi/quark/torch/algorithm/utils/auto_config/index", "autoapi/quark/torch/algorithm/utils/index", "autoapi/quark/torch/algorithm/utils/module/index", "autoapi/quark/torch/algorithm/utils/prepare/index", "autoapi/quark/torch/algorithm/utils/utils/index", "autoapi/quark/torch/export/api/index", "autoapi/quark/torch/export/config/config/index", "autoapi/quark/torch/export/config/index", "autoapi/quark/torch/export/constants/index", "autoapi/quark/torch/export/gguf_export/api/index", "autoapi/quark/torch/export/gguf_export/gguf_model_converter/index", "autoapi/quark/torch/export/gguf_export/gguf_model_writer/index", "autoapi/quark/torch/export/gguf_export/index", "autoapi/quark/torch/export/gguf_export/tensor_convert/index", "autoapi/quark/torch/export/gguf_export/utils/index", "autoapi/quark/torch/export/index", "autoapi/quark/torch/export/json_export/builder/index", "autoapi/quark/torch/export/json_export/builder/llm_info/index", "autoapi/quark/torch/export/json_export/builder/llm_info_builder/index", "autoapi/quark/torch/export/json_export/builder/native_model_info_builder/index", "autoapi/quark/torch/export/json_export/converter/index", "autoapi/quark/torch/export/json_export/converter/llm_info_converter/index", "autoapi/quark/torch/export/json_export/index", "autoapi/quark/torch/export/json_export/utils/index", "autoapi/quark/torch/export/json_export/utils/utils/index", "autoapi/quark/torch/export/main_export/index", "autoapi/quark/torch/export/main_export/model_post_process/index", "autoapi/quark/torch/export/main_export/quant_config_parser/index", "autoapi/quark/torch/export/main_import/index", "autoapi/quark/torch/export/main_import/pretrained_config/index", "autoapi/quark/torch/export/nn/index", "autoapi/quark/torch/export/nn/modules/index", "autoapi/quark/torch/export/nn/modules/qparamslinear/index", "autoapi/quark/torch/export/nn/modules/realquantizer/index", "autoapi/quark/torch/export/onnx/index", "autoapi/quark/torch/export/utils/index", "autoapi/quark/torch/extensions/brevitas/algos/index", "autoapi/quark/torch/extensions/brevitas/api/index", "autoapi/quark/torch/extensions/brevitas/config/index", "autoapi/quark/torch/extensions/brevitas/index", "autoapi/quark/torch/extensions/brevitas/mapping/index", "autoapi/quark/torch/extensions/brevitas/verification/index", "autoapi/quark/torch/extensions/index", "autoapi/quark/torch/index", "autoapi/quark/torch/kernel/hw_emulation/extensions/index", "autoapi/quark/torch/kernel/hw_emulation/hw_emulation_interface/index", "autoapi/quark/torch/kernel/hw_emulation/index", "autoapi/quark/torch/kernel/index", "autoapi/quark/torch/pruning/api/index", "autoapi/quark/torch/pruning/config/index", "autoapi/quark/torch/pruning/index", "autoapi/quark/torch/pruning/model_transformation/index", "autoapi/quark/torch/pruning/utils/index", "autoapi/quark/torch/quantization/api/index", "autoapi/quark/torch/quantization/config/config/index", "autoapi/quark/torch/quantization/config/config_verification/index", "autoapi/quark/torch/quantization/config/index", "autoapi/quark/torch/quantization/config/type/index", "autoapi/quark/torch/quantization/config/utils/index", "autoapi/quark/torch/quantization/constants/index", "autoapi/quark/torch/quantization/debug/index", "autoapi/quark/torch/quantization/graph/fx/base/index", "autoapi/quark/torch/quantization/graph/fx/index", "autoapi/quark/torch/quantization/graph/index", "autoapi/quark/torch/quantization/graph/optimization/activate_dropout/index", "autoapi/quark/torch/quantization/graph/optimization/index", "autoapi/quark/torch/quantization/graph/optimization/model_optimization/index", "autoapi/quark/torch/quantization/graph/optimization/modify_reshape_param/index", "autoapi/quark/torch/quantization/graph/optimization/opt_pass_manager/index", "autoapi/quark/torch/quantization/graph/optimization/post_quant/index", "autoapi/quark/torch/quantization/graph/optimization/post_quant/opt_pass_after_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/opt_pass_before_quant/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv2d_to_qtconv2d/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv_bn_to_qt_model/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_convtranspose2d_to_qtconvtranspose2d/index", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_linear_to_qtlinear/index", "autoapi/quark/torch/quantization/graph/optimization/remove_dropout_node/index", "autoapi/quark/torch/quantization/graph/optimization/utils/index", "autoapi/quark/torch/quantization/graph/processor/index", "autoapi/quark/torch/quantization/graph/processor/insert_quantizer/index", "autoapi/quark/torch/quantization/graph/processor/processor/index", "autoapi/quark/torch/quantization/graph/processor/processor_utils/index", "autoapi/quark/torch/quantization/graph/torch_utils/index", "autoapi/quark/torch/quantization/index", "autoapi/quark/torch/quantization/model_transformation/index", "autoapi/quark/torch/quantization/nn/index", "autoapi/quark/torch/quantization/nn/modules/index", "autoapi/quark/torch/quantization/nn/modules/mixin/index", "autoapi/quark/torch/quantization/nn/modules/quantize_conv/index", "autoapi/quark/torch/quantization/nn/modules/quantize_conv_bn_fused/index", "autoapi/quark/torch/quantization/nn/modules/quantize_embed/index", "autoapi/quark/torch/quantization/nn/modules/quantize_linear/index", "autoapi/quark/torch/quantization/nn/utils/index", "autoapi/quark/torch/quantization/observer/index", "autoapi/quark/torch/quantization/observer/lsq_observer/index", "autoapi/quark/torch/quantization/observer/observer/index", "autoapi/quark/torch/quantization/observer/tqt_observer/index", "autoapi/quark/torch/quantization/tensor_quantize/index", "autoapi/quark/torch/quantization/utils/index", "autoapi/quark/torch/utils/index", "autoapi/quark/torch/utils/pack/index", "autoapi/quark/version/index", "basic_usage", "index", "install", "intro", "onnx/accuracy_algorithms/ada", "onnx/accuracy_algorithms/cle", "onnx/accuracy_algorithms/quarot", "onnx/accuracy_algorithms/sq", "onnx/accuracy_improvement_algorithms", "onnx/appendix_full_quant_config_features", "onnx/basic_usage_onnx", "onnx/bfp16", "onnx/config/calibration_datasets", "onnx/config/calibration_methods", "onnx/config/quantization_schemes", "onnx/config/quantization_strategies", "onnx/config/quantization_symmetry", "onnx/config/user_guide_onnx_model_inference_save_input_npy", "onnx/example_quark_onnx_BFP", "onnx/example_quark_onnx_MX", "onnx/example_quark_onnx_adaquant", "onnx/example_quark_onnx_adaround", "onnx/example_quark_onnx_auto_search", "onnx/example_quark_onnx_cle", "onnx/example_quark_onnx_dynamic_quantization_llama2", "onnx/example_quark_onnx_dynamic_quantization_opt", "onnx/example_quark_onnx_gptq", "onnx/example_quark_onnx_image_classification", "onnx/example_quark_onnx_language_models", "onnx/example_quark_onnx_mixed_precision", "onnx/example_quark_onnx_quarot", "onnx/example_quark_onnx_ryzenai", "onnx/example_quark_onnx_ryzenai_yolov3_customer_evaluator", "onnx/example_quark_onnx_smoothquant", "onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2", "onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2", "onnx/example_ryzenai_autosearch_resnet50", "onnx/gpu_usage_guide", "onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice", "onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice", "onnx/onnx_apis", "onnx/onnx_examples", "onnx/onnx_faq", "onnx/optional_utilities", "onnx/ryzen_ai_best_practice", "onnx/tools", "onnx/tutorial_bf16_quantization", "onnx/tutorial_bfp16_quantization", "onnx/tutorial_microexponents_quantization", "onnx/tutorial_microscaling_quantization", "onnx/tutorial_mix_precision", "onnx/user_guide_auto_search", "onnx/user_guide_config_description", "onnx/user_guide_supported_optype_datatype", "pytorch/adv_mx", "pytorch/adv_two_level", "pytorch/basic_usage_pytorch", "pytorch/calibration_datasets", "pytorch/calibration_methods", "pytorch/debug", "pytorch/example_quark_torch_brevitas", "pytorch/example_quark_torch_diffusers", "pytorch/example_quark_torch_llm_eval", "pytorch/example_quark_torch_llm_eval_harness", "pytorch/example_quark_torch_llm_eval_harness_offline", "pytorch/example_quark_torch_llm_eval_perplexity", "pytorch/example_quark_torch_llm_eval_rouge_meteor", "pytorch/example_quark_torch_llm_pruning", "pytorch/example_quark_torch_llm_ptq", "pytorch/example_quark_torch_llm_qat", "pytorch/example_quark_torch_pytorch_light", "pytorch/example_quark_torch_vision", "pytorch/export/gguf_llamacpp", "pytorch/export/quark_export", "pytorch/export/quark_export_gguf", "pytorch/export/quark_export_hf", "pytorch/export/quark_export_oga", "pytorch/export/quark_export_onnx", "pytorch/export/quark_export_quark", "pytorch/extensions", "pytorch/llm_quark", "pytorch/pytorch_apis", "pytorch/pytorch_examples", "pytorch/pytorch_faq", "pytorch/quantization_schemes", "pytorch/quantization_strategies", "pytorch/quantization_symmetry", "pytorch/quark_save_load", "pytorch/quark_torch_best_practices", "pytorch/smoothquant", "pytorch/tutorial_bfp16", "pytorch/tutorial_quarot", "pytorch/tutorial_rotation", "pytorch/user_guide_config_description", "release_note"], "envversion": {"sphinx": 64, "sphinx.domains.c": 3, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 9, "sphinx.domains.index": 1, "sphinx.domains.javascript": 3, "sphinx.domains.math": 2, "sphinx.domains.python": 4, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1}, "filenames": ["autoapi/quark/index.rst", "autoapi/quark/onnx/auto_search/index.rst", "autoapi/quark/onnx/bias_correction/index.rst", "autoapi/quark/onnx/calibrate/index.rst", "autoapi/quark/onnx/cpu_quantizer/index.rst", "autoapi/quark/onnx/equalization/index.rst", "autoapi/quark/onnx/finetuning/create_torch/base_fn_quantizers/index.rst", "autoapi/quark/onnx/finetuning/create_torch/base_qdq_quantizers/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_test/index.rst", "autoapi/quark/onnx/finetuning/create_torch/create_model_utils/index.rst", "autoapi/quark/onnx/finetuning/create_torch/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_base_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_conv_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_gemm_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_matmul_ops/index.rst", "autoapi/quark/onnx/finetuning/create_torch/quant_norm_ops/index.rst", "autoapi/quark/onnx/finetuning/fast_finetune/index.rst", "autoapi/quark/onnx/finetuning/index.rst", "autoapi/quark/onnx/finetuning/onnx_evaluate/index.rst", "autoapi/quark/onnx/finetuning/onnx_subgraph/index.rst", "autoapi/quark/onnx/finetuning/torch_utils/index.rst", "autoapi/quark/onnx/finetuning/torch_utils_test/index.rst", "autoapi/quark/onnx/finetuning/train_torch/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model_loss/index.rst", "autoapi/quark/onnx/finetuning/train_torch/train_model_param/index.rst", "autoapi/quark/onnx/gptq/gptq/index.rst", "autoapi/quark/onnx/gptq/index.rst", "autoapi/quark/onnx/graph_transformations/index.rst", "autoapi/quark/onnx/graph_transformations/model_transformer/index.rst", "autoapi/quark/onnx/graph_transformations/model_transformer_test/index.rst", "autoapi/quark/onnx/graph_transformations/transforms/index.rst", "autoapi/quark/onnx/graph_transformations/transforms_pipeline/index.rst", "autoapi/quark/onnx/index.rst", "autoapi/quark/onnx/mprecision/auto_mixprecision/index.rst", "autoapi/quark/onnx/mprecision/index.rst", "autoapi/quark/onnx/mprecision/mixed_bfp/index.rst", "autoapi/quark/onnx/onnx_quantizer/index.rst", "autoapi/quark/onnx/operators/custom_ops/build_vai_custom_op/index.rst", "autoapi/quark/onnx/operators/custom_ops/index.rst", "autoapi/quark/onnx/operators/index.rst", "autoapi/quark/onnx/operators/vai_ops/concat/index.rst", "autoapi/quark/onnx/operators/vai_ops/hardsigmoid/index.rst", "autoapi/quark/onnx/operators/vai_ops/index.rst", "autoapi/quark/onnx/operators/vai_ops/layernorm/index.rst", "autoapi/quark/onnx/operators/vai_ops/prelu/index.rst", "autoapi/quark/onnx/operators/vai_ops/qdq_ops/index.rst", "autoapi/quark/onnx/operators/vai_ops/softmax/index.rst", "autoapi/quark/onnx/optimizations/convert_transforms/index.rst", "autoapi/quark/onnx/optimizations/convert_transforms_pipeline/index.rst", "autoapi/quark/onnx/optimizations/index.rst", "autoapi/quark/onnx/optimize/index.rst", "autoapi/quark/onnx/qdq_quantizer/index.rst", "autoapi/quark/onnx/quant_utils/index.rst", "autoapi/quark/onnx/quantization/api/index.rst", "autoapi/quark/onnx/quantization/config/config/index.rst", "autoapi/quark/onnx/quantization/config/custom_config/index.rst", "autoapi/quark/onnx/quantization/config/index.rst", "autoapi/quark/onnx/quantization/index.rst", "autoapi/quark/onnx/quantize/index.rst", "autoapi/quark/onnx/quantizers/bfp_quantizer/index.rst", "autoapi/quark/onnx/quantizers/cpu_quantizer/index.rst", "autoapi/quark/onnx/quantizers/extended_quantizer/index.rst", "autoapi/quark/onnx/quantizers/index.rst", "autoapi/quark/onnx/quantizers/matmul_nbits_quantizer/index.rst", "autoapi/quark/onnx/quantizers/npu_cnn_quantizer/index.rst", "autoapi/quark/onnx/quantizers/npu_transformer_quantizer/index.rst", "autoapi/quark/onnx/quantizers/onnx_quantizer/index.rst", "autoapi/quark/onnx/quantizers/qdq_quantizer/index.rst", "autoapi/quark/onnx/quarot/index.rst", "autoapi/quark/onnx/refine/index.rst", "autoapi/quark/onnx/registry/index.rst", "autoapi/quark/onnx/simulate_dpu/index.rst", "autoapi/quark/onnx/simulate_dpu_softmax/index.rst", "autoapi/quark/onnx/smooth_quant/index.rst", "autoapi/quark/onnx/tools/convert_a8w8_npu_to_a8w8_cpu/index.rst", "autoapi/quark/onnx/tools/convert_customqdq_to_qdq/index.rst", "autoapi/quark/onnx/tools/convert_dynamic_to_fixed/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_bf16/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_bfp16/index.rst", "autoapi/quark/onnx/tools/convert_fp16_to_fp32/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_bf16/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_bfp16/index.rst", "autoapi/quark/onnx/tools/convert_fp32_to_fp16/index.rst", "autoapi/quark/onnx/tools/convert_lstm_to_customlstm/index.rst", "autoapi/quark/onnx/tools/convert_nchw_to_nhwc/index.rst", "autoapi/quark/onnx/tools/convert_onnx_to_onnxtxt/index.rst", "autoapi/quark/onnx/tools/convert_onnxtxt_to_onnx/index.rst", "autoapi/quark/onnx/tools/convert_opset_version/index.rst", "autoapi/quark/onnx/tools/convert_qdq_to_qop/index.rst", "autoapi/quark/onnx/tools/convert_quant_to_float/index.rst", "autoapi/quark/onnx/tools/convert_resize_fs_to_pof2s/index.rst", "autoapi/quark/onnx/tools/convert_s8s8_to_u8s8/index.rst", "autoapi/quark/onnx/tools/convert_shared_initializer_to_unique/index.rst", "autoapi/quark/onnx/tools/convert_u16s8_to_s16s8/index.rst", "autoapi/quark/onnx/tools/convert_u16u8_to_u8u8/index.rst", "autoapi/quark/onnx/tools/evaluate/index.rst", "autoapi/quark/onnx/tools/float16/index.rst", "autoapi/quark/onnx/tools/index.rst", "autoapi/quark/onnx/tools/insert_clip_bfloat16_qdq/index.rst", "autoapi/quark/onnx/tools/print_a16w8_a8w8_nodes/index.rst", "autoapi/quark/onnx/tools/random_quantize/index.rst", "autoapi/quark/onnx/tools/remove_bf16_cast/index.rst", "autoapi/quark/onnx/tools/remove_initializer_from_input/index.rst", "autoapi/quark/onnx/tools/remove_qdq/index.rst", "autoapi/quark/onnx/tools/remove_qdq_between_ops/index.rst", "autoapi/quark/onnx/tools/remove_qdq_mul_add/index.rst", "autoapi/quark/onnx/tools/replace_bfloat16_qdq_cast/index.rst", "autoapi/quark/onnx/tools/replace_inf_weights/index.rst", "autoapi/quark/onnx/tools/save_tensor_hist/index.rst", "autoapi/quark/onnx/tools/save_weights_hist/index.rst", "autoapi/quark/onnx/utils/index.rst", "autoapi/quark/onnx/utils/model_utils/index.rst", "autoapi/quark/shares/index.rst", "autoapi/quark/shares/utils/import_utils/index.rst", "autoapi/quark/shares/utils/index.rst", "autoapi/quark/shares/utils/log/index.rst", "autoapi/quark/shares/utils/testing_utils/index.rst", "autoapi/quark/torch/algorithm/api/index.rst", "autoapi/quark/torch/algorithm/awq/auto_smooth/index.rst", "autoapi/quark/torch/algorithm/awq/awq/index.rst", "autoapi/quark/torch/algorithm/awq/index.rst", "autoapi/quark/torch/algorithm/awq/modules/act/index.rst", "autoapi/quark/torch/algorithm/awq/modules/index.rst", "autoapi/quark/torch/algorithm/awq/scale/index.rst", "autoapi/quark/torch/algorithm/awq/smooth/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_tuning/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/blockwise_utils/index.rst", "autoapi/quark/torch/algorithm/blockwise_tuning/index.rst", "autoapi/quark/torch/algorithm/gptq/gptq/index.rst", "autoapi/quark/torch/algorithm/gptq/index.rst", "autoapi/quark/torch/algorithm/index.rst", "autoapi/quark/torch/algorithm/osscar/index.rst", "autoapi/quark/torch/algorithm/osscar/osscar/index.rst", "autoapi/quark/torch/algorithm/processor/index.rst", "autoapi/quark/torch/algorithm/quarot/index.rst", "autoapi/quark/torch/algorithm/quarot/monkeypatch/index.rst", "autoapi/quark/torch/algorithm/quarot/quarot/index.rst", "autoapi/quark/torch/algorithm/quarot/utils/index.rst", "autoapi/quark/torch/algorithm/rotation/hadamard/index.rst", "autoapi/quark/torch/algorithm/rotation/index.rst", "autoapi/quark/torch/algorithm/rotation/rotation/index.rst", "autoapi/quark/torch/algorithm/rotation/rotation_utils/index.rst", "autoapi/quark/torch/algorithm/utils/auto_config/index.rst", "autoapi/quark/torch/algorithm/utils/index.rst", "autoapi/quark/torch/algorithm/utils/module/index.rst", "autoapi/quark/torch/algorithm/utils/prepare/index.rst", "autoapi/quark/torch/algorithm/utils/utils/index.rst", "autoapi/quark/torch/export/api/index.rst", "autoapi/quark/torch/export/config/config/index.rst", "autoapi/quark/torch/export/config/index.rst", "autoapi/quark/torch/export/constants/index.rst", "autoapi/quark/torch/export/gguf_export/api/index.rst", "autoapi/quark/torch/export/gguf_export/gguf_model_converter/index.rst", "autoapi/quark/torch/export/gguf_export/gguf_model_writer/index.rst", "autoapi/quark/torch/export/gguf_export/index.rst", "autoapi/quark/torch/export/gguf_export/tensor_convert/index.rst", "autoapi/quark/torch/export/gguf_export/utils/index.rst", "autoapi/quark/torch/export/index.rst", "autoapi/quark/torch/export/json_export/builder/index.rst", "autoapi/quark/torch/export/json_export/builder/llm_info/index.rst", "autoapi/quark/torch/export/json_export/builder/llm_info_builder/index.rst", "autoapi/quark/torch/export/json_export/builder/native_model_info_builder/index.rst", "autoapi/quark/torch/export/json_export/converter/index.rst", "autoapi/quark/torch/export/json_export/converter/llm_info_converter/index.rst", "autoapi/quark/torch/export/json_export/index.rst", "autoapi/quark/torch/export/json_export/utils/index.rst", "autoapi/quark/torch/export/json_export/utils/utils/index.rst", "autoapi/quark/torch/export/main_export/index.rst", "autoapi/quark/torch/export/main_export/model_post_process/index.rst", "autoapi/quark/torch/export/main_export/quant_config_parser/index.rst", "autoapi/quark/torch/export/main_import/index.rst", "autoapi/quark/torch/export/main_import/pretrained_config/index.rst", "autoapi/quark/torch/export/nn/index.rst", "autoapi/quark/torch/export/nn/modules/index.rst", "autoapi/quark/torch/export/nn/modules/qparamslinear/index.rst", "autoapi/quark/torch/export/nn/modules/realquantizer/index.rst", "autoapi/quark/torch/export/onnx/index.rst", "autoapi/quark/torch/export/utils/index.rst", "autoapi/quark/torch/extensions/brevitas/algos/index.rst", "autoapi/quark/torch/extensions/brevitas/api/index.rst", "autoapi/quark/torch/extensions/brevitas/config/index.rst", "autoapi/quark/torch/extensions/brevitas/index.rst", "autoapi/quark/torch/extensions/brevitas/mapping/index.rst", "autoapi/quark/torch/extensions/brevitas/verification/index.rst", "autoapi/quark/torch/extensions/index.rst", "autoapi/quark/torch/index.rst", "autoapi/quark/torch/kernel/hw_emulation/extensions/index.rst", "autoapi/quark/torch/kernel/hw_emulation/hw_emulation_interface/index.rst", "autoapi/quark/torch/kernel/hw_emulation/index.rst", "autoapi/quark/torch/kernel/index.rst", "autoapi/quark/torch/pruning/api/index.rst", "autoapi/quark/torch/pruning/config/index.rst", "autoapi/quark/torch/pruning/index.rst", "autoapi/quark/torch/pruning/model_transformation/index.rst", "autoapi/quark/torch/pruning/utils/index.rst", "autoapi/quark/torch/quantization/api/index.rst", "autoapi/quark/torch/quantization/config/config/index.rst", "autoapi/quark/torch/quantization/config/config_verification/index.rst", "autoapi/quark/torch/quantization/config/index.rst", "autoapi/quark/torch/quantization/config/type/index.rst", "autoapi/quark/torch/quantization/config/utils/index.rst", "autoapi/quark/torch/quantization/constants/index.rst", "autoapi/quark/torch/quantization/debug/index.rst", "autoapi/quark/torch/quantization/graph/fx/base/index.rst", "autoapi/quark/torch/quantization/graph/fx/index.rst", "autoapi/quark/torch/quantization/graph/index.rst", "autoapi/quark/torch/quantization/graph/optimization/activate_dropout/index.rst", "autoapi/quark/torch/quantization/graph/optimization/index.rst", "autoapi/quark/torch/quantization/graph/optimization/model_optimization/index.rst", "autoapi/quark/torch/quantization/graph/optimization/modify_reshape_param/index.rst", "autoapi/quark/torch/quantization/graph/optimization/opt_pass_manager/index.rst", "autoapi/quark/torch/quantization/graph/optimization/post_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/post_quant/opt_pass_after_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/opt_pass_before_quant/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv2d_to_qtconv2d/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_conv_bn_to_qt_model/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_convtranspose2d_to_qtconvtranspose2d/index.rst", "autoapi/quark/torch/quantization/graph/optimization/pre_quant/replace_linear_to_qtlinear/index.rst", "autoapi/quark/torch/quantization/graph/optimization/remove_dropout_node/index.rst", "autoapi/quark/torch/quantization/graph/optimization/utils/index.rst", "autoapi/quark/torch/quantization/graph/processor/index.rst", "autoapi/quark/torch/quantization/graph/processor/insert_quantizer/index.rst", "autoapi/quark/torch/quantization/graph/processor/processor/index.rst", "autoapi/quark/torch/quantization/graph/processor/processor_utils/index.rst", "autoapi/quark/torch/quantization/graph/torch_utils/index.rst", "autoapi/quark/torch/quantization/index.rst", "autoapi/quark/torch/quantization/model_transformation/index.rst", "autoapi/quark/torch/quantization/nn/index.rst", "autoapi/quark/torch/quantization/nn/modules/index.rst", "autoapi/quark/torch/quantization/nn/modules/mixin/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_conv/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_conv_bn_fused/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_embed/index.rst", "autoapi/quark/torch/quantization/nn/modules/quantize_linear/index.rst", "autoapi/quark/torch/quantization/nn/utils/index.rst", "autoapi/quark/torch/quantization/observer/index.rst", "autoapi/quark/torch/quantization/observer/lsq_observer/index.rst", "autoapi/quark/torch/quantization/observer/observer/index.rst", "autoapi/quark/torch/quantization/observer/tqt_observer/index.rst", "autoapi/quark/torch/quantization/tensor_quantize/index.rst", "autoapi/quark/torch/quantization/utils/index.rst", "autoapi/quark/torch/utils/index.rst", "autoapi/quark/torch/utils/pack/index.rst", "autoapi/quark/version/index.rst", "basic_usage.rst", "index.rst", "install.rst", "intro.rst", "onnx/accuracy_algorithms/ada.rst", "onnx/accuracy_algorithms/cle.rst", "onnx/accuracy_algorithms/quarot.rst", "onnx/accuracy_algorithms/sq.rst", "onnx/accuracy_improvement_algorithms.rst", "onnx/appendix_full_quant_config_features.rst", "onnx/basic_usage_onnx.rst", "onnx/bfp16.rst", "onnx/config/calibration_datasets.rst", "onnx/config/calibration_methods.rst", "onnx/config/quantization_schemes.rst", "onnx/config/quantization_strategies.rst", "onnx/config/quantization_symmetry.rst", "onnx/config/user_guide_onnx_model_inference_save_input_npy.rst", "onnx/example_quark_onnx_BFP.rst", "onnx/example_quark_onnx_MX.rst", "onnx/example_quark_onnx_adaquant.rst", "onnx/example_quark_onnx_adaround.rst", "onnx/example_quark_onnx_auto_search.rst", "onnx/example_quark_onnx_cle.rst", "onnx/example_quark_onnx_dynamic_quantization_llama2.rst", "onnx/example_quark_onnx_dynamic_quantization_opt.rst", "onnx/example_quark_onnx_gptq.rst", "onnx/example_quark_onnx_image_classification.rst", "onnx/example_quark_onnx_language_models.rst", "onnx/example_quark_onnx_mixed_precision.rst", "onnx/example_quark_onnx_quarot.rst", "onnx/example_quark_onnx_ryzenai.rst", "onnx/example_quark_onnx_ryzenai_yolov3_customer_evaluator.rst", "onnx/example_quark_onnx_smoothquant.rst", "onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.rst", "onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.rst", "onnx/example_ryzenai_autosearch_resnet50.rst", "onnx/gpu_usage_guide.rst", "onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.rst", "onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.rst", "onnx/onnx_apis.rst", "onnx/onnx_examples.rst", "onnx/onnx_faq.rst", "onnx/optional_utilities.rst", "onnx/ryzen_ai_best_practice.rst", "onnx/tools.rst", "onnx/tutorial_bf16_quantization.rst", "onnx/tutorial_bfp16_quantization.rst", "onnx/tutorial_microexponents_quantization.rst", "onnx/tutorial_microscaling_quantization.rst", "onnx/tutorial_mix_precision.rst", "onnx/user_guide_auto_search.rst", "onnx/user_guide_config_description.rst", "onnx/user_guide_supported_optype_datatype.rst", "pytorch/adv_mx.rst", "pytorch/adv_two_level.rst", "pytorch/basic_usage_pytorch.rst", "pytorch/calibration_datasets.rst", "pytorch/calibration_methods.rst", "pytorch/debug.rst", "pytorch/example_quark_torch_brevitas.rst", "pytorch/example_quark_torch_diffusers.rst", "pytorch/example_quark_torch_llm_eval.rst", "pytorch/example_quark_torch_llm_eval_harness.rst", "pytorch/example_quark_torch_llm_eval_harness_offline.rst", "pytorch/example_quark_torch_llm_eval_perplexity.rst", "pytorch/example_quark_torch_llm_eval_rouge_meteor.rst", "pytorch/example_quark_torch_llm_pruning.rst", "pytorch/example_quark_torch_llm_ptq.rst", "pytorch/example_quark_torch_llm_qat.rst", "pytorch/example_quark_torch_pytorch_light.rst", "pytorch/example_quark_torch_vision.rst", "pytorch/export/gguf_llamacpp.rst", "pytorch/export/quark_export.rst", "pytorch/export/quark_export_gguf.rst", "pytorch/export/quark_export_hf.rst", "pytorch/export/quark_export_oga.rst", "pytorch/export/quark_export_onnx.rst", "pytorch/export/quark_export_quark.rst", "pytorch/extensions.rst", "pytorch/llm_quark.rst", "pytorch/pytorch_apis.rst", "pytorch/pytorch_examples.rst", "pytorch/pytorch_faq.rst", "pytorch/quantization_schemes.rst", "pytorch/quantization_strategies.rst", "pytorch/quantization_symmetry.rst", "pytorch/quark_save_load.rst", "pytorch/quark_torch_best_practices.rst", "pytorch/smoothquant.rst", "pytorch/tutorial_bfp16.rst", "pytorch/tutorial_quarot.rst", "pytorch/tutorial_rotation.rst", "pytorch/user_guide_config_description.rst", "release_note.rst"], "indexentries": {"activatedropoutnode (class in quark.torch.quantization.graph.optimization.activate_dropout)": [[209, "quark.torch.quantization.graph.optimization.activate_dropout.ActivateDropoutNode", false]], "activation_stats_hook() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.activation_stats_hook", false]], "activationequalization (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.ActivationEqualization", false]], "adaroundconstants (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundConstants", false]], "adaroundintquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer", false]], "add_checks() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassmanager method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager.add_checks", false]], "add_pass() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassmanager method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager.add_pass", false]], "add_pre_optimization_config() (quark.torch.quantization.config.config.config method)": [[199, "quark.torch.quantization.config.config.Config.add_pre_optimization_config", false]], "add_qk_rotation_after_function_call_in_forward() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.add_qk_rotation_after_function_call_in_forward", false]], "add_wrapper_after_function_call_in_method() (in module quark.torch.algorithm.quarot.monkeypatch)": [[138, "quark.torch.algorithm.quarot.monkeypatch.add_wrapper_after_function_call_in_method", false]], "addqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform", false]], "adjust_quantize_info() (in module quark.onnx.refine)": [[72, "quark.onnx.refine.adjust_quantize_info", false]], "algoconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.AlgoConfig", false]], "algoconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AlgoConfig", false]], "algoconfigbase (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.AlgoConfigBase", false]], "algoconfigbase (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AlgoConfigBase", false]], "align_quantize_info() (in module quark.onnx.refine)": [[72, "quark.onnx.refine.align_quantize_info", false]], "allow_exported_model_train_eval() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.allow_exported_model_train_eval", false]], "allow_multi_consumers (quark.onnx.graph_transformations.transforms.transform property)": [[33, "quark.onnx.graph_transformations.transforms.Transform.allow_multi_consumers", false]], "apply() (quark.onnx.graph_transformations.transforms_pipeline.transformspipeline method)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline.apply", false]], "apply() (quark.onnx.optimizations.convert_transforms_pipeline.convertqdqtoqoptransformspipeline method)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline.apply", false]], "apply() (quark.onnx.optimizations.convert_transforms_pipeline.removeqdqtransformspipeline method)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline.apply", false]], "assembleidxs (class in quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.AssembleIdxs", false]], "augment_graph() (quark.onnx.calibrate.powoftwocalibrater method)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater.augment_graph", false]], "auto_mixprecision() (in module quark.onnx.mprecision.auto_mixprecision)": [[36, "quark.onnx.mprecision.auto_mixprecision.auto_mixprecision", false]], "autosmoothquantconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AutoSmoothQuantConfig", false]], "autosmoothquantprocessor (class in quark.torch.algorithm.awq.auto_smooth)": [[121, "quark.torch.algorithm.awq.auto_smooth.AutoSmoothQuantProcessor", false]], "average_l2() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.average_L2", false]], "awqconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.AWQConfig", false]], "awqprocessor (class in quark.torch.algorithm.awq.awq)": [[122, "quark.torch.algorithm.awq.awq.AwqProcessor", false]], "backend (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.Backend", false]], "backward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.bfpprimequantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPPrimeQuantDequantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.bfpquantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantDequantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.mxquantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantDequantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intdequantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTDeQuantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantdequantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantDequantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantFunction.backward", false]], "backward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.roundhalftoeven static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.RoundHalfToEven.backward", false]], "backward() (quark.torch.kernel.dequante4m3function static method)": [[192, "quark.torch.kernel.DequantE4M3Function.backward", false]], "backward() (quark.torch.kernel.dequante5m2function static method)": [[192, "quark.torch.kernel.DequantE5M2Function.backward", false]], "backward() (quark.torch.kernel.dequantizefunction static method)": [[192, "quark.torch.kernel.DeQuantizeFunction.backward", false]], "backward() (quark.torch.kernel.lsqquantize static method)": [[192, "quark.torch.kernel.LSQQuantize.backward", false]], "backward() (quark.torch.kernel.nonscaledfakequantizefunction static method)": [[192, "quark.torch.kernel.NonScaledFakeQuantizeFunction.backward", false]], "backward() (quark.torch.kernel.nonscaledrealquantizefunction static method)": [[192, "quark.torch.kernel.NonScaledRealQuantizeFunction.backward", false]], "backward() (quark.torch.kernel.quante4m3function static method)": [[192, "quark.torch.kernel.QuantE4M3Function.backward", false]], "backward() (quark.torch.kernel.quante5m2function static method)": [[192, "quark.torch.kernel.QuantE5M2Function.backward", false]], "backward() (quark.torch.kernel.scaledfakequantizefunction static method)": [[192, "quark.torch.kernel.ScaledFakeQuantizeFunction.backward", false]], "backward() (quark.torch.kernel.scaledrealquantizefunction static method)": [[192, "quark.torch.kernel.ScaledRealQuantizeFunction.backward", false]], "backward() (quark.torch.kernel.tqtquantize static method)": [[192, "quark.torch.kernel.TQTQuantize.backward", false]], "barplot() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.barplot", false]], "basealgoprocessor (class in quark.torch.algorithm.processor)": [[136, "quark.torch.algorithm.processor.BaseAlgoProcessor", false]], "basevocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.BaseVocab", false]], "batchnorm_ops (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.BATCHNORM_OPS", false]], "bfloat16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Bfloat16Spec", false]], "bfp16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.BFP16Spec", false]], "bfpprimequantdequantfunction (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPPrimeQuantDequantFunction", false]], "bfpquantdequantfunction (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantDequantFunction", false]], "bfpquantizer (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantizer", false]], "biascorrection (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.BiasCorrection", false]], "blockwisetuningconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.BlockwiseTuningConfig", false]], "blockwisetuningprocessor (class in quark.torch.algorithm.blockwise_tuning.blockwise_tuning)": [[128, "quark.torch.algorithm.blockwise_tuning.blockwise_tuning.BlockwiseTuningProcessor", false]], "bpevocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.BpeVocab", false]], "buildin_eval_func() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.buildin_eval_func", false]], "cacheddatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.CachedDataReader", false]], "cacheddataset (class in quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.CachedDataset", false]], "calc_recon_loss() (quark.onnx.finetuning.train_torch.train_model_loss.trainloss static method)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss.calc_recon_loss", false]], "calc_round_loss() (quark.onnx.finetuning.train_torch.train_model_loss.trainloss class method)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss.calc_round_loss", false]], "calculate_qparams() (quark.torch.quantization.observer.observer.pergroupminmaxobserver method)": [[241, "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver.calculate_qparams", false]], "calculate_qparams() (quark.torch.quantization.observer.observer.uniformscalingobserver method)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver.calculate_qparams", false]], "calculate_quantization_params() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.calculate_quantization_params", false]], "call() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.call", false]], "call() (quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.convertclip2reluqopass method)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertbn2d2convqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertreducemean2gapqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass.call", false]], "call() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.splitquantmodulecalledoveronce method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce.call", false]], "cat_ops (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.CAT_OPS", false]], "check_hard_sigmoid_condition() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_hard_sigmoid_condition", false]], "check_min_max_valid() (in module quark.torch.quantization.nn.utils)": [[238, "quark.torch.quantization.nn.utils.check_min_max_valid", false]], "check_model_quantizable() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_model_quantizable", false]], "check_onnx_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_onnx_model", false]], "check_reduce_mean_condition() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_reduce_mean_condition", false]], "check_relu_like_node() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.check_relu_like_node", false]], "cle_pair_type (class in quark.onnx.equalization)": [[5, "quark.onnx.equalization.CLE_PAIR_TYPE", false]], "cle_transforms() (in module quark.onnx.equalization)": [[5, "quark.onnx.equalization.cle_transforms", false]], "clip (class in quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.Clip", false]], "collect() (quark.onnx.calibrate.powoftwocollector method)": [[3, "quark.onnx.calibrate.PowOfTwoCollector.collect", false]], "collect_data() (quark.onnx.calibrate.powoftwocalibrater method)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater.collect_data", false]], "collect_quantization_statistics() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.collect_quantization_statistics", false]], "compute_collection_result() (quark.onnx.calibrate.powoftwocollector method)": [[3, "quark.onnx.calibrate.PowOfTwoCollector.compute_collection_result", false]], "compute_range() (quark.onnx.calibrate.powoftwocalibrater method)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater.compute_range", false]], "compute_scale_zp() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.compute_scale_zp", false]], "compute_scale_zp_fp() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.compute_scale_zp_fp", false]], "config (class in quark.onnx.quantization.config.config)": [[57, "quark.onnx.quantization.config.config.Config", false]], "config (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.Config", false]], "config (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.Config", false]], "config (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Config", false]], "configbase (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.ConfigBase", false]], "configbase (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.ConfigBase", false]], "configverifier (class in quark.torch.extensions.brevitas.verification)": [[186, "quark.torch.extensions.brevitas.verification.ConfigVerifier", false]], "convert_act() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_act", false]], "convert_bn_to_conv() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_bn_to_conv", false]], "convert_clip_to_relu() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_clip_to_relu", false]], "convert_conv() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_conv", false]], "convert_customqdq_to_qdq() (in module quark.onnx.tools.convert_customqdq_to_qdq)": [[78, "quark.onnx.tools.convert_customqdq_to_qdq.convert_customqdq_to_qdq", false]], "convert_exported_model_to_gguf() (in module quark.torch.export.gguf_export.api)": [[154, "quark.torch.export.gguf_export.api.convert_exported_model_to_gguf", false]], "convert_float16_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float16_to_float", false]], "convert_float_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float_to_float16", false]], "convert_float_to_float16_model_path() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_float_to_float16_model_path", false]], "convert_gemm() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_gemm", false]], "convert_initializers_to_float() (in module quark.onnx.tools.convert_quant_to_float)": [[92, "quark.onnx.tools.convert_quant_to_float.convert_initializers_to_float", false]], "convert_lstm_to_customlstm() (in module quark.onnx.tools.convert_lstm_to_customlstm)": [[86, "quark.onnx.tools.convert_lstm_to_customlstm.convert_lstm_to_customlstm", false]], "convert_matmul() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_matmul", false]], "convert_norm() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_norm", false]], "convert_np_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_np_to_float", false]], "convert_np_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_np_to_float16", false]], "convert_onnx_to_torch() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.convert_onnx_to_torch", false]], "convert_ops_to_modules() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.convert_ops_to_modules", false]], "convert_qdq_to_qop() (in module quark.onnx.tools.convert_qdq_to_qop)": [[91, "quark.onnx.tools.convert_qdq_to_qop.convert_qdq_to_qop", false]], "convert_reduce_mean_to_global_avg_pool() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_reduce_mean_to_global_avg_pool", false]], "convert_split_to_slice() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.convert_split_to_slice", false]], "convert_tensor_float16_to_float() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_tensor_float16_to_float", false]], "convert_tensor_float_to_float16() (in module quark.onnx.tools.float16)": [[99, "quark.onnx.tools.float16.convert_tensor_float_to_float16", false]], "convert_torch_to_onnx() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.convert_torch_to_onnx", false]], "convertbn2d2convqopass (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass", false]], "convertclip2reluqopass (class in quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass", false]], "convertqdqtoqoptransformspipeline (class in quark.onnx.optimizations.convert_transforms_pipeline)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline", false]], "convertreducemean2gapqopass (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass", false]], "convqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform", false]], "copy_func_with_new_globals() (in module quark.torch.algorithm.quarot.monkeypatch)": [[138, "quark.torch.algorithm.quarot.monkeypatch.copy_func_with_new_globals", false]], "cos_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.cos_metric", false]], "create_calibrator_float_scale() (in module quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.create_calibrator_float_scale", false]], "create_calibrator_power_of_two() (in module quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.create_calibrator_power_of_two", false]], "create_session() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.create_session", false]], "custom_ops_infer_shapes() (in module quark.onnx.tools.convert_customqdq_to_qdq)": [[78, "quark.onnx.tools.convert_customqdq_to_qdq.custom_ops_infer_shapes", false]], "custom_ops_infer_shapes() (in module quark.onnx.tools.convert_lstm_to_customlstm)": [[86, "quark.onnx.tools.convert_lstm_to_customlstm.custom_ops_infer_shapes", false]], "customformatter (class in quark.shares.utils.log)": [[118, "quark.shares.utils.log.CustomFormatter", false]], "customqdq_to_contribqdq() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.customqdq_to_contribqdq", false]], "dataclass_pretty_string() (in module quark.torch.quantization.config.utils)": [[203, "quark.torch.quantization.config.utils.dataclass_pretty_string", false]], "datatypespec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.DataTypeSpec", false]], "delete_directory_content() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.delete_directory_content", false]], "dequante4m3function (class in quark.torch.kernel)": [[192, "quark.torch.kernel.DequantE4M3Function", false]], "dequante5m2function (class in quark.torch.kernel)": [[192, "quark.torch.kernel.DequantE5M2Function", false]], "dequantize_data() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dequantize_data", false]], "dequantizefunction (class in quark.torch.kernel)": [[192, "quark.torch.kernel.DeQuantizeFunction", false]], "devicetype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.DeviceType", false]], "distribution_plot() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.distribution_plot", false]], "dpu_leaky_relu_alpha() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dpu_leaky_relu_alpha", false]], "dtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.Dtype", false]], "dump_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.dump_model", false]], "duplicatefilter (class in quark.shares.utils.log)": [[118, "quark.shares.utils.log.DuplicateFilter", false]], "embeddingtype (class in quark.torch.export.json_export.builder.llm_info)": [[162, "quark.torch.export.json_export.builder.llm_info.EmbeddingType", false]], "ensures() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.ensures", false]], "entropycalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.EntropyCalibrater", false]], "equalization (class in quark.onnx.equalization)": [[5, "quark.onnx.equalization.Equalization", false]], "export_gguf_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_gguf_model", false]], "export_onnx_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_onnx_model", false]], "export_onnx_model() (quark.torch.extensions.brevitas.api.modelexporter method)": [[182, "quark.torch.extensions.brevitas.api.ModelExporter.export_onnx_model", false]], "export_quark_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.export_quark_model", false]], "exporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.ExporterConfig", false]], "extra_repr() (quark.torch.quantization.observer.observer.placeholderobserver method)": [[241, "quark.torch.quantization.observer.observer.PlaceholderObserver.extra_repr", false]], "extra_repr() (quark.torch.quantization.observer.observer.uniformscalingobserver method)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver.extra_repr", false]], "extra_repr() (quark.torch.quantization.tensor_quantize.scaledfakequantize method)": [[243, "quark.torch.quantization.tensor_quantize.ScaledFakeQuantize.extra_repr", false]], "extract_attr_values() (in module quark.onnx.finetuning.create_torch.create_model_utils)": [[11, "quark.onnx.finetuning.create_torch.create_model_utils.extract_attr_values", false]], "extract_padding_params() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_padding_params", false]], "extract_padding_params_for_conv() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_padding_params_for_conv", false]], "extract_weight_and_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.extract_weight_and_bias", false]], "fakequantizebase (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.FakeQuantizeBase", false]], "fast_finetune() (in module quark.onnx.finetuning.fast_finetune)": [[18, "quark.onnx.finetuning.fast_finetune.fast_finetune", false]], "filter() (quark.shares.utils.log.duplicatefilter method)": [[118, "quark.shares.utils.log.DuplicateFilter.filter", false]], "find_int16_scale() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.find_int16_scale", false]], "find_node_by_output() (in module quark.onnx.tools.remove_qdq_between_ops)": [[107, "quark.onnx.tools.remove_qdq_between_ops.find_node_by_output", false]], "find_node_by_output() (in module quark.onnx.tools.remove_qdq_mul_add)": [[108, "quark.onnx.tools.remove_qdq_mul_add.find_node_by_output", false]], "find_quant_scale_zp() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.find_quant_scale_zp", false]], "find_quantized_value() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.find_quantized_value", false]], "float16spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Float16Spec", false]], "fold_batch_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fold_batch_norm", false]], "fold_batch_norm_after_concat() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fold_batch_norm_after_concat", false]], "format() (quark.shares.utils.log.customformatter method)": [[118, "quark.shares.utils.log.CustomFormatter.format", false]], "forward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.bfpprimequantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPPrimeQuantDequantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.bfpquantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantDequantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_fn_quantizers.mxquantdequantfunction static method)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantDequantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intdequantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTDeQuantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantdequantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantDequantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantfunction static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantFunction.forward", false]], "forward() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.roundhalftoeven static method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.RoundHalfToEven.forward", false]], "forward() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.forward", false]], "forward() (quark.torch.algorithm.rotation.rotation_utils.rmsnorm method)": [[144, "quark.torch.algorithm.rotation.rotation_utils.RMSNorm.forward", false]], "forward() (quark.torch.export.nn.modules.qparamslinear.qparamslinear method)": [[177, "quark.torch.export.nn.modules.qparamslinear.QParamsLinear.forward", false]], "forward() (quark.torch.kernel.dequante4m3function static method)": [[192, "quark.torch.kernel.DequantE4M3Function.forward", false]], "forward() (quark.torch.kernel.dequante5m2function static method)": [[192, "quark.torch.kernel.DequantE5M2Function.forward", false]], "forward() (quark.torch.kernel.dequantizefunction static method)": [[192, "quark.torch.kernel.DeQuantizeFunction.forward", false]], "forward() (quark.torch.kernel.lsqquantize static method)": [[192, "quark.torch.kernel.LSQQuantize.forward", false]], "forward() (quark.torch.kernel.nonscaledfakequantizefunction static method)": [[192, "quark.torch.kernel.NonScaledFakeQuantizeFunction.forward", false]], "forward() (quark.torch.kernel.nonscaledrealquantizefunction static method)": [[192, "quark.torch.kernel.NonScaledRealQuantizeFunction.forward", false]], "forward() (quark.torch.kernel.quante4m3function static method)": [[192, "quark.torch.kernel.QuantE4M3Function.forward", false]], "forward() (quark.torch.kernel.quante5m2function static method)": [[192, "quark.torch.kernel.QuantE5M2Function.forward", false]], "forward() (quark.torch.kernel.scaledfakequantizefunction static method)": [[192, "quark.torch.kernel.ScaledFakeQuantizeFunction.forward", false]], "forward() (quark.torch.kernel.scaledrealquantizefunction static method)": [[192, "quark.torch.kernel.ScaledRealQuantizeFunction.forward", false]], "forward() (quark.torch.kernel.tqtquantize static method)": [[192, "quark.torch.kernel.TQTQuantize.forward", false]], "forward() (quark.torch.quantization.nn.modules.quantize_embed.quantembeddingbag method)": [[236, "quark.torch.quantization.nn.modules.quantize_embed.QuantEmbeddingBag.forward", false]], "forward() (quark.torch.quantization.observer.observer.pertensorhistogramobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserver.forward", false]], "forward() (quark.torch.quantization.observer.observer.pertensorminmaxobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver.forward", false]], "fp8e4m3perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerChannelSpec", false]], "fp8e4m3pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerGroupSpec", false]], "fp8e4m3pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E4M3PerTensorSpec", false]], "fp8e5m2perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerChannelSpec", false]], "fp8e5m2pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerGroupSpec", false]], "fp8e5m2pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.FP8E5M2PerTensorSpec", false]], "fpquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.FPQuantizer", false]], "freeze() (quark.torch.quantization.api.modelquantizer static method)": [[198, "quark.torch.quantization.api.ModelQuantizer.freeze", false]], "freeze_model() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.freeze_model", false]], "freezedscaledfakequantize (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.FreezedScaledFakeQuantize", false]], "from_module() (quark.torch.export.nn.modules.qparamslinear.qparamslinear class method)": [[177, "quark.torch.export.nn.modules.qparamslinear.QParamsLinear.from_module", false]], "fuse_instance_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fuse_instance_norm", false]], "fuse_l2_norm() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.fuse_l2_norm", false]], "generate_initializer() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.generate_initializer", false]], "generate_input_initializer() (in module quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.generate_input_initializer", false]], "get_annotate_tensors() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_annotate_tensors", false]], "get_bias() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.get_bias", false]], "get_clip_min_max() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_clip_min_max", false]], "get_configs() (quark.onnx.graph_transformations.transforms_pipeline.transformspipeline method)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline.get_configs", false]], "get_datatype_shape() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_datatype_shape", false]], "get_exclude_nodes() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_exclude_nodes", false]], "get_export_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.get_export_model", false]], "get_fix_position() (quark.torch.quantization.observer.tqt_observer.tqtobserver method)": [[242, "quark.torch.quantization.observer.tqt_observer.TQTObserver.get_fix_position", false]], "get_hadamard_matrices() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.get_hadamard_matrices", false]], "get_min_max_by_mse() (quark.torch.quantization.observer.observer.pertensormseobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorMSEObserver.get_min_max_by_mse", false]], "get_min_max_by_percentile() (quark.torch.quantization.observer.observer.pertensorpercentileobserver method)": [[241, "quark.torch.quantization.observer.observer.PerTensorPercentileObserver.get_min_max_by_percentile", false]], "get_modules_optimized_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.get_modules_optimized_bias", false]], "get_modules_optimized_weight() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.get_modules_optimized_weight", false]], "get_nested_attr_from_module() (in module quark.torch.algorithm.utils.module)": [[147, "quark.torch.algorithm.utils.module.get_nested_attr_from_module", false]], "get_next() (quark.onnx.quant_utils.cacheddatareader method)": [[55, "quark.onnx.quant_utils.CachedDataReader.get_next", false]], "get_next() (quark.onnx.quant_utils.pathdatareader method)": [[55, "quark.onnx.quant_utils.PathDataReader.get_next", false]], "get_next() (quark.onnx.quant_utils.randomdatareader method)": [[55, "quark.onnx.quant_utils.RandomDataReader.get_next", false]], "get_next() (quark.onnx.tools.save_tensor_hist.histdatareader method)": [[111, "quark.onnx.tools.save_tensor_hist.HistDataReader.get_next", false]], "get_qdq_to_remove() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qdq_to_remove", false]], "get_qmin_qmax_for_qtype() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qmin_qmax_for_qType", false]], "get_qrange_for_qtype() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.get_qrange_for_qType", false]], "get_rotation_matrix() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.get_rotation_matrix", false]], "get_tensor_value() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.get_tensor_value", false]], "get_weight() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.get_weight", false]], "gpfa2q (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPFA2Q", false]], "gpfq (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPFQ", false]], "gptq (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.GPTQ", false]], "gptqconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.GPTQConfig", false]], "gptqprocessor (class in quark.torch.algorithm.gptq.gptq)": [[131, "quark.torch.algorithm.gptq.gptq.GptqProcessor", false]], "graphtransform (class in quark.torch.quantization.graph.fx.base)": [[206, "quark.torch.quantization.graph.fx.base.GraphTransform", false]], "hadamard_multiply() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.hadamard_multiply", false]], "hadamard_transform() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.hadamard_transform", false]], "hardmard_transform() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.hardmard_transform", false]], "histdatareader (class in quark.onnx.tools.save_tensor_hist)": [[111, "quark.onnx.tools.save_tensor_hist.HistDataReader", false]], "import_model() (quark.torch.export.api.modelimporter method)": [[150, "quark.torch.export.api.ModelImporter.import_model", false]], "import_model_info() (quark.torch.export.api.modelimporter method)": [[150, "quark.torch.export.api.ModelImporter.import_model_info", false]], "in_place_replace_layer() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.in_place_replace_layer", false]], "infer_shape() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.infer_shape", false]], "inference_model() (in module quark.onnx.finetuning.onnx_evaluate)": [[20, "quark.onnx.finetuning.onnx_evaluate.inference_model", false]], "initialize_alpha() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.adaroundintquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer.initialize_alpha", false]], "insert_quantizer() (in module quark.torch.quantization.graph.processor.insert_quantizer)": [[225, "quark.torch.quantization.graph.processor.insert_quantizer.insert_quantizer", false]], "insert_stats_hooks() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.insert_stats_hooks", false]], "int16method (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.Int16Method", false]], "int4perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerChannelSpec", false]], "int4pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerGroupSpec", false]], "int4pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int4PerTensorSpec", false]], "int8perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerChannelSpec", false]], "int8pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerGroupSpec", false]], "int8pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Int8PerTensorSpec", false]], "intdequantfunction (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTDeQuantFunction", false]], "intquantdequantfunction (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantDequantFunction", false]], "intquantfunction (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantFunction", false]], "intquantizer (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer", false]], "is_approximately_equal() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_approximately_equal", false]], "is_batchnorm2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_batchnorm2d_node", false]], "is_cat_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_cat_node", false]], "is_clip_with_min_max() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_clip_with_min_max", false]], "is_conv1d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv1d_node", false]], "is_conv2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv2d_node", false]], "is_conv3d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_conv3d_node", false]], "is_convtranspose2d_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_convtranspose2d_node", false]], "is_dropout_node() (in module quark.torch.quantization.graph.torch_utils)": [[228, "quark.torch.quantization.graph.torch_utils.is_dropout_node", false]], "is_leaky_relu_with_alpha() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_leaky_relu_with_alpha", false]], "is_node_needs_annotated() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_node_needs_annotated", false]], "is_ort_version_below() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.is_ort_version_below", false]], "jsonexporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.JsonExporterConfig", false]], "l1_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.l1_metric", false]], "l2_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.l2_metric", false]], "layernormtype (class in quark.torch.export.json_export.builder.llm_info)": [[162, "quark.torch.export.json_export.builder.llm_info.LayerNormType", false]], "llamamodelwriter (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.LlamaModelWriter", false]], "load_params() (in module quark.torch.quantization.api)": [[198, "quark.torch.quantization.api.load_params", false]], "load_pre_optimization_config_from_file() (in module quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.load_pre_optimization_config_from_file", false]], "load_quant_algo_config_from_file() (in module quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.load_quant_algo_config_from_file", false]], "load_weight_and_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.load_weight_and_bias", false]], "lsqobserver (class in quark.torch.quantization.observer.lsq_observer)": [[240, "quark.torch.quantization.observer.lsq_observer.LSQObserver", false]], "lsqquantize (class in quark.torch.kernel)": [[192, "quark.torch.kernel.LSQQuantize", false]], "mark_exclude_nodes() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.mark_exclude_nodes", false]], "matmulnbitsquantizer (class in quark.onnx.quantizers.matmul_nbits_quantizer)": [[66, "quark.onnx.quantizers.matmul_nbits_quantizer.MatMulNBitsQuantizer", false]], "matmulqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform", false]], "minmaxcalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.MinMaxCalibrater", false]], "modelexporter (class in quark.torch.export.api)": [[150, "quark.torch.export.api.ModelExporter", false]], "modelexporter (class in quark.torch.extensions.brevitas.api)": [[182, "quark.torch.extensions.brevitas.api.ModelExporter", false]], "modelimporter (class in quark.torch.export.api)": [[150, "quark.torch.export.api.ModelImporter", false]], "modeloptimizer (class in quark.onnx.finetuning.train_torch.train_model)": [[25, "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer", false]], "modelpruner (class in quark.torch.pruning.api)": [[193, "quark.torch.pruning.api.ModelPruner", false]], "modelquantizer (class in quark.onnx.quantization.api)": [[56, "quark.onnx.quantization.api.ModelQuantizer", false]], "modelquantizer (class in quark.torch.extensions.brevitas.api)": [[182, "quark.torch.extensions.brevitas.api.ModelQuantizer", false]], "modelquantizer (class in quark.torch.quantization.api)": [[198, "quark.torch.quantization.api.ModelQuantizer", false]], "modeltransformer (class in quark.onnx.graph_transformations.model_transformer)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer", false]], "modeltransformer.nodetype (class in quark.onnx.graph_transformations.model_transformer)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer.NodeType", false]], "modeltransformertest (class in quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest", false]], "modeltransformertest.removerelu (class in quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.RemoveRelu", false]], "modeltransformertest.replacewholemodel (class in quark.onnx.graph_transformations.model_transformer_test)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.ReplaceWholeModel", false]], "modelwriter (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.ModelWriter", false]], "modified_annotate_input() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.modified_annotate_input", false]], "modify_reshape_param() (in module quark.torch.quantization.graph.optimization.modify_reshape_param)": [[212, "quark.torch.quantization.graph.optimization.modify_reshape_param.modify_reshape_param", false]], "module": [[0, "module-quark", false], [1, "module-quark.onnx.auto_search", false], [2, "module-quark.onnx.bias_correction", false], [3, "module-quark.onnx.calibrate", false], [4, "module-quark.onnx.cpu_quantizer", false], [5, "module-quark.onnx.equalization", false], [6, "module-quark.onnx.finetuning.create_torch.base_fn_quantizers", false], [7, "module-quark.onnx.finetuning.create_torch.base_qdq_quantizers", false], [8, "module-quark.onnx.finetuning.create_torch.create_model", false], [9, "module-quark.onnx.finetuning.create_torch.create_model_ops", false], [10, "module-quark.onnx.finetuning.create_torch.create_model_test", false], [11, "module-quark.onnx.finetuning.create_torch.create_model_utils", false], [12, "module-quark.onnx.finetuning.create_torch", false], [13, "module-quark.onnx.finetuning.create_torch.quant_base_ops", false], [14, "module-quark.onnx.finetuning.create_torch.quant_conv_ops", false], [15, "module-quark.onnx.finetuning.create_torch.quant_gemm_ops", false], [16, "module-quark.onnx.finetuning.create_torch.quant_matmul_ops", false], [17, "module-quark.onnx.finetuning.create_torch.quant_norm_ops", false], [18, "module-quark.onnx.finetuning.fast_finetune", false], [19, "module-quark.onnx.finetuning", false], [20, "module-quark.onnx.finetuning.onnx_evaluate", false], [21, "module-quark.onnx.finetuning.onnx_subgraph", false], [22, "module-quark.onnx.finetuning.torch_utils", false], [23, "module-quark.onnx.finetuning.torch_utils_test", false], [24, "module-quark.onnx.finetuning.train_torch", false], [25, "module-quark.onnx.finetuning.train_torch.train_model", false], [26, "module-quark.onnx.finetuning.train_torch.train_model_loss", false], [27, "module-quark.onnx.finetuning.train_torch.train_model_param", false], [28, "module-quark.onnx.gptq.gptq", false], [29, "module-quark.onnx.gptq", false], [30, "module-quark.onnx.graph_transformations", false], [31, "module-quark.onnx.graph_transformations.model_transformer", false], [32, "module-quark.onnx.graph_transformations.model_transformer_test", false], [33, "module-quark.onnx.graph_transformations.transforms", false], [34, "module-quark.onnx.graph_transformations.transforms_pipeline", false], [35, "module-quark.onnx", false], [36, "module-quark.onnx.mprecision.auto_mixprecision", false], [37, "module-quark.onnx.mprecision", false], [38, "module-quark.onnx.mprecision.mixed_bfp", false], [39, "module-quark.onnx.onnx_quantizer", false], [40, "module-quark.onnx.operators.custom_ops.build_vai_custom_op", false], [41, "module-quark.onnx.operators.custom_ops", false], [42, "module-quark.onnx.operators", false], [43, "module-quark.onnx.operators.vai_ops.concat", false], [44, "module-quark.onnx.operators.vai_ops.hardsigmoid", false], [45, "module-quark.onnx.operators.vai_ops", false], [46, "module-quark.onnx.operators.vai_ops.layernorm", false], [47, "module-quark.onnx.operators.vai_ops.prelu", false], [48, "module-quark.onnx.operators.vai_ops.qdq_ops", false], [49, "module-quark.onnx.operators.vai_ops.softmax", false], [50, "module-quark.onnx.optimizations.convert_transforms", false], [51, "module-quark.onnx.optimizations.convert_transforms_pipeline", false], [52, "module-quark.onnx.optimizations", false], [53, "module-quark.onnx.optimize", false], [54, "module-quark.onnx.qdq_quantizer", false], [55, "module-quark.onnx.quant_utils", false], [56, "module-quark.onnx.quantization.api", false], [57, "module-quark.onnx.quantization.config.config", false], [58, "module-quark.onnx.quantization.config.custom_config", false], [59, "module-quark.onnx.quantization.config", false], [60, "module-quark.onnx.quantization", false], [61, "module-quark.onnx.quantize", false], [62, "module-quark.onnx.quantizers.bfp_quantizer", false], [63, "module-quark.onnx.quantizers.cpu_quantizer", false], [64, "module-quark.onnx.quantizers.extended_quantizer", false], [65, "module-quark.onnx.quantizers", false], [66, "module-quark.onnx.quantizers.matmul_nbits_quantizer", false], [67, "module-quark.onnx.quantizers.npu_cnn_quantizer", false], [68, "module-quark.onnx.quantizers.npu_transformer_quantizer", false], [69, "module-quark.onnx.quantizers.onnx_quantizer", false], [70, "module-quark.onnx.quantizers.qdq_quantizer", false], [71, "module-quark.onnx.quarot", false], [72, "module-quark.onnx.refine", false], [73, "module-quark.onnx.registry", false], [74, "module-quark.onnx.simulate_dpu", false], [75, "module-quark.onnx.simulate_dpu_softmax", false], [76, "module-quark.onnx.smooth_quant", false], [77, "module-quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu", false], [78, "module-quark.onnx.tools.convert_customqdq_to_qdq", false], [79, "module-quark.onnx.tools.convert_dynamic_to_fixed", false], [80, "module-quark.onnx.tools.convert_fp16_to_bf16", false], [81, "module-quark.onnx.tools.convert_fp16_to_bfp16", false], [82, "module-quark.onnx.tools.convert_fp16_to_fp32", false], [83, "module-quark.onnx.tools.convert_fp32_to_bf16", false], [84, "module-quark.onnx.tools.convert_fp32_to_bfp16", false], [85, "module-quark.onnx.tools.convert_fp32_to_fp16", false], [86, "module-quark.onnx.tools.convert_lstm_to_customlstm", false], [87, "module-quark.onnx.tools.convert_nchw_to_nhwc", false], [88, "module-quark.onnx.tools.convert_onnx_to_onnxtxt", false], [89, "module-quark.onnx.tools.convert_onnxtxt_to_onnx", false], [90, "module-quark.onnx.tools.convert_opset_version", false], [91, "module-quark.onnx.tools.convert_qdq_to_qop", false], [92, "module-quark.onnx.tools.convert_quant_to_float", false], [93, "module-quark.onnx.tools.convert_resize_fs_to_pof2s", false], [94, "module-quark.onnx.tools.convert_s8s8_to_u8s8", false], [95, "module-quark.onnx.tools.convert_shared_initializer_to_unique", false], [96, "module-quark.onnx.tools.convert_u16s8_to_s16s8", false], [97, "module-quark.onnx.tools.convert_u16u8_to_u8u8", false], [98, "module-quark.onnx.tools.evaluate", false], [99, "module-quark.onnx.tools.float16", false], [100, "module-quark.onnx.tools", false], [101, "module-quark.onnx.tools.insert_clip_bfloat16_qdq", false], [102, "module-quark.onnx.tools.print_a16w8_a8w8_nodes", false], [103, "module-quark.onnx.tools.random_quantize", false], [104, "module-quark.onnx.tools.remove_bf16_cast", false], [105, "module-quark.onnx.tools.remove_initializer_from_input", false], [106, "module-quark.onnx.tools.remove_qdq", false], [107, "module-quark.onnx.tools.remove_qdq_between_ops", false], [108, "module-quark.onnx.tools.remove_qdq_mul_add", false], [109, "module-quark.onnx.tools.replace_bfloat16_qdq_cast", false], [110, "module-quark.onnx.tools.replace_inf_weights", false], [111, "module-quark.onnx.tools.save_tensor_hist", false], [112, "module-quark.onnx.tools.save_weights_hist", false], [113, "module-quark.onnx.utils", false], [114, "module-quark.onnx.utils.model_utils", false], [115, "module-quark.shares", false], [116, "module-quark.shares.utils.import_utils", false], [117, "module-quark.shares.utils", false], [118, "module-quark.shares.utils.log", false], [119, "module-quark.shares.utils.testing_utils", false], [120, "module-quark.torch.algorithm.api", false], [121, "module-quark.torch.algorithm.awq.auto_smooth", false], [122, "module-quark.torch.algorithm.awq.awq", false], [123, "module-quark.torch.algorithm.awq", false], [124, "module-quark.torch.algorithm.awq.modules.act", false], [125, "module-quark.torch.algorithm.awq.modules", false], [126, "module-quark.torch.algorithm.awq.scale", false], [127, "module-quark.torch.algorithm.awq.smooth", false], [128, "module-quark.torch.algorithm.blockwise_tuning.blockwise_tuning", false], [129, "module-quark.torch.algorithm.blockwise_tuning.blockwise_utils", false], [130, "module-quark.torch.algorithm.blockwise_tuning", false], [131, "module-quark.torch.algorithm.gptq.gptq", false], [132, "module-quark.torch.algorithm.gptq", false], [133, "module-quark.torch.algorithm", false], [134, "module-quark.torch.algorithm.osscar", false], [135, "module-quark.torch.algorithm.osscar.osscar", false], [136, "module-quark.torch.algorithm.processor", false], [137, "module-quark.torch.algorithm.quarot", false], [138, "module-quark.torch.algorithm.quarot.monkeypatch", false], [139, "module-quark.torch.algorithm.quarot.quarot", false], [140, "module-quark.torch.algorithm.quarot.utils", false], [141, "module-quark.torch.algorithm.rotation.hadamard", false], [142, "module-quark.torch.algorithm.rotation", false], [143, "module-quark.torch.algorithm.rotation.rotation", false], [144, "module-quark.torch.algorithm.rotation.rotation_utils", false], [145, "module-quark.torch.algorithm.utils.auto_config", false], [146, "module-quark.torch.algorithm.utils", false], [147, "module-quark.torch.algorithm.utils.module", false], [148, "module-quark.torch.algorithm.utils.prepare", false], [149, "module-quark.torch.algorithm.utils.utils", false], [150, "module-quark.torch.export.api", false], [151, "module-quark.torch.export.config.config", false], [152, "module-quark.torch.export.config", false], [153, "module-quark.torch.export.constants", false], [154, "module-quark.torch.export.gguf_export.api", false], [155, "module-quark.torch.export.gguf_export.gguf_model_converter", false], [156, "module-quark.torch.export.gguf_export.gguf_model_writer", false], [157, "module-quark.torch.export.gguf_export", false], [158, "module-quark.torch.export.gguf_export.tensor_convert", false], [159, "module-quark.torch.export.gguf_export.utils", false], [160, "module-quark.torch.export", false], [161, "module-quark.torch.export.json_export.builder", false], [162, "module-quark.torch.export.json_export.builder.llm_info", false], [163, "module-quark.torch.export.json_export.builder.llm_info_builder", false], [164, "module-quark.torch.export.json_export.builder.native_model_info_builder", false], [165, "module-quark.torch.export.json_export.converter", false], [166, "module-quark.torch.export.json_export.converter.llm_info_converter", false], [167, "module-quark.torch.export.json_export", false], [168, "module-quark.torch.export.json_export.utils", false], [169, "module-quark.torch.export.json_export.utils.utils", false], [170, "module-quark.torch.export.main_export", false], [171, "module-quark.torch.export.main_export.model_post_process", false], [172, "module-quark.torch.export.main_export.quant_config_parser", false], [173, "module-quark.torch.export.main_import", false], [174, "module-quark.torch.export.main_import.pretrained_config", false], [175, "module-quark.torch.export.nn", false], [176, "module-quark.torch.export.nn.modules", false], [177, "module-quark.torch.export.nn.modules.qparamslinear", false], [178, "module-quark.torch.export.nn.modules.realquantizer", false], [179, "module-quark.torch.export.onnx", false], [180, "module-quark.torch.export.utils", false], [181, "module-quark.torch.extensions.brevitas.algos", false], [182, "module-quark.torch.extensions.brevitas.api", false], [183, "module-quark.torch.extensions.brevitas.config", false], [184, "module-quark.torch.extensions.brevitas", false], [185, "module-quark.torch.extensions.brevitas.mapping", false], [186, "module-quark.torch.extensions.brevitas.verification", false], [187, "module-quark.torch.extensions", false], [188, "module-quark.torch", false], [189, "module-quark.torch.kernel.hw_emulation.extensions", false], [190, "module-quark.torch.kernel.hw_emulation.hw_emulation_interface", false], [191, "module-quark.torch.kernel.hw_emulation", false], [192, "module-quark.torch.kernel", false], [193, "module-quark.torch.pruning.api", false], [194, "module-quark.torch.pruning.config", false], [195, "module-quark.torch.pruning", false], [196, "module-quark.torch.pruning.model_transformation", false], [197, "module-quark.torch.pruning.utils", false], [198, "module-quark.torch.quantization.api", false], [199, "module-quark.torch.quantization.config.config", false], [200, "module-quark.torch.quantization.config.config_verification", false], [201, "module-quark.torch.quantization.config", false], [202, "module-quark.torch.quantization.config.type", false], [203, "module-quark.torch.quantization.config.utils", false], [204, "module-quark.torch.quantization.constants", false], [205, "module-quark.torch.quantization.debug", false], [206, "module-quark.torch.quantization.graph.fx.base", false], [207, "module-quark.torch.quantization.graph.fx", false], [208, "module-quark.torch.quantization.graph", false], [209, "module-quark.torch.quantization.graph.optimization.activate_dropout", false], [210, "module-quark.torch.quantization.graph.optimization", false], [211, "module-quark.torch.quantization.graph.optimization.model_optimization", false], [212, "module-quark.torch.quantization.graph.optimization.modify_reshape_param", false], [213, "module-quark.torch.quantization.graph.optimization.opt_pass_manager", false], [214, "module-quark.torch.quantization.graph.optimization.post_quant", false], [215, "module-quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant", false], [216, "module-quark.torch.quantization.graph.optimization.pre_quant", false], [217, "module-quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant", false], [218, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d", false], [219, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model", false], [220, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d", false], [221, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear", false], [222, "module-quark.torch.quantization.graph.optimization.remove_dropout_node", false], [223, "module-quark.torch.quantization.graph.optimization.utils", false], [224, "module-quark.torch.quantization.graph.processor", false], [225, "module-quark.torch.quantization.graph.processor.insert_quantizer", false], [226, "module-quark.torch.quantization.graph.processor.processor", false], [227, "module-quark.torch.quantization.graph.processor.processor_utils", false], [228, "module-quark.torch.quantization.graph.torch_utils", false], [229, "module-quark.torch.quantization", false], [230, "module-quark.torch.quantization.model_transformation", false], [231, "module-quark.torch.quantization.nn", false], [232, "module-quark.torch.quantization.nn.modules", false], [233, "module-quark.torch.quantization.nn.modules.mixin", false], [234, "module-quark.torch.quantization.nn.modules.quantize_conv", false], [235, "module-quark.torch.quantization.nn.modules.quantize_conv_bn_fused", false], [236, "module-quark.torch.quantization.nn.modules.quantize_embed", false], [237, "module-quark.torch.quantization.nn.modules.quantize_linear", false], [238, "module-quark.torch.quantization.nn.utils", false], [239, "module-quark.torch.quantization.observer", false], [240, "module-quark.torch.quantization.observer.lsq_observer", false], [241, "module-quark.torch.quantization.observer.observer", false], [242, "module-quark.torch.quantization.observer.tqt_observer", false], [243, "module-quark.torch.quantization.tensor_quantize", false], [244, "module-quark.torch.quantization.utils", false], [245, "module-quark.torch.utils", false], [246, "module-quark.torch.utils.pack", false], [247, "module-quark.version", false]], "mulqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform", false]], "mx6spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MX6Spec", false]], "mx9spec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MX9Spec", false]], "mxquantdequantfunction (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantDequantFunction", false]], "mxquantizer (class in quark.onnx.finetuning.create_torch.base_fn_quantizers)": [[6, "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantizer", false]], "mxspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.MXSpec", false]], "nodetree (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.NodeTree", false]], "nonscaledfakequantize (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.NonScaledFakeQuantize", false]], "nonscaledfakequantizefunction (class in quark.torch.kernel)": [[192, "quark.torch.kernel.NonScaledFakeQuantizeFunction", false]], "nonscaledrealquantizefunction (class in quark.torch.kernel)": [[192, "quark.torch.kernel.NonScaledRealQuantizeFunction", false]], "nonscaledrealquantizer (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer", false]], "novocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.NoVocab", false]], "observerbase (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.ObserverBase", false]], "onnxexporterconfig (class in quark.torch.export.config.config)": [[151, "quark.torch.export.config.config.OnnxExporterConfig", false]], "onnxquantizer (class in quark.onnx.onnx_quantizer)": [[39, "quark.onnx.onnx_quantizer.ONNXQuantizer", false]], "optimize (class in quark.onnx.optimize)": [[53, "quark.onnx.optimize.Optimize", false]], "optimize() (in module quark.onnx.optimize)": [[53, "quark.onnx.optimize.optimize", false]], "optimize_module() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.optimize_module", false]], "optpassbase (class in quark.torch.quantization.graph.optimization.opt_pass_manager)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase", false]], "optpassmanager (class in quark.torch.quantization.graph.optimization.opt_pass_manager)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager", false]], "optypepattern (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.OpTypePattern", false]], "osscarconfig (class in quark.torch.pruning.config)": [[194, "quark.torch.pruning.config.OSSCARConfig", false]], "osscarprocessor (class in quark.torch.algorithm.osscar.osscar)": [[135, "quark.torch.algorithm.osscar.osscar.OsscarProcessor", false]], "pack_qinfo() (quark.torch.export.nn.modules.qparamslinear.qparamslinear method)": [[177, "quark.torch.export.nn.modules.qparamslinear.QParamsLinear.pack_qinfo", false]], "param_is_symmetric() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.param_is_symmetric", false]], "paramtype (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.ParamType", false]], "parse_options_to_params() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.parse_options_to_params", false]], "pathdatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.PathDataReader", false]], "pattern() (quark.onnx.graph_transformations.model_transformer_test.modeltransformertest.removerelu method)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.RemoveRelu.pattern", false]], "pattern() (quark.onnx.graph_transformations.model_transformer_test.modeltransformertest.replacewholemodel method)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.ReplaceWholeModel.pattern", false]], "pattern() (quark.onnx.graph_transformations.transforms.transform method)": [[33, "quark.onnx.graph_transformations.transforms.Transform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.addqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.convqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.matmulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.mulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.removeqdqtransform method)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform.pattern", false]], "pattern() (quark.onnx.optimizations.convert_transforms.sigmoidqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform.pattern", false]], "perblockbfpobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerBlockBFPObserver", false]], "perblockmxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerBlockMXObserver", false]], "percentilecalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PercentileCalibrater", false]], "perchannelminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerChannelMinMaxObserver", false]], "pergroupminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver", false]], "pertensorhistogramobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserver", false]], "pertensorhistogramobserverpro (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorHistogramObserverPro", false]], "pertensorminmaxobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver", false]], "pertensormseobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorMSEObserver", false]], "pertensorpercentileobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PerTensorPercentileObserver", false]], "placeholderobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.PlaceholderObserver", false]], "pos2scale() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.pos2scale", false]], "pos2scale() (in module quark.onnx.tools.convert_resize_fs_to_pof2s)": [[93, "quark.onnx.tools.convert_resize_fs_to_pof2s.pos2scale", false]], "poweroftwomethod (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.PowerOfTwoMethod", false]], "powoftwocalibrater (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PowOfTwoCalibrater", false]], "powoftwocollector (class in quark.onnx.calibrate)": [[3, "quark.onnx.calibrate.PowOfTwoCollector", false]], "preprocess (class in quark.torch.extensions.brevitas.algos)": [[181, "quark.torch.extensions.brevitas.algos.Preprocess", false]], "preprocess_import_info() (in module quark.torch.export.utils)": [[180, "quark.torch.export.utils.preprocess_import_info", false]], "prequantoptconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.PreQuantOptConfig", false]], "print_quantize_dynamic_info() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.print_quantize_dynamic_info", false]], "print_quantize_info() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.print_quantize_info", false]], "process_model_transformation() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.process_model_transformation", false]], "pruning_model() (quark.torch.pruning.api.modelpruner method)": [[193, "quark.torch.pruning.api.ModelPruner.pruning_model", false]], "psnr_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.psnr_metric", false]], "qconv1d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv1d", false]], "qconv2d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv2d", false]], "qconv3d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConv3d", false]], "qconvtranspose1d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose1d", false]], "qconvtranspose2d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose2d", false]], "qconvtranspose3d (class in quark.onnx.finetuning.create_torch.quant_conv_ops)": [[14, "quark.onnx.finetuning.create_torch.quant_conv_ops.QConvTranspose3d", false]], "qdqnputransformerquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer", false]], "qdqquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.QDQQuantizer", false]], "qgemm (class in quark.onnx.finetuning.create_torch.quant_gemm_ops)": [[15, "quark.onnx.finetuning.create_torch.quant_gemm_ops.QGemm", false]], "qinstancenorm1d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm1d", false]], "qinstancenorm2d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm2d", false]], "qinstancenorm3d (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QInstanceNorm3d", false]], "qkrotation (class in quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.QKRotation", false]], "qlayernorm (class in quark.onnx.finetuning.create_torch.quant_norm_ops)": [[17, "quark.onnx.finetuning.create_torch.quant_norm_ops.QLayerNorm", false]], "qmatmul (class in quark.onnx.finetuning.create_torch.quant_matmul_ops)": [[16, "quark.onnx.finetuning.create_torch.quant_matmul_ops.QMatMul", false]], "qparamslinear (class in quark.torch.export.nn.modules.qparamslinear)": [[177, "quark.torch.export.nn.modules.qparamslinear.QParamsLinear", false]], "qparamsoperator (class in quark.torch.export.nn.modules.qparamslinear)": [[177, "quark.torch.export.nn.modules.qparamslinear.QparamsOperator", false]], "qschemetype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.QSchemeType", false]], "quantconv2d (class in quark.torch.quantization.nn.modules.quantize_conv)": [[234, "quark.torch.quantization.nn.modules.quantize_conv.QuantConv2d", false]], "quantconvtranspose2d (class in quark.torch.quantization.nn.modules.quantize_conv)": [[234, "quark.torch.quantization.nn.modules.quantize_conv.QuantConvTranspose2d", false]], "quante4m3function (class in quark.torch.kernel)": [[192, "quark.torch.kernel.QuantE4M3Function", false]], "quante5m2function (class in quark.torch.kernel)": [[192, "quark.torch.kernel.QuantE5M2Function", false]], "quantembedding (class in quark.torch.quantization.nn.modules.quantize_embed)": [[236, "quark.torch.quantization.nn.modules.quantize_embed.QuantEmbedding", false]], "quantembeddingbag (class in quark.torch.quantization.nn.modules.quantize_embed)": [[236, "quark.torch.quantization.nn.modules.quantize_embed.QuantEmbeddingBag", false]], "quantizationconfig (class in quark.onnx.quantization.config.config)": [[57, "quark.onnx.quantization.config.config.QuantizationConfig", false]], "quantizationconfig (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantizationConfig", false]], "quantizationconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuantizationConfig", false]], "quantizationmode (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.QuantizationMode", false]], "quantizationmodule (class in quark.onnx.finetuning.create_torch.quant_base_ops)": [[13, "quark.onnx.finetuning.create_torch.quant_base_ops.QuantizationModule", false]], "quantizationspec (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantizationSpec", false]], "quantizationspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuantizationSpec", false]], "quantize_bias_static() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_bias_static", false]], "quantize_bias_tensor() (quark.onnx.qdq_quantizer.qdqnputransformerquantizer method)": [[54, "quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer.quantize_bias_tensor", false]], "quantize_bias_tensor() (quark.onnx.qdq_quantizer.qdqquantizer method)": [[54, "quark.onnx.qdq_quantizer.QDQQuantizer.quantize_bias_tensor", false]], "quantize_data_pof2s() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.quantize_data_pof2s", false]], "quantize_dynamic() (in module quark.onnx.quantize)": [[61, "quark.onnx.quantize.quantize_dynamic", false]], "quantize_initializer() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_initializer", false]], "quantize_model() (quark.onnx.quantization.api.modelquantizer method)": [[56, "quark.onnx.quantization.api.ModelQuantizer.quantize_model", false]], "quantize_model() (quark.torch.extensions.brevitas.api.modelquantizer method)": [[182, "quark.torch.extensions.brevitas.api.ModelQuantizer.quantize_model", false]], "quantize_model() (quark.torch.quantization.api.modelquantizer method)": [[198, "quark.torch.quantization.api.ModelQuantizer.quantize_model", false]], "quantize_weight() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_weight", false]], "quantize_weight_per_channel() (quark.onnx.onnx_quantizer.vitisonnxquantizer method)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer.quantize_weight_per_channel", false]], "quantizedconvbatchnorm2d (class in quark.torch.quantization.nn.modules.quantize_conv_bn_fused)": [[235, "quark.torch.quantization.nn.modules.quantize_conv_bn_fused.QuantizedConvBatchNorm2d", false]], "quantizewrapper (class in quark.onnx.finetuning.create_torch.quant_base_ops)": [[13, "quark.onnx.finetuning.create_torch.quant_base_ops.QuantizeWrapper", false]], "quantlinear (class in quark.torch.quantization.nn.modules.quantize_linear)": [[237, "quark.torch.quantization.nn.modules.quantize_linear.QuantLinear", false]], "quantmixin (class in quark.torch.quantization.nn.modules.mixin)": [[233, "quark.torch.quantization.nn.modules.mixin.QuantMixin", false]], "quanttype (class in quark.torch.extensions.brevitas.config)": [[183, "quark.torch.extensions.brevitas.config.QuantType", false]], "quark": [[0, "module-quark", false]], "quark.onnx": [[35, "module-quark.onnx", false]], "quark.onnx.auto_search": [[1, "module-quark.onnx.auto_search", false]], "quark.onnx.bias_correction": [[2, "module-quark.onnx.bias_correction", false]], "quark.onnx.calibrate": [[3, "module-quark.onnx.calibrate", false]], "quark.onnx.cpu_quantizer": [[4, "module-quark.onnx.cpu_quantizer", false]], "quark.onnx.equalization": [[5, "module-quark.onnx.equalization", false]], "quark.onnx.finetuning": [[19, "module-quark.onnx.finetuning", false]], "quark.onnx.finetuning.create_torch": [[12, "module-quark.onnx.finetuning.create_torch", false]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, "module-quark.onnx.finetuning.create_torch.base_fn_quantizers", false]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, "module-quark.onnx.finetuning.create_torch.base_qdq_quantizers", false]], "quark.onnx.finetuning.create_torch.create_model": [[8, "module-quark.onnx.finetuning.create_torch.create_model", false]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, "module-quark.onnx.finetuning.create_torch.create_model_ops", false]], "quark.onnx.finetuning.create_torch.create_model_test": [[10, "module-quark.onnx.finetuning.create_torch.create_model_test", false]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, "module-quark.onnx.finetuning.create_torch.create_model_utils", false]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, "module-quark.onnx.finetuning.create_torch.quant_base_ops", false]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, "module-quark.onnx.finetuning.create_torch.quant_conv_ops", false]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, "module-quark.onnx.finetuning.create_torch.quant_gemm_ops", false]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, "module-quark.onnx.finetuning.create_torch.quant_matmul_ops", false]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, "module-quark.onnx.finetuning.create_torch.quant_norm_ops", false]], "quark.onnx.finetuning.fast_finetune": [[18, "module-quark.onnx.finetuning.fast_finetune", false]], "quark.onnx.finetuning.onnx_evaluate": [[20, "module-quark.onnx.finetuning.onnx_evaluate", false]], "quark.onnx.finetuning.onnx_subgraph": [[21, "module-quark.onnx.finetuning.onnx_subgraph", false]], "quark.onnx.finetuning.torch_utils": [[22, "module-quark.onnx.finetuning.torch_utils", false]], "quark.onnx.finetuning.torch_utils_test": [[23, "module-quark.onnx.finetuning.torch_utils_test", false]], "quark.onnx.finetuning.train_torch": [[24, "module-quark.onnx.finetuning.train_torch", false]], "quark.onnx.finetuning.train_torch.train_model": [[25, "module-quark.onnx.finetuning.train_torch.train_model", false]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, "module-quark.onnx.finetuning.train_torch.train_model_loss", false]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, "module-quark.onnx.finetuning.train_torch.train_model_param", false]], "quark.onnx.gptq": [[29, "module-quark.onnx.gptq", false]], "quark.onnx.gptq.gptq": [[28, "module-quark.onnx.gptq.gptq", false]], "quark.onnx.graph_transformations": [[30, "module-quark.onnx.graph_transformations", false]], "quark.onnx.graph_transformations.model_transformer": [[31, "module-quark.onnx.graph_transformations.model_transformer", false]], "quark.onnx.graph_transformations.model_transformer_test": [[32, "module-quark.onnx.graph_transformations.model_transformer_test", false]], "quark.onnx.graph_transformations.transforms": [[33, "module-quark.onnx.graph_transformations.transforms", false]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, "module-quark.onnx.graph_transformations.transforms_pipeline", false]], "quark.onnx.mprecision": [[37, "module-quark.onnx.mprecision", false]], "quark.onnx.mprecision.auto_mixprecision": [[36, "module-quark.onnx.mprecision.auto_mixprecision", false]], "quark.onnx.mprecision.mixed_bfp": [[38, "module-quark.onnx.mprecision.mixed_bfp", false]], "quark.onnx.onnx_quantizer": [[39, "module-quark.onnx.onnx_quantizer", false]], "quark.onnx.operators": [[42, "module-quark.onnx.operators", false]], "quark.onnx.operators.custom_ops": [[41, "module-quark.onnx.operators.custom_ops", false]], "quark.onnx.operators.custom_ops.build_vai_custom_op": [[40, "module-quark.onnx.operators.custom_ops.build_vai_custom_op", false]], "quark.onnx.operators.vai_ops": [[45, "module-quark.onnx.operators.vai_ops", false]], "quark.onnx.operators.vai_ops.concat": [[43, "module-quark.onnx.operators.vai_ops.concat", false]], "quark.onnx.operators.vai_ops.hardsigmoid": [[44, "module-quark.onnx.operators.vai_ops.hardsigmoid", false]], "quark.onnx.operators.vai_ops.layernorm": [[46, "module-quark.onnx.operators.vai_ops.layernorm", false]], "quark.onnx.operators.vai_ops.prelu": [[47, "module-quark.onnx.operators.vai_ops.prelu", false]], "quark.onnx.operators.vai_ops.qdq_ops": [[48, "module-quark.onnx.operators.vai_ops.qdq_ops", false]], "quark.onnx.operators.vai_ops.softmax": [[49, "module-quark.onnx.operators.vai_ops.softmax", false]], "quark.onnx.optimizations": [[52, "module-quark.onnx.optimizations", false]], "quark.onnx.optimizations.convert_transforms": [[50, "module-quark.onnx.optimizations.convert_transforms", false]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, "module-quark.onnx.optimizations.convert_transforms_pipeline", false]], "quark.onnx.optimize": [[53, "module-quark.onnx.optimize", false]], "quark.onnx.qdq_quantizer": [[54, "module-quark.onnx.qdq_quantizer", false]], "quark.onnx.quant_utils": [[55, "module-quark.onnx.quant_utils", false]], "quark.onnx.quantization": [[60, "module-quark.onnx.quantization", false]], "quark.onnx.quantization.api": [[56, "module-quark.onnx.quantization.api", false]], "quark.onnx.quantization.config": [[59, "module-quark.onnx.quantization.config", false]], "quark.onnx.quantization.config.config": [[57, "module-quark.onnx.quantization.config.config", false]], "quark.onnx.quantization.config.custom_config": [[58, "module-quark.onnx.quantization.config.custom_config", false]], "quark.onnx.quantize": [[61, "module-quark.onnx.quantize", false]], "quark.onnx.quantizers": [[65, "module-quark.onnx.quantizers", false]], "quark.onnx.quantizers.bfp_quantizer": [[62, "module-quark.onnx.quantizers.bfp_quantizer", false]], "quark.onnx.quantizers.cpu_quantizer": [[63, "module-quark.onnx.quantizers.cpu_quantizer", false]], "quark.onnx.quantizers.extended_quantizer": [[64, "module-quark.onnx.quantizers.extended_quantizer", false]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, "module-quark.onnx.quantizers.matmul_nbits_quantizer", false]], "quark.onnx.quantizers.npu_cnn_quantizer": [[67, "module-quark.onnx.quantizers.npu_cnn_quantizer", false]], "quark.onnx.quantizers.npu_transformer_quantizer": [[68, "module-quark.onnx.quantizers.npu_transformer_quantizer", false]], "quark.onnx.quantizers.onnx_quantizer": [[69, "module-quark.onnx.quantizers.onnx_quantizer", false]], "quark.onnx.quantizers.qdq_quantizer": [[70, "module-quark.onnx.quantizers.qdq_quantizer", false]], "quark.onnx.quarot": [[71, "module-quark.onnx.quarot", false]], "quark.onnx.refine": [[72, "module-quark.onnx.refine", false]], "quark.onnx.registry": [[73, "module-quark.onnx.registry", false]], "quark.onnx.simulate_dpu": [[74, "module-quark.onnx.simulate_dpu", false]], "quark.onnx.simulate_dpu_softmax": [[75, "module-quark.onnx.simulate_dpu_softmax", false]], "quark.onnx.smooth_quant": [[76, "module-quark.onnx.smooth_quant", false]], "quark.onnx.tools": [[100, "module-quark.onnx.tools", false]], "quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu": [[77, "module-quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu", false]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, "module-quark.onnx.tools.convert_customqdq_to_qdq", false]], "quark.onnx.tools.convert_dynamic_to_fixed": [[79, "module-quark.onnx.tools.convert_dynamic_to_fixed", false]], "quark.onnx.tools.convert_fp16_to_bf16": [[80, "module-quark.onnx.tools.convert_fp16_to_bf16", false]], "quark.onnx.tools.convert_fp16_to_bfp16": [[81, "module-quark.onnx.tools.convert_fp16_to_bfp16", false]], "quark.onnx.tools.convert_fp16_to_fp32": [[82, "module-quark.onnx.tools.convert_fp16_to_fp32", false]], "quark.onnx.tools.convert_fp32_to_bf16": [[83, "module-quark.onnx.tools.convert_fp32_to_bf16", false]], "quark.onnx.tools.convert_fp32_to_bfp16": [[84, "module-quark.onnx.tools.convert_fp32_to_bfp16", false]], "quark.onnx.tools.convert_fp32_to_fp16": [[85, "module-quark.onnx.tools.convert_fp32_to_fp16", false]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, "module-quark.onnx.tools.convert_lstm_to_customlstm", false]], "quark.onnx.tools.convert_nchw_to_nhwc": [[87, "module-quark.onnx.tools.convert_nchw_to_nhwc", false]], "quark.onnx.tools.convert_onnx_to_onnxtxt": [[88, "module-quark.onnx.tools.convert_onnx_to_onnxtxt", false]], "quark.onnx.tools.convert_onnxtxt_to_onnx": [[89, "module-quark.onnx.tools.convert_onnxtxt_to_onnx", false]], "quark.onnx.tools.convert_opset_version": [[90, "module-quark.onnx.tools.convert_opset_version", false]], "quark.onnx.tools.convert_qdq_to_qop": [[91, "module-quark.onnx.tools.convert_qdq_to_qop", false]], "quark.onnx.tools.convert_quant_to_float": [[92, "module-quark.onnx.tools.convert_quant_to_float", false]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, "module-quark.onnx.tools.convert_resize_fs_to_pof2s", false]], "quark.onnx.tools.convert_s8s8_to_u8s8": [[94, "module-quark.onnx.tools.convert_s8s8_to_u8s8", false]], "quark.onnx.tools.convert_shared_initializer_to_unique": [[95, "module-quark.onnx.tools.convert_shared_initializer_to_unique", false]], "quark.onnx.tools.convert_u16s8_to_s16s8": [[96, "module-quark.onnx.tools.convert_u16s8_to_s16s8", false]], "quark.onnx.tools.convert_u16u8_to_u8u8": [[97, "module-quark.onnx.tools.convert_u16u8_to_u8u8", false]], "quark.onnx.tools.evaluate": [[98, "module-quark.onnx.tools.evaluate", false]], "quark.onnx.tools.float16": [[99, "module-quark.onnx.tools.float16", false]], "quark.onnx.tools.insert_clip_bfloat16_qdq": [[101, "module-quark.onnx.tools.insert_clip_bfloat16_qdq", false]], "quark.onnx.tools.print_a16w8_a8w8_nodes": [[102, "module-quark.onnx.tools.print_a16w8_a8w8_nodes", false]], "quark.onnx.tools.random_quantize": [[103, "module-quark.onnx.tools.random_quantize", false]], "quark.onnx.tools.remove_bf16_cast": [[104, "module-quark.onnx.tools.remove_bf16_cast", false]], "quark.onnx.tools.remove_initializer_from_input": [[105, "module-quark.onnx.tools.remove_initializer_from_input", false]], "quark.onnx.tools.remove_qdq": [[106, "module-quark.onnx.tools.remove_qdq", false]], "quark.onnx.tools.remove_qdq_between_ops": [[107, "module-quark.onnx.tools.remove_qdq_between_ops", false]], "quark.onnx.tools.remove_qdq_mul_add": [[108, "module-quark.onnx.tools.remove_qdq_mul_add", false]], "quark.onnx.tools.replace_bfloat16_qdq_cast": [[109, "module-quark.onnx.tools.replace_bfloat16_qdq_cast", false]], "quark.onnx.tools.replace_inf_weights": [[110, "module-quark.onnx.tools.replace_inf_weights", false]], "quark.onnx.tools.save_tensor_hist": [[111, "module-quark.onnx.tools.save_tensor_hist", false]], "quark.onnx.tools.save_weights_hist": [[112, "module-quark.onnx.tools.save_weights_hist", false]], "quark.onnx.utils": [[113, "module-quark.onnx.utils", false]], "quark.onnx.utils.model_utils": [[114, "module-quark.onnx.utils.model_utils", false]], "quark.shares": [[115, "module-quark.shares", false]], "quark.shares.utils": [[117, "module-quark.shares.utils", false]], "quark.shares.utils.import_utils": [[116, "module-quark.shares.utils.import_utils", false]], "quark.shares.utils.log": [[118, "module-quark.shares.utils.log", false]], "quark.shares.utils.testing_utils": [[119, "module-quark.shares.utils.testing_utils", false]], "quark.torch": [[188, "module-quark.torch", false]], "quark.torch.algorithm": [[133, "module-quark.torch.algorithm", false]], "quark.torch.algorithm.api": [[120, "module-quark.torch.algorithm.api", false]], "quark.torch.algorithm.awq": [[123, "module-quark.torch.algorithm.awq", false]], "quark.torch.algorithm.awq.auto_smooth": [[121, "module-quark.torch.algorithm.awq.auto_smooth", false]], "quark.torch.algorithm.awq.awq": [[122, "module-quark.torch.algorithm.awq.awq", false]], "quark.torch.algorithm.awq.modules": [[125, "module-quark.torch.algorithm.awq.modules", false]], "quark.torch.algorithm.awq.modules.act": [[124, "module-quark.torch.algorithm.awq.modules.act", false]], "quark.torch.algorithm.awq.scale": [[126, "module-quark.torch.algorithm.awq.scale", false]], "quark.torch.algorithm.awq.smooth": [[127, "module-quark.torch.algorithm.awq.smooth", false]], "quark.torch.algorithm.blockwise_tuning": [[130, "module-quark.torch.algorithm.blockwise_tuning", false]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, "module-quark.torch.algorithm.blockwise_tuning.blockwise_tuning", false]], "quark.torch.algorithm.blockwise_tuning.blockwise_utils": [[129, "module-quark.torch.algorithm.blockwise_tuning.blockwise_utils", false]], "quark.torch.algorithm.gptq": [[132, "module-quark.torch.algorithm.gptq", false]], "quark.torch.algorithm.gptq.gptq": [[131, "module-quark.torch.algorithm.gptq.gptq", false]], "quark.torch.algorithm.osscar": [[134, "module-quark.torch.algorithm.osscar", false]], "quark.torch.algorithm.osscar.osscar": [[135, "module-quark.torch.algorithm.osscar.osscar", false]], "quark.torch.algorithm.processor": [[136, "module-quark.torch.algorithm.processor", false]], "quark.torch.algorithm.quarot": [[137, "module-quark.torch.algorithm.quarot", false]], "quark.torch.algorithm.quarot.monkeypatch": [[138, "module-quark.torch.algorithm.quarot.monkeypatch", false]], "quark.torch.algorithm.quarot.quarot": [[139, "module-quark.torch.algorithm.quarot.quarot", false]], "quark.torch.algorithm.quarot.utils": [[140, "module-quark.torch.algorithm.quarot.utils", false]], "quark.torch.algorithm.rotation": [[142, "module-quark.torch.algorithm.rotation", false]], "quark.torch.algorithm.rotation.hadamard": [[141, "module-quark.torch.algorithm.rotation.hadamard", false]], "quark.torch.algorithm.rotation.rotation": [[143, "module-quark.torch.algorithm.rotation.rotation", false]], "quark.torch.algorithm.rotation.rotation_utils": [[144, "module-quark.torch.algorithm.rotation.rotation_utils", false]], "quark.torch.algorithm.utils": [[146, "module-quark.torch.algorithm.utils", false]], "quark.torch.algorithm.utils.auto_config": [[145, "module-quark.torch.algorithm.utils.auto_config", false]], "quark.torch.algorithm.utils.module": [[147, "module-quark.torch.algorithm.utils.module", false]], "quark.torch.algorithm.utils.prepare": [[148, "module-quark.torch.algorithm.utils.prepare", false]], "quark.torch.algorithm.utils.utils": [[149, "module-quark.torch.algorithm.utils.utils", false]], "quark.torch.export": [[160, "module-quark.torch.export", false]], "quark.torch.export.api": [[150, "module-quark.torch.export.api", false]], "quark.torch.export.config": [[152, "module-quark.torch.export.config", false]], "quark.torch.export.config.config": [[151, "module-quark.torch.export.config.config", false]], "quark.torch.export.constants": [[153, "module-quark.torch.export.constants", false]], "quark.torch.export.gguf_export": [[157, "module-quark.torch.export.gguf_export", false]], "quark.torch.export.gguf_export.api": [[154, "module-quark.torch.export.gguf_export.api", false]], "quark.torch.export.gguf_export.gguf_model_converter": [[155, "module-quark.torch.export.gguf_export.gguf_model_converter", false]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, "module-quark.torch.export.gguf_export.gguf_model_writer", false]], "quark.torch.export.gguf_export.tensor_convert": [[158, "module-quark.torch.export.gguf_export.tensor_convert", false]], "quark.torch.export.gguf_export.utils": [[159, "module-quark.torch.export.gguf_export.utils", false]], "quark.torch.export.json_export": [[167, "module-quark.torch.export.json_export", false]], "quark.torch.export.json_export.builder": [[161, "module-quark.torch.export.json_export.builder", false]], "quark.torch.export.json_export.builder.llm_info": [[162, "module-quark.torch.export.json_export.builder.llm_info", false]], "quark.torch.export.json_export.builder.llm_info_builder": [[163, "module-quark.torch.export.json_export.builder.llm_info_builder", false]], "quark.torch.export.json_export.builder.native_model_info_builder": [[164, "module-quark.torch.export.json_export.builder.native_model_info_builder", false]], "quark.torch.export.json_export.converter": [[165, "module-quark.torch.export.json_export.converter", false]], "quark.torch.export.json_export.converter.llm_info_converter": [[166, "module-quark.torch.export.json_export.converter.llm_info_converter", false]], "quark.torch.export.json_export.utils": [[168, "module-quark.torch.export.json_export.utils", false]], "quark.torch.export.json_export.utils.utils": [[169, "module-quark.torch.export.json_export.utils.utils", false]], "quark.torch.export.main_export": [[170, "module-quark.torch.export.main_export", false]], "quark.torch.export.main_export.model_post_process": [[171, "module-quark.torch.export.main_export.model_post_process", false]], "quark.torch.export.main_export.quant_config_parser": [[172, "module-quark.torch.export.main_export.quant_config_parser", false]], "quark.torch.export.main_import": [[173, "module-quark.torch.export.main_import", false]], "quark.torch.export.main_import.pretrained_config": [[174, "module-quark.torch.export.main_import.pretrained_config", false]], "quark.torch.export.nn": [[175, "module-quark.torch.export.nn", false]], "quark.torch.export.nn.modules": [[176, "module-quark.torch.export.nn.modules", false]], "quark.torch.export.nn.modules.qparamslinear": [[177, "module-quark.torch.export.nn.modules.qparamslinear", false]], "quark.torch.export.nn.modules.realquantizer": [[178, "module-quark.torch.export.nn.modules.realquantizer", false]], "quark.torch.export.onnx": [[179, "module-quark.torch.export.onnx", false]], "quark.torch.export.utils": [[180, "module-quark.torch.export.utils", false]], "quark.torch.extensions": [[187, "module-quark.torch.extensions", false]], "quark.torch.extensions.brevitas": [[184, "module-quark.torch.extensions.brevitas", false]], "quark.torch.extensions.brevitas.algos": [[181, "module-quark.torch.extensions.brevitas.algos", false]], "quark.torch.extensions.brevitas.api": [[182, "module-quark.torch.extensions.brevitas.api", false]], "quark.torch.extensions.brevitas.config": [[183, "module-quark.torch.extensions.brevitas.config", false]], "quark.torch.extensions.brevitas.mapping": [[185, "module-quark.torch.extensions.brevitas.mapping", false]], "quark.torch.extensions.brevitas.verification": [[186, "module-quark.torch.extensions.brevitas.verification", false]], "quark.torch.kernel": [[192, "module-quark.torch.kernel", false]], "quark.torch.kernel.hw_emulation": [[191, "module-quark.torch.kernel.hw_emulation", false]], "quark.torch.kernel.hw_emulation.extensions": [[189, "module-quark.torch.kernel.hw_emulation.extensions", false]], "quark.torch.kernel.hw_emulation.hw_emulation_interface": [[190, "module-quark.torch.kernel.hw_emulation.hw_emulation_interface", false]], "quark.torch.pruning": [[195, "module-quark.torch.pruning", false]], "quark.torch.pruning.api": [[193, "module-quark.torch.pruning.api", false]], "quark.torch.pruning.config": [[194, "module-quark.torch.pruning.config", false]], "quark.torch.pruning.model_transformation": [[196, "module-quark.torch.pruning.model_transformation", false]], "quark.torch.pruning.utils": [[197, "module-quark.torch.pruning.utils", false]], "quark.torch.quantization": [[229, "module-quark.torch.quantization", false]], "quark.torch.quantization.api": [[198, "module-quark.torch.quantization.api", false]], "quark.torch.quantization.config": [[201, "module-quark.torch.quantization.config", false]], "quark.torch.quantization.config.config": [[199, "module-quark.torch.quantization.config.config", false]], "quark.torch.quantization.config.config_verification": [[200, "module-quark.torch.quantization.config.config_verification", false]], "quark.torch.quantization.config.type": [[202, "module-quark.torch.quantization.config.type", false]], "quark.torch.quantization.config.utils": [[203, "module-quark.torch.quantization.config.utils", false]], "quark.torch.quantization.constants": [[204, "module-quark.torch.quantization.constants", false]], "quark.torch.quantization.debug": [[205, "module-quark.torch.quantization.debug", false]], "quark.torch.quantization.graph": [[208, "module-quark.torch.quantization.graph", false]], "quark.torch.quantization.graph.fx": [[207, "module-quark.torch.quantization.graph.fx", false]], "quark.torch.quantization.graph.fx.base": [[206, "module-quark.torch.quantization.graph.fx.base", false]], "quark.torch.quantization.graph.optimization": [[210, "module-quark.torch.quantization.graph.optimization", false]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, "module-quark.torch.quantization.graph.optimization.activate_dropout", false]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, "module-quark.torch.quantization.graph.optimization.model_optimization", false]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, "module-quark.torch.quantization.graph.optimization.modify_reshape_param", false]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, "module-quark.torch.quantization.graph.optimization.opt_pass_manager", false]], "quark.torch.quantization.graph.optimization.post_quant": [[214, "module-quark.torch.quantization.graph.optimization.post_quant", false]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, "module-quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant": [[216, "module-quark.torch.quantization.graph.optimization.pre_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, "module-quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d", false]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, "module-quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear", false]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, "module-quark.torch.quantization.graph.optimization.remove_dropout_node", false]], "quark.torch.quantization.graph.optimization.utils": [[223, "module-quark.torch.quantization.graph.optimization.utils", false]], "quark.torch.quantization.graph.processor": [[224, "module-quark.torch.quantization.graph.processor", false]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, "module-quark.torch.quantization.graph.processor.insert_quantizer", false]], "quark.torch.quantization.graph.processor.processor": [[226, "module-quark.torch.quantization.graph.processor.processor", false]], "quark.torch.quantization.graph.processor.processor_utils": [[227, "module-quark.torch.quantization.graph.processor.processor_utils", false]], "quark.torch.quantization.graph.torch_utils": [[228, "module-quark.torch.quantization.graph.torch_utils", false]], "quark.torch.quantization.model_transformation": [[230, "module-quark.torch.quantization.model_transformation", false]], "quark.torch.quantization.nn": [[231, "module-quark.torch.quantization.nn", false]], "quark.torch.quantization.nn.modules": [[232, "module-quark.torch.quantization.nn.modules", false]], "quark.torch.quantization.nn.modules.mixin": [[233, "module-quark.torch.quantization.nn.modules.mixin", false]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, "module-quark.torch.quantization.nn.modules.quantize_conv", false]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, "module-quark.torch.quantization.nn.modules.quantize_conv_bn_fused", false]], "quark.torch.quantization.nn.modules.quantize_embed": [[236, "module-quark.torch.quantization.nn.modules.quantize_embed", false]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, "module-quark.torch.quantization.nn.modules.quantize_linear", false]], "quark.torch.quantization.nn.utils": [[238, "module-quark.torch.quantization.nn.utils", false]], "quark.torch.quantization.observer": [[239, "module-quark.torch.quantization.observer", false]], "quark.torch.quantization.observer.lsq_observer": [[240, "module-quark.torch.quantization.observer.lsq_observer", false]], "quark.torch.quantization.observer.observer": [[241, "module-quark.torch.quantization.observer.observer", false]], "quark.torch.quantization.observer.tqt_observer": [[242, "module-quark.torch.quantization.observer.tqt_observer", false]], "quark.torch.quantization.tensor_quantize": [[243, "module-quark.torch.quantization.tensor_quantize", false]], "quark.torch.quantization.utils": [[244, "module-quark.torch.quantization.utils", false]], "quark.torch.utils": [[245, "module-quark.torch.utils", false]], "quark.torch.utils.pack": [[246, "module-quark.torch.utils.pack", false]], "quark.version": [[247, "module-quark.version", false]], "quarot (class in quark.onnx.quarot)": [[71, "quark.onnx.quarot.QuaRot", false]], "quarotconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.QuaRotConfig", false]], "quarotprocessor (class in quark.torch.algorithm.quarot.quarot)": [[139, "quark.torch.algorithm.quarot.quarot.QuaRotProcessor", false]], "r4wrapper (class in quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.R4Wrapper", false]], "random_hadamard_matrix() (in module quark.torch.algorithm.rotation.hadamard)": [[141, "quark.torch.algorithm.rotation.hadamard.random_hadamard_matrix", false]], "randomdatareader (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.RandomDataReader", false]], "realquantizerbase (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.RealQuantizerBase", false]], "remove_initializers() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.remove_initializers", false]], "remove_nodes() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.remove_nodes", false]], "remove_qdq() (in module quark.onnx.tools.remove_qdq)": [[106, "quark.onnx.tools.remove_qdq.remove_qdq", false]], "remove_qdq_between_ops() (in module quark.onnx.tools.remove_qdq_between_ops)": [[107, "quark.onnx.tools.remove_qdq_between_ops.remove_qdq_between_ops", false]], "remove_qdq_mul_add() (in module quark.onnx.tools.remove_qdq_mul_add)": [[108, "quark.onnx.tools.remove_qdq_mul_add.remove_qdq_mul_add", false]], "removedropoutnode (class in quark.torch.quantization.graph.optimization.remove_dropout_node)": [[222, "quark.torch.quantization.graph.optimization.remove_dropout_node.RemoveDropoutNode", false]], "removeqdqtransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform", false]], "removeqdqtransformspipeline (class in quark.onnx.optimizations.convert_transforms_pipeline)": [[51, "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline", false]], "replace_conv2d_qtconv2d() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d)": [[218, "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d.replace_conv2d_qtconv2d", false]], "replace_conv2dbn_quantizedconv_module() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model)": [[219, "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model.replace_conv2dbn_quantizedconv_module", false]], "replace_convtranspose2d_qtconvtranspose2d() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d)": [[220, "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d.replace_convtranspose2d_qtconvtranspose2d", false]], "replace_inf_in_onnx_weights() (in module quark.onnx.tools.replace_inf_weights)": [[110, "quark.onnx.tools.replace_inf_weights.replace_inf_in_onnx_weights", false]], "replace_linear_qtlinear() (in module quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear)": [[221, "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear.replace_linear_qtlinear", false]], "replacement() (quark.onnx.graph_transformations.model_transformer_test.modeltransformertest.removerelu method)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.RemoveRelu.replacement", false]], "replacement() (quark.onnx.graph_transformations.model_transformer_test.modeltransformertest.replacewholemodel method)": [[32, "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.ReplaceWholeModel.replacement", false]], "replacement() (quark.onnx.graph_transformations.transforms.transform method)": [[33, "quark.onnx.graph_transformations.transforms.Transform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.addqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.convqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.matmulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.mulqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.removeqdqtransform method)": [[50, "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform.replacement", false]], "replacement() (quark.onnx.optimizations.convert_transforms.sigmoidqdqtoqoptransform method)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform.replacement", false]], "require_accelerate() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.require_accelerate", false]], "require_torch_gpu() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.require_torch_gpu", false]], "requires() (quark.torch.quantization.graph.optimization.opt_pass_manager.optpassbase method)": [[213, "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase.requires", false]], "requires() (quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.convertclip2reluqopass method)": [[215, "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertbn2d2convqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.convertreducemean2gapqopass method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass.requires", false]], "requires() (quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.splitquantmodulecalledoveronce method)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce.requires", false]], "reset_iter() (quark.onnx.quant_utils.cacheddatareader method)": [[55, "quark.onnx.quant_utils.CachedDataReader.reset_iter", false]], "reset_min_max_vals() (quark.torch.quantization.observer.observer.uniformscalingobserver method)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver.reset_min_max_vals", false]], "reset_model() (quark.torch.export.api.modelexporter method)": [[150, "quark.torch.export.api.ModelExporter.reset_model", false]], "retry_flaky_test() (in module quark.shares.utils.testing_utils)": [[119, "quark.shares.utils.testing_utils.retry_flaky_test", false]], "rmsnorm (class in quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.RMSNorm", false]], "rotate_in_channels() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.rotate_in_channels", false]], "rotate_in_channels() (quark.onnx.quarot.quarot method)": [[71, "quark.onnx.quarot.QuaRot.rotate_in_channels", false]], "rotate_in_channels2() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.rotate_in_channels2", false]], "rotate_out_channels() (in module quark.torch.algorithm.rotation.rotation_utils)": [[144, "quark.torch.algorithm.rotation.rotation_utils.rotate_out_channels", false]], "rotate_out_channels() (quark.onnx.quarot.quarot method)": [[71, "quark.onnx.quarot.QuaRot.rotate_out_channels", false]], "rotate_out_channels2() (in module quark.torch.algorithm.quarot.utils)": [[140, "quark.torch.algorithm.quarot.utils.rotate_out_channels2", false]], "rotationconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.RotationConfig", false]], "rotationprocessor (class in quark.torch.algorithm.rotation.rotation)": [[143, "quark.torch.algorithm.rotation.rotation.RotationProcessor", false]], "round_impl() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.adaroundintquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer.round_impl", false]], "round_impl() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer.round_impl", false]], "roundhalftoeven (class in quark.onnx.finetuning.create_torch.base_qdq_quantizers)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.RoundHalfToEven", false]], "roundtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.RoundType", false]], "run() (quark.onnx.auto_search.assembleidxs method)": [[1, "quark.onnx.auto_search.AssembleIdxs.run", false]], "run() (quark.onnx.finetuning.train_torch.train_model.modeloptimizer class method)": [[25, "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer.run", false]], "run_onnx_model() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.run_onnx_model", false]], "save_distribution_histogram() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.save_distribution_histogram", false]], "save_model() (in module quark.onnx.utils.model_utils)": [[114, "quark.onnx.utils.model_utils.save_model", false]], "save_params() (in module quark.torch.export.api)": [[150, "quark.torch.export.api.save_params", false]], "save_torch_model() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.save_torch_model", false]], "scale2pos() (in module quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.scale2pos", false]], "scale2pos() (in module quark.onnx.tools.convert_resize_fs_to_pof2s)": [[93, "quark.onnx.tools.convert_resize_fs_to_pof2s.scale2pos", false]], "scaledactivation (class in quark.torch.algorithm.awq.modules.act)": [[124, "quark.torch.algorithm.awq.modules.act.ScaledActivation", false]], "scaledfakequantize (class in quark.torch.quantization.tensor_quantize)": [[243, "quark.torch.quantization.tensor_quantize.ScaledFakeQuantize", false]], "scaledfakequantizefunction (class in quark.torch.kernel)": [[192, "quark.torch.kernel.ScaledFakeQuantizeFunction", false]], "scaledrealquantizefunction (class in quark.torch.kernel)": [[192, "quark.torch.kernel.ScaledRealQuantizeFunction", false]], "scaledrealquantizer (class in quark.torch.export.nn.modules.realquantizer)": [[178, "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer", false]], "scaletype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.ScaleType", false]], "search_forward() (quark.onnx.auto_search.assembleidxs method)": [[1, "quark.onnx.auto_search.AssembleIdxs.search_forward", false]], "searchspace (class in quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.SearchSpace", false]], "sentencepiecetokentypes (class in quark.torch.export.gguf_export.gguf_model_writer)": [[156, "quark.torch.export.gguf_export.gguf_model_writer.SentencePieceTokenTypes", false]], "set_algo_config() (quark.torch.quantization.config.config.config method)": [[199, "quark.torch.quantization.config.config.Config.set_algo_config", false]], "set_bias() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.set_bias", false]], "set_modules_original_bias() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.set_modules_original_bias", false]], "set_modules_original_weight() (in module quark.onnx.finetuning.create_torch.create_model_ops)": [[9, "quark.onnx.finetuning.create_torch.create_model_ops.set_modules_original_weight", false]], "set_op_by_name() (in module quark.torch.quantization.utils)": [[244, "quark.torch.quantization.utils.set_op_by_name", false]], "set_weight() (quark.onnx.finetuning.create_torch.create_model.torchmodel method)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel.set_weight", false]], "setup_config_per_layer() (in module quark.torch.quantization.model_transformation)": [[230, "quark.torch.quantization.model_transformation.setup_config_per_layer", false]], "setup_seed() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.setup_seed", false]], "sigmoidqdqtoqoptransform (class in quark.onnx.optimizations.convert_transforms)": [[50, "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform", false]], "simulate_transforms() (in module quark.onnx.simulate_dpu)": [[74, "quark.onnx.simulate_dpu.simulate_transforms", false]], "smoothquant (class in quark.onnx.smooth_quant)": [[76, "quark.onnx.smooth_quant.SmoothQuant", false]], "smoothquantconfig (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.SmoothQuantConfig", false]], "smoothquantprocessor (class in quark.torch.algorithm.awq.smooth)": [[127, "quark.torch.algorithm.awq.smooth.SmoothQuantProcessor", false]], "split_large_kernel_pool() (quark.onnx.optimize.optimize method)": [[53, "quark.onnx.optimize.Optimize.split_large_kernel_pool", false]], "split_model_info() (in module quark.torch.export.json_export.utils.utils)": [[169, "quark.torch.export.json_export.utils.utils.split_model_info", false]], "split_params_for_dbrxexperts() (in module quark.torch.export.utils)": [[180, "quark.torch.export.utils.split_params_for_DbrxExperts", false]], "splitquantmodulecalledoveronce (class in quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant)": [[217, "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce", false]], "ssim_metric() (in module quark.onnx.auto_search)": [[1, "quark.onnx.auto_search.ssim_metric", false]], "state_dict() (quark.torch.export.nn.modules.qparamslinear.qparamslinear method)": [[177, "quark.torch.export.nn.modules.qparamslinear.QParamsLinear.state_dict", false]], "subgraph (class in quark.onnx.finetuning.onnx_subgraph)": [[21, "quark.onnx.finetuning.onnx_subgraph.Subgraph", false]], "summarize_activation() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.summarize_activation", false]], "summarize_weight() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.summarize_weight", false]], "t_exponent() (in module quark.torch.quantization.utils)": [[244, "quark.torch.quantization.utils.t_exponent", false]], "tensor_sync() (quark.onnx.finetuning.create_torch.base_qdq_quantizers.intquantizer method)": [[7, "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer.tensor_sync", false]], "tensordata (class in quark.torch.algorithm.utils.utils)": [[149, "quark.torch.algorithm.utils.utils.TensorData", false]], "three_level_spaces() (quark.onnx.auto_search.searchspace method)": [[1, "quark.onnx.auto_search.SearchSpace.three_level_spaces", false]], "to_real_quantize_params() (quark.torch.export.nn.modules.realquantizer.nonscaledrealquantizer method)": [[178, "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer.to_real_quantize_params", false]], "to_real_quantize_params() (quark.torch.export.nn.modules.realquantizer.scaledrealquantizer method)": [[178, "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer.to_real_quantize_params", false]], "torchmodel (class in quark.onnx.finetuning.create_torch.create_model)": [[8, "quark.onnx.finetuning.create_torch.create_model.TorchModel", false]], "tqtobserver (class in quark.torch.quantization.observer.tqt_observer)": [[242, "quark.torch.quantization.observer.tqt_observer.TQTObserver", false]], "tqtquantize (class in quark.torch.kernel)": [[192, "quark.torch.kernel.TQTQuantize", false]], "tqtspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.TQTSpec", false]], "tqtthresholdinitmeth (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.TQTThresholdInitMeth", false]], "train_torch_module_api() (in module quark.onnx.finetuning.torch_utils)": [[22, "quark.onnx.finetuning.torch_utils.train_torch_module_api", false]], "trainloss (class in quark.onnx.finetuning.train_torch.train_model_loss)": [[26, "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss", false]], "trainparameters (class in quark.onnx.finetuning.train_torch.train_model_param)": [[27, "quark.onnx.finetuning.train_torch.train_model_param.TrainParameters", false]], "trans_opsfunc_2_quant_module() (in module quark.torch.quantization.graph.optimization.model_optimization)": [[211, "quark.torch.quantization.graph.optimization.model_optimization.trans_opsfunc_2_quant_module", false]], "transform (class in quark.onnx.graph_transformations.transforms)": [[33, "quark.onnx.graph_transformations.transforms.Transform", false]], "transform (class in quark.torch.quantization.graph.fx.base)": [[206, "quark.torch.quantization.graph.fx.base.Transform", false]], "transform() (quark.onnx.graph_transformations.model_transformer.modeltransformer method)": [[31, "quark.onnx.graph_transformations.model_transformer.ModelTransformer.transform", false]], "transform_for_annotation() (in module quark.torch.quantization.graph.processor.processor)": [[226, "quark.torch.quantization.graph.processor.processor.transform_for_annotation", false]], "transformspipeline (class in quark.onnx.graph_transformations.transforms_pipeline)": [[34, "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline", false]], "uint4perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerChannelSpec", false]], "uint4pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerGroupSpec", false]], "uint4pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint4PerTensorSpec", false]], "uint8perchannelspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerChannelSpec", false]], "uint8pergroupspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerGroupSpec", false]], "uint8pertensorspec (class in quark.torch.quantization.config.config)": [[199, "quark.torch.quantization.config.config.Uint8PerTensorSpec", false]], "uniformscalingobserver (class in quark.torch.quantization.observer.observer)": [[241, "quark.torch.quantization.observer.observer.UniformScalingObserver", false]], "update_buffer() (quark.torch.quantization.tensor_quantize.fakequantizebase method)": [[243, "quark.torch.quantization.tensor_quantize.FakeQuantizeBase.update_buffer", false]], "vitisbfpquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisBFPQuantizer", false]], "vitisextendedquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisExtendedQuantizer", false]], "vitisonnxquantizer (class in quark.onnx.onnx_quantizer)": [[39, "quark.onnx.onnx_quantizer.VitisONNXQuantizer", false]], "vitisqdqcpuquantizer (class in quark.onnx.cpu_quantizer)": [[4, "quark.onnx.cpu_quantizer.VitisQDQCPUQuantizer", false]], "vitisqdqcpuquantizer (class in quark.onnx.quantizers.cpu_quantizer)": [[63, "quark.onnx.quantizers.cpu_quantizer.VitisQDQCPUQuantizer", false]], "vitisqdqnpucnnquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisQDQNPUCNNQuantizer", false]], "vitisqdqquantizer (class in quark.onnx.qdq_quantizer)": [[54, "quark.onnx.qdq_quantizer.VitisQDQQuantizer", false]], "vitisquantformat (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.VitisQuantFormat", false]], "vitisquanttype (class in quark.onnx.quant_utils)": [[55, "quark.onnx.quant_utils.VitisQuantType", false]], "vocab (class in quark.torch.export.gguf_export.utils)": [[159, "quark.torch.export.gguf_export.utils.Vocab", false]], "weight_stats_hook() (in module quark.torch.quantization.debug)": [[205, "quark.torch.quantization.debug.weight_stats_hook", false]], "zeropointtype (class in quark.torch.quantization.config.type)": [[202, "quark.torch.quantization.config.type.ZeroPointType", false]]}, "objects": {"": [[0, 0, 0, "-", "quark"]], "quark": [[35, 0, 0, "-", "onnx"], [115, 0, 0, "-", "shares"], [188, 0, 0, "-", "torch"], [247, 0, 0, "-", "version"]], "quark.onnx": [[1, 0, 0, "-", "auto_search"], [2, 0, 0, "-", "bias_correction"], [3, 0, 0, "-", "calibrate"], [4, 0, 0, "-", "cpu_quantizer"], [5, 0, 0, "-", "equalization"], [19, 0, 0, "-", "finetuning"], [29, 0, 0, "-", "gptq"], [30, 0, 0, "-", "graph_transformations"], [37, 0, 0, "-", "mprecision"], [39, 0, 0, "-", "onnx_quantizer"], [42, 0, 0, "-", "operators"], [52, 0, 0, "-", "optimizations"], [53, 0, 0, "-", "optimize"], [54, 0, 0, "-", "qdq_quantizer"], [55, 0, 0, "-", "quant_utils"], [60, 0, 0, "-", "quantization"], [61, 0, 0, "-", "quantize"], [65, 0, 0, "-", "quantizers"], [71, 0, 0, "-", "quarot"], [72, 0, 0, "-", "refine"], [73, 0, 0, "-", "registry"], [74, 0, 0, "-", "simulate_dpu"], [75, 0, 0, "-", "simulate_dpu_softmax"], [76, 0, 0, "-", "smooth_quant"], [100, 0, 0, "-", "tools"], [113, 0, 0, "-", "utils"]], "quark.onnx.auto_search": [[1, 1, 1, "", "AssembleIdxs"], [1, 1, 1, "", "SearchSpace"], [1, 3, 1, "", "buildin_eval_func"], [1, 3, 1, "", "cos_metric"], [1, 3, 1, "", "l1_metric"], [1, 3, 1, "", "l2_metric"], [1, 3, 1, "", "psnr_metric"], [1, 3, 1, "", "ssim_metric"]], "quark.onnx.auto_search.AssembleIdxs": [[1, 2, 1, "", "run"], [1, 2, 1, "", "search_forward"]], "quark.onnx.auto_search.SearchSpace": [[1, 2, 1, "", "three_level_spaces"]], "quark.onnx.calibrate": [[3, 1, 1, "", "EntropyCalibrater"], [3, 1, 1, "", "MinMaxCalibrater"], [3, 1, 1, "", "PercentileCalibrater"], [3, 1, 1, "", "PowOfTwoCalibrater"], [3, 1, 1, "", "PowOfTwoCollector"], [3, 3, 1, "", "create_calibrator_float_scale"], [3, 3, 1, "", "create_calibrator_power_of_two"]], "quark.onnx.calibrate.PowOfTwoCalibrater": [[3, 2, 1, "", "augment_graph"], [3, 2, 1, "", "collect_data"], [3, 2, 1, "", "compute_range"]], "quark.onnx.calibrate.PowOfTwoCollector": [[3, 2, 1, "", "collect"], [3, 2, 1, "", "compute_collection_result"]], "quark.onnx.cpu_quantizer": [[4, 1, 1, "", "VitisQDQCPUQuantizer"]], "quark.onnx.equalization": [[5, 1, 1, "", "CLE_PAIR_TYPE"], [5, 1, 1, "", "Equalization"], [5, 3, 1, "", "cle_transforms"]], "quark.onnx.finetuning": [[12, 0, 0, "-", "create_torch"], [18, 0, 0, "-", "fast_finetune"], [20, 0, 0, "-", "onnx_evaluate"], [21, 0, 0, "-", "onnx_subgraph"], [22, 0, 0, "-", "torch_utils"], [23, 0, 0, "-", "torch_utils_test"], [24, 0, 0, "-", "train_torch"]], "quark.onnx.finetuning.create_torch": [[6, 0, 0, "-", "base_fn_quantizers"], [7, 0, 0, "-", "base_qdq_quantizers"], [8, 0, 0, "-", "create_model"], [9, 0, 0, "-", "create_model_ops"], [10, 0, 0, "-", "create_model_test"], [11, 0, 0, "-", "create_model_utils"], [13, 0, 0, "-", "quant_base_ops"], [14, 0, 0, "-", "quant_conv_ops"], [15, 0, 0, "-", "quant_gemm_ops"], [16, 0, 0, "-", "quant_matmul_ops"], [17, 0, 0, "-", "quant_norm_ops"]], "quark.onnx.finetuning.create_torch.base_fn_quantizers": [[6, 1, 1, "", "BFPPrimeQuantDequantFunction"], [6, 1, 1, "", "BFPQuantDequantFunction"], [6, 1, 1, "", "BFPQuantizer"], [6, 1, 1, "", "MXQuantDequantFunction"], [6, 1, 1, "", "MXQuantizer"]], "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPPrimeQuantDequantFunction": [[6, 2, 1, "", "backward"], [6, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_fn_quantizers.BFPQuantDequantFunction": [[6, 2, 1, "", "backward"], [6, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_fn_quantizers.MXQuantDequantFunction": [[6, 2, 1, "", "backward"], [6, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers": [[7, 1, 1, "", "AdaroundConstants"], [7, 1, 1, "", "AdaroundINTQuantizer"], [7, 1, 1, "", "FPQuantizer"], [7, 1, 1, "", "INTDeQuantFunction"], [7, 1, 1, "", "INTQuantDequantFunction"], [7, 1, 1, "", "INTQuantFunction"], [7, 1, 1, "", "INTQuantizer"], [7, 1, 1, "", "RoundHalfToEven"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.AdaroundINTQuantizer": [[7, 2, 1, "", "initialize_alpha"], [7, 2, 1, "", "round_impl"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTDeQuantFunction": [[7, 2, 1, "", "backward"], [7, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantDequantFunction": [[7, 2, 1, "", "backward"], [7, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantFunction": [[7, 2, 1, "", "backward"], [7, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.INTQuantizer": [[7, 2, 1, "", "round_impl"], [7, 2, 1, "", "tensor_sync"]], "quark.onnx.finetuning.create_torch.base_qdq_quantizers.RoundHalfToEven": [[7, 2, 1, "", "backward"], [7, 2, 1, "", "forward"]], "quark.onnx.finetuning.create_torch.create_model": [[8, 1, 1, "", "TorchModel"]], "quark.onnx.finetuning.create_torch.create_model.TorchModel": [[8, 2, 1, "", "forward"], [8, 2, 1, "", "get_bias"], [8, 2, 1, "", "get_weight"], [8, 2, 1, "", "set_bias"], [8, 2, 1, "", "set_weight"]], "quark.onnx.finetuning.create_torch.create_model_ops": [[9, 1, 1, "", "Clip"], [9, 3, 1, "", "convert_act"], [9, 3, 1, "", "convert_conv"], [9, 3, 1, "", "convert_gemm"], [9, 3, 1, "", "convert_matmul"], [9, 3, 1, "", "convert_norm"], [9, 3, 1, "", "convert_ops_to_modules"], [9, 3, 1, "", "extract_padding_params"], [9, 3, 1, "", "extract_padding_params_for_conv"], [9, 3, 1, "", "extract_weight_and_bias"], [9, 3, 1, "", "get_modules_optimized_bias"], [9, 3, 1, "", "get_modules_optimized_weight"], [9, 3, 1, "", "load_weight_and_bias"], [9, 3, 1, "", "param_is_symmetric"], [9, 3, 1, "", "set_modules_original_bias"], [9, 3, 1, "", "set_modules_original_weight"]], "quark.onnx.finetuning.create_torch.create_model_utils": [[11, 3, 1, "", "extract_attr_values"]], "quark.onnx.finetuning.create_torch.quant_base_ops": [[13, 1, 1, "", "QuantizationModule"], [13, 1, 1, "", "QuantizeWrapper"]], "quark.onnx.finetuning.create_torch.quant_conv_ops": [[14, 1, 1, "", "QConv1d"], [14, 1, 1, "", "QConv2d"], [14, 1, 1, "", "QConv3d"], [14, 1, 1, "", "QConvTranspose1d"], [14, 1, 1, "", "QConvTranspose2d"], [14, 1, 1, "", "QConvTranspose3d"]], "quark.onnx.finetuning.create_torch.quant_gemm_ops": [[15, 1, 1, "", "QGemm"]], "quark.onnx.finetuning.create_torch.quant_matmul_ops": [[16, 1, 1, "", "QMatMul"]], "quark.onnx.finetuning.create_torch.quant_norm_ops": [[17, 1, 1, "", "QInstanceNorm1d"], [17, 1, 1, "", "QInstanceNorm2d"], [17, 1, 1, "", "QInstanceNorm3d"], [17, 1, 1, "", "QLayerNorm"]], "quark.onnx.finetuning.fast_finetune": [[18, 3, 1, "", "fast_finetune"]], "quark.onnx.finetuning.onnx_evaluate": [[20, 3, 1, "", "average_L2"], [20, 3, 1, "", "create_session"], [20, 3, 1, "", "inference_model"]], "quark.onnx.finetuning.onnx_subgraph": [[21, 1, 1, "", "Subgraph"]], "quark.onnx.finetuning.torch_utils": [[22, 1, 1, "", "CachedDataset"], [22, 3, 1, "", "convert_onnx_to_torch"], [22, 3, 1, "", "convert_torch_to_onnx"], [22, 3, 1, "", "optimize_module"], [22, 3, 1, "", "parse_options_to_params"], [22, 3, 1, "", "save_torch_model"], [22, 3, 1, "", "setup_seed"], [22, 3, 1, "", "train_torch_module_api"]], "quark.onnx.finetuning.train_torch": [[25, 0, 0, "-", "train_model"], [26, 0, 0, "-", "train_model_loss"], [27, 0, 0, "-", "train_model_param"]], "quark.onnx.finetuning.train_torch.train_model": [[25, 1, 1, "", "ModelOptimizer"]], "quark.onnx.finetuning.train_torch.train_model.ModelOptimizer": [[25, 2, 1, "", "run"]], "quark.onnx.finetuning.train_torch.train_model_loss": [[26, 1, 1, "", "TrainLoss"]], "quark.onnx.finetuning.train_torch.train_model_loss.TrainLoss": [[26, 2, 1, "", "calc_recon_loss"], [26, 2, 1, "", "calc_round_loss"]], "quark.onnx.finetuning.train_torch.train_model_param": [[27, 1, 1, "", "TrainParameters"]], "quark.onnx.gptq": [[28, 0, 0, "-", "gptq"]], "quark.onnx.graph_transformations": [[31, 0, 0, "-", "model_transformer"], [32, 0, 0, "-", "model_transformer_test"], [33, 0, 0, "-", "transforms"], [34, 0, 0, "-", "transforms_pipeline"]], "quark.onnx.graph_transformations.model_transformer": [[31, 1, 1, "", "ModelTransformer"]], "quark.onnx.graph_transformations.model_transformer.ModelTransformer": [[31, 1, 1, "", "NodeType"], [31, 2, 1, "", "transform"]], "quark.onnx.graph_transformations.model_transformer_test": [[32, 1, 1, "", "ModelTransformerTest"], [32, 3, 1, "", "generate_input_initializer"]], "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest": [[32, 1, 1, "", "RemoveRelu"], [32, 1, 1, "", "ReplaceWholeModel"]], "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.RemoveRelu": [[32, 2, 1, "", "pattern"], [32, 2, 1, "", "replacement"]], "quark.onnx.graph_transformations.model_transformer_test.ModelTransformerTest.ReplaceWholeModel": [[32, 2, 1, "", "pattern"], [32, 2, 1, "", "replacement"]], "quark.onnx.graph_transformations.transforms": [[33, 1, 1, "", "NodeTree"], [33, 1, 1, "", "OpTypePattern"], [33, 1, 1, "", "Transform"]], "quark.onnx.graph_transformations.transforms.Transform": [[33, 4, 1, "", "allow_multi_consumers"], [33, 2, 1, "", "pattern"], [33, 2, 1, "", "replacement"]], "quark.onnx.graph_transformations.transforms_pipeline": [[34, 1, 1, "", "TransformsPipeline"]], "quark.onnx.graph_transformations.transforms_pipeline.TransformsPipeline": [[34, 2, 1, "", "apply"], [34, 2, 1, "", "get_configs"]], "quark.onnx.mprecision": [[36, 0, 0, "-", "auto_mixprecision"], [38, 0, 0, "-", "mixed_bfp"]], "quark.onnx.mprecision.auto_mixprecision": [[36, 3, 1, "", "auto_mixprecision"]], "quark.onnx.onnx_quantizer": [[39, 1, 1, "", "ONNXQuantizer"], [39, 1, 1, "", "VitisONNXQuantizer"]], "quark.onnx.onnx_quantizer.VitisONNXQuantizer": [[39, 2, 1, "", "calculate_quantization_params"], [39, 2, 1, "", "find_quant_scale_zp"], [39, 2, 1, "", "find_quantized_value"], [39, 2, 1, "", "quantize_bias_static"], [39, 2, 1, "", "quantize_initializer"], [39, 2, 1, "", "quantize_weight"], [39, 2, 1, "", "quantize_weight_per_channel"]], "quark.onnx.operators": [[41, 0, 0, "-", "custom_ops"], [45, 0, 0, "-", "vai_ops"]], "quark.onnx.operators.custom_ops": [[40, 0, 0, "-", "build_vai_custom_op"]], "quark.onnx.operators.vai_ops": [[43, 0, 0, "-", "concat"], [44, 0, 0, "-", "hardsigmoid"], [46, 0, 0, "-", "layernorm"], [47, 0, 0, "-", "prelu"], [48, 0, 0, "-", "qdq_ops"], [49, 0, 0, "-", "softmax"]], "quark.onnx.optimizations": [[50, 0, 0, "-", "convert_transforms"], [51, 0, 0, "-", "convert_transforms_pipeline"]], "quark.onnx.optimizations.convert_transforms": [[50, 1, 1, "", "AddQDQToQOPTransform"], [50, 1, 1, "", "ConvQDQToQOPTransform"], [50, 1, 1, "", "MatMulQDQToQOPTransform"], [50, 1, 1, "", "MulQDQToQOPTransform"], [50, 1, 1, "", "RemoveQDQTransform"], [50, 1, 1, "", "SigmoidQDQToQOPTransform"]], "quark.onnx.optimizations.convert_transforms.AddQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.ConvQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.MatMulQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.MulQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.RemoveQDQTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms.SigmoidQDQToQOPTransform": [[50, 2, 1, "", "pattern"], [50, 2, 1, "", "replacement"]], "quark.onnx.optimizations.convert_transforms_pipeline": [[51, 1, 1, "", "ConvertQDQToQOPTransformsPipeline"], [51, 1, 1, "", "RemoveQDQTransformsPipeline"]], "quark.onnx.optimizations.convert_transforms_pipeline.ConvertQDQToQOPTransformsPipeline": [[51, 2, 1, "", "apply"]], "quark.onnx.optimizations.convert_transforms_pipeline.RemoveQDQTransformsPipeline": [[51, 2, 1, "", "apply"]], "quark.onnx.optimize": [[53, 1, 1, "", "Optimize"], [53, 3, 1, "", "optimize"]], "quark.onnx.optimize.Optimize": [[53, 2, 1, "", "convert_bn_to_conv"], [53, 2, 1, "", "convert_clip_to_relu"], [53, 2, 1, "", "convert_reduce_mean_to_global_avg_pool"], [53, 2, 1, "", "convert_split_to_slice"], [53, 2, 1, "", "fold_batch_norm"], [53, 2, 1, "", "fold_batch_norm_after_concat"], [53, 2, 1, "", "fuse_instance_norm"], [53, 2, 1, "", "fuse_l2_norm"], [53, 2, 1, "", "split_large_kernel_pool"]], "quark.onnx.qdq_quantizer": [[54, 1, 1, "", "QDQNPUTransformerQuantizer"], [54, 1, 1, "", "QDQQuantizer"], [54, 1, 1, "", "VitisBFPQuantizer"], [54, 1, 1, "", "VitisExtendedQuantizer"], [54, 1, 1, "", "VitisQDQNPUCNNQuantizer"], [54, 1, 1, "", "VitisQDQQuantizer"]], "quark.onnx.qdq_quantizer.QDQNPUTransformerQuantizer": [[54, 2, 1, "", "quantize_bias_tensor"]], "quark.onnx.qdq_quantizer.QDQQuantizer": [[54, 2, 1, "", "quantize_bias_tensor"]], "quark.onnx.quant_utils": [[55, 1, 1, "", "CachedDataReader"], [55, 1, 1, "", "Int16Method"], [55, 1, 1, "", "PathDataReader"], [55, 1, 1, "", "PowerOfTwoMethod"], [55, 1, 1, "", "RandomDataReader"], [55, 1, 1, "", "VitisQuantFormat"], [55, 1, 1, "", "VitisQuantType"], [55, 3, 1, "", "check_hard_sigmoid_condition"], [55, 3, 1, "", "check_model_quantizable"], [55, 3, 1, "", "check_onnx_model"], [55, 3, 1, "", "check_reduce_mean_condition"], [55, 3, 1, "", "check_relu_like_node"], [55, 3, 1, "", "compute_scale_zp"], [55, 3, 1, "", "compute_scale_zp_fp"], [55, 3, 1, "", "customqdq_to_contribqdq"], [55, 3, 1, "", "dequantize_data"], [55, 3, 1, "", "dpu_leaky_relu_alpha"], [55, 3, 1, "", "dump_model"], [55, 3, 1, "", "find_int16_scale"], [55, 3, 1, "", "get_annotate_tensors"], [55, 3, 1, "", "get_clip_min_max"], [55, 3, 1, "", "get_datatype_shape"], [55, 3, 1, "", "get_exclude_nodes"], [55, 3, 1, "", "get_qdq_to_remove"], [55, 3, 1, "", "get_qmin_qmax_for_qType"], [55, 3, 1, "", "get_qrange_for_qType"], [55, 3, 1, "", "infer_shape"], [55, 3, 1, "", "is_approximately_equal"], [55, 3, 1, "", "is_clip_with_min_max"], [55, 3, 1, "", "is_leaky_relu_with_alpha"], [55, 3, 1, "", "is_node_needs_annotated"], [55, 3, 1, "", "is_ort_version_below"], [55, 3, 1, "", "modified_annotate_input"], [55, 3, 1, "", "pos2scale"], [55, 3, 1, "", "print_quantize_dynamic_info"], [55, 3, 1, "", "print_quantize_info"], [55, 3, 1, "", "quantize_data_pof2s"], [55, 3, 1, "", "remove_initializers"], [55, 3, 1, "", "remove_nodes"], [55, 3, 1, "", "run_onnx_model"], [55, 3, 1, "", "scale2pos"]], "quark.onnx.quant_utils.CachedDataReader": [[55, 2, 1, "", "get_next"], [55, 2, 1, "", "reset_iter"]], "quark.onnx.quant_utils.PathDataReader": [[55, 2, 1, "", "get_next"]], "quark.onnx.quant_utils.RandomDataReader": [[55, 2, 1, "", "get_next"]], "quark.onnx.quantization": [[56, 0, 0, "-", "api"], [59, 0, 0, "-", "config"]], "quark.onnx.quantization.api": [[56, 1, 1, "", "ModelQuantizer"]], "quark.onnx.quantization.api.ModelQuantizer": [[56, 2, 1, "", "quantize_model"]], "quark.onnx.quantization.config": [[57, 0, 0, "-", "config"], [58, 0, 0, "-", "custom_config"]], "quark.onnx.quantization.config.config": [[57, 1, 1, "", "Config"], [57, 1, 1, "", "QuantizationConfig"]], "quark.onnx.quantize": [[61, 3, 1, "", "quantize_dynamic"]], "quark.onnx.quantizers": [[62, 0, 0, "-", "bfp_quantizer"], [63, 0, 0, "-", "cpu_quantizer"], [64, 0, 0, "-", "extended_quantizer"], [66, 0, 0, "-", "matmul_nbits_quantizer"], [67, 0, 0, "-", "npu_cnn_quantizer"], [68, 0, 0, "-", "npu_transformer_quantizer"], [69, 0, 0, "-", "onnx_quantizer"], [70, 0, 0, "-", "qdq_quantizer"]], "quark.onnx.quantizers.cpu_quantizer": [[63, 1, 1, "", "VitisQDQCPUQuantizer"]], "quark.onnx.quantizers.matmul_nbits_quantizer": [[66, 1, 1, "", "MatMulNBitsQuantizer"]], "quark.onnx.quarot": [[71, 1, 1, "", "QuaRot"]], "quark.onnx.quarot.QuaRot": [[71, 2, 1, "", "rotate_in_channels"], [71, 2, 1, "", "rotate_out_channels"]], "quark.onnx.refine": [[72, 3, 1, "", "adjust_quantize_info"], [72, 3, 1, "", "align_quantize_info"]], "quark.onnx.simulate_dpu": [[74, 3, 1, "", "simulate_transforms"]], "quark.onnx.smooth_quant": [[76, 1, 1, "", "SmoothQuant"]], "quark.onnx.tools": [[77, 0, 0, "-", "convert_a8w8_npu_to_a8w8_cpu"], [78, 0, 0, "-", "convert_customqdq_to_qdq"], [79, 0, 0, "-", "convert_dynamic_to_fixed"], [80, 0, 0, "-", "convert_fp16_to_bf16"], [81, 0, 0, "-", "convert_fp16_to_bfp16"], [82, 0, 0, "-", "convert_fp16_to_fp32"], [83, 0, 0, "-", "convert_fp32_to_bf16"], [84, 0, 0, "-", "convert_fp32_to_bfp16"], [85, 0, 0, "-", "convert_fp32_to_fp16"], [86, 0, 0, "-", "convert_lstm_to_customlstm"], [87, 0, 0, "-", "convert_nchw_to_nhwc"], [88, 0, 0, "-", "convert_onnx_to_onnxtxt"], [89, 0, 0, "-", "convert_onnxtxt_to_onnx"], [90, 0, 0, "-", "convert_opset_version"], [91, 0, 0, "-", "convert_qdq_to_qop"], [92, 0, 0, "-", "convert_quant_to_float"], [93, 0, 0, "-", "convert_resize_fs_to_pof2s"], [94, 0, 0, "-", "convert_s8s8_to_u8s8"], [95, 0, 0, "-", "convert_shared_initializer_to_unique"], [96, 0, 0, "-", "convert_u16s8_to_s16s8"], [97, 0, 0, "-", "convert_u16u8_to_u8u8"], [98, 0, 0, "-", "evaluate"], [99, 0, 0, "-", "float16"], [101, 0, 0, "-", "insert_clip_bfloat16_qdq"], [102, 0, 0, "-", "print_a16w8_a8w8_nodes"], [103, 0, 0, "-", "random_quantize"], [104, 0, 0, "-", "remove_bf16_cast"], [105, 0, 0, "-", "remove_initializer_from_input"], [106, 0, 0, "-", "remove_qdq"], [107, 0, 0, "-", "remove_qdq_between_ops"], [108, 0, 0, "-", "remove_qdq_mul_add"], [109, 0, 0, "-", "replace_bfloat16_qdq_cast"], [110, 0, 0, "-", "replace_inf_weights"], [111, 0, 0, "-", "save_tensor_hist"], [112, 0, 0, "-", "save_weights_hist"]], "quark.onnx.tools.convert_customqdq_to_qdq": [[78, 3, 1, "", "convert_customqdq_to_qdq"], [78, 3, 1, "", "custom_ops_infer_shapes"]], "quark.onnx.tools.convert_lstm_to_customlstm": [[86, 3, 1, "", "convert_lstm_to_customlstm"], [86, 3, 1, "", "custom_ops_infer_shapes"]], "quark.onnx.tools.convert_qdq_to_qop": [[91, 3, 1, "", "convert_qdq_to_qop"]], "quark.onnx.tools.convert_quant_to_float": [[92, 3, 1, "", "convert_initializers_to_float"]], "quark.onnx.tools.convert_resize_fs_to_pof2s": [[93, 3, 1, "", "pos2scale"], [93, 3, 1, "", "scale2pos"]], "quark.onnx.tools.float16": [[99, 3, 1, "", "convert_float16_to_float"], [99, 3, 1, "", "convert_float_to_float16"], [99, 3, 1, "", "convert_float_to_float16_model_path"], [99, 3, 1, "", "convert_np_to_float"], [99, 3, 1, "", "convert_np_to_float16"], [99, 3, 1, "", "convert_tensor_float16_to_float"], [99, 3, 1, "", "convert_tensor_float_to_float16"]], "quark.onnx.tools.remove_qdq": [[106, 3, 1, "", "remove_qdq"]], "quark.onnx.tools.remove_qdq_between_ops": [[107, 3, 1, "", "find_node_by_output"], [107, 3, 1, "", "remove_qdq_between_ops"]], "quark.onnx.tools.remove_qdq_mul_add": [[108, 3, 1, "", "find_node_by_output"], [108, 3, 1, "", "remove_qdq_mul_add"]], "quark.onnx.tools.replace_inf_weights": [[110, 3, 1, "", "replace_inf_in_onnx_weights"]], "quark.onnx.tools.save_tensor_hist": [[111, 1, 1, "", "HistDataReader"]], "quark.onnx.tools.save_tensor_hist.HistDataReader": [[111, 2, 1, "", "get_next"]], "quark.onnx.utils": [[114, 0, 0, "-", "model_utils"]], "quark.onnx.utils.model_utils": [[114, 3, 1, "", "generate_initializer"], [114, 3, 1, "", "get_tensor_value"], [114, 3, 1, "", "save_model"]], "quark.shares": [[117, 0, 0, "-", "utils"]], "quark.shares.utils": [[116, 0, 0, "-", "import_utils"], [118, 0, 0, "-", "log"], [119, 0, 0, "-", "testing_utils"]], "quark.shares.utils.log": [[118, 1, 1, "", "CustomFormatter"], [118, 1, 1, "", "DuplicateFilter"]], "quark.shares.utils.log.CustomFormatter": [[118, 2, 1, "", "format"]], "quark.shares.utils.log.DuplicateFilter": [[118, 2, 1, "", "filter"]], "quark.shares.utils.testing_utils": [[119, 3, 1, "", "delete_directory_content"], [119, 3, 1, "", "require_accelerate"], [119, 3, 1, "", "require_torch_gpu"], [119, 3, 1, "", "retry_flaky_test"]], "quark.torch": [[133, 0, 0, "-", "algorithm"], [160, 0, 0, "-", "export"], [187, 0, 0, "-", "extensions"], [192, 0, 0, "-", "kernel"], [195, 0, 0, "-", "pruning"], [229, 0, 0, "-", "quantization"], [245, 0, 0, "-", "utils"]], "quark.torch.algorithm": [[120, 0, 0, "-", "api"], [123, 0, 0, "-", "awq"], [130, 0, 0, "-", "blockwise_tuning"], [132, 0, 0, "-", "gptq"], [134, 0, 0, "-", "osscar"], [136, 0, 0, "-", "processor"], [137, 0, 0, "-", "quarot"], [142, 0, 0, "-", "rotation"], [146, 0, 0, "-", "utils"]], "quark.torch.algorithm.awq": [[121, 0, 0, "-", "auto_smooth"], [122, 0, 0, "-", "awq"], [125, 0, 0, "-", "modules"], [126, 0, 0, "-", "scale"], [127, 0, 0, "-", "smooth"]], "quark.torch.algorithm.awq.auto_smooth": [[121, 1, 1, "", "AutoSmoothQuantProcessor"]], "quark.torch.algorithm.awq.awq": [[122, 1, 1, "", "AwqProcessor"]], "quark.torch.algorithm.awq.modules": [[124, 0, 0, "-", "act"]], "quark.torch.algorithm.awq.modules.act": [[124, 1, 1, "", "ScaledActivation"]], "quark.torch.algorithm.awq.smooth": [[127, 1, 1, "", "SmoothQuantProcessor"]], "quark.torch.algorithm.blockwise_tuning": [[128, 0, 0, "-", "blockwise_tuning"], [129, 0, 0, "-", "blockwise_utils"]], "quark.torch.algorithm.blockwise_tuning.blockwise_tuning": [[128, 1, 1, "", "BlockwiseTuningProcessor"]], "quark.torch.algorithm.gptq": [[131, 0, 0, "-", "gptq"]], "quark.torch.algorithm.gptq.gptq": [[131, 1, 1, "", "GptqProcessor"]], "quark.torch.algorithm.osscar": [[135, 0, 0, "-", "osscar"]], "quark.torch.algorithm.osscar.osscar": [[135, 1, 1, "", "OsscarProcessor"]], "quark.torch.algorithm.processor": [[136, 1, 1, "", "BaseAlgoProcessor"]], "quark.torch.algorithm.quarot": [[138, 0, 0, "-", "monkeypatch"], [139, 0, 0, "-", "quarot"], [140, 0, 0, "-", "utils"]], "quark.torch.algorithm.quarot.monkeypatch": [[138, 3, 1, "", "add_wrapper_after_function_call_in_method"], [138, 3, 1, "", "copy_func_with_new_globals"]], "quark.torch.algorithm.quarot.quarot": [[139, 1, 1, "", "QuaRotProcessor"]], "quark.torch.algorithm.quarot.utils": [[140, 1, 1, "", "QKRotation"], [140, 1, 1, "", "R4Wrapper"], [140, 3, 1, "", "add_qk_rotation_after_function_call_in_forward"], [140, 3, 1, "", "hadamard_multiply"], [140, 3, 1, "", "hadamard_transform"], [140, 3, 1, "", "rotate_in_channels2"], [140, 3, 1, "", "rotate_out_channels2"]], "quark.torch.algorithm.rotation": [[141, 0, 0, "-", "hadamard"], [143, 0, 0, "-", "rotation"], [144, 0, 0, "-", "rotation_utils"]], "quark.torch.algorithm.rotation.hadamard": [[141, 3, 1, "", "get_hadamard_matrices"], [141, 3, 1, "", "hardmard_transform"], [141, 3, 1, "", "random_hadamard_matrix"]], "quark.torch.algorithm.rotation.rotation": [[143, 1, 1, "", "RotationProcessor"]], "quark.torch.algorithm.rotation.rotation_utils": [[144, 1, 1, "", "RMSNorm"], [144, 3, 1, "", "get_rotation_matrix"], [144, 3, 1, "", "rotate_in_channels"], [144, 3, 1, "", "rotate_out_channels"]], "quark.torch.algorithm.rotation.rotation_utils.RMSNorm": [[144, 2, 1, "", "forward"]], "quark.torch.algorithm.utils": [[145, 0, 0, "-", "auto_config"], [147, 0, 0, "-", "module"], [148, 0, 0, "-", "prepare"], [149, 0, 0, "-", "utils"]], "quark.torch.algorithm.utils.module": [[147, 3, 1, "", "get_nested_attr_from_module"]], "quark.torch.algorithm.utils.utils": [[149, 1, 1, "", "TensorData"]], "quark.torch.export": [[150, 0, 0, "-", "api"], [152, 0, 0, "-", "config"], [153, 0, 0, "-", "constants"], [157, 0, 0, "-", "gguf_export"], [167, 0, 0, "-", "json_export"], [170, 0, 0, "-", "main_export"], [173, 0, 0, "-", "main_import"], [175, 0, 0, "-", "nn"], [179, 0, 0, "-", "onnx"], [180, 0, 0, "-", "utils"]], "quark.torch.export.api": [[150, 1, 1, "", "ModelExporter"], [150, 1, 1, "", "ModelImporter"], [150, 3, 1, "", "save_params"]], "quark.torch.export.api.ModelExporter": [[150, 2, 1, "", "export_gguf_model"], [150, 2, 1, "", "export_onnx_model"], [150, 2, 1, "", "export_quark_model"], [150, 2, 1, "", "get_export_model"], [150, 2, 1, "", "reset_model"]], "quark.torch.export.api.ModelImporter": [[150, 2, 1, "", "import_model"], [150, 2, 1, "", "import_model_info"]], "quark.torch.export.config": [[151, 0, 0, "-", "config"]], "quark.torch.export.config.config": [[151, 1, 1, "", "ExporterConfig"], [151, 1, 1, "", "JsonExporterConfig"], [151, 1, 1, "", "OnnxExporterConfig"]], "quark.torch.export.gguf_export": [[154, 0, 0, "-", "api"], [155, 0, 0, "-", "gguf_model_converter"], [156, 0, 0, "-", "gguf_model_writer"], [158, 0, 0, "-", "tensor_convert"], [159, 0, 0, "-", "utils"]], "quark.torch.export.gguf_export.api": [[154, 3, 1, "", "convert_exported_model_to_gguf"]], "quark.torch.export.gguf_export.gguf_model_writer": [[156, 1, 1, "", "LlamaModelWriter"], [156, 1, 1, "", "ModelWriter"], [156, 1, 1, "", "SentencePieceTokenTypes"]], "quark.torch.export.gguf_export.utils": [[159, 1, 1, "", "BaseVocab"], [159, 1, 1, "", "BpeVocab"], [159, 1, 1, "", "NoVocab"], [159, 1, 1, "", "Vocab"]], "quark.torch.export.json_export": [[161, 0, 0, "-", "builder"], [165, 0, 0, "-", "converter"], [168, 0, 0, "-", "utils"]], "quark.torch.export.json_export.builder": [[162, 0, 0, "-", "llm_info"], [163, 0, 0, "-", "llm_info_builder"], [164, 0, 0, "-", "native_model_info_builder"]], "quark.torch.export.json_export.builder.llm_info": [[162, 1, 1, "", "EmbeddingType"], [162, 1, 1, "", "LayerNormType"]], "quark.torch.export.json_export.converter": [[166, 0, 0, "-", "llm_info_converter"]], "quark.torch.export.json_export.utils": [[169, 0, 0, "-", "utils"]], "quark.torch.export.json_export.utils.utils": [[169, 3, 1, "", "split_model_info"]], "quark.torch.export.main_export": [[171, 0, 0, "-", "model_post_process"], [172, 0, 0, "-", "quant_config_parser"]], "quark.torch.export.main_import": [[174, 0, 0, "-", "pretrained_config"]], "quark.torch.export.nn": [[176, 0, 0, "-", "modules"]], "quark.torch.export.nn.modules": [[177, 0, 0, "-", "qparamslinear"], [178, 0, 0, "-", "realquantizer"]], "quark.torch.export.nn.modules.qparamslinear": [[177, 1, 1, "", "QParamsLinear"], [177, 1, 1, "", "QparamsOperator"]], "quark.torch.export.nn.modules.qparamslinear.QParamsLinear": [[177, 2, 1, "", "forward"], [177, 2, 1, "", "from_module"], [177, 2, 1, "", "pack_qinfo"], [177, 2, 1, "", "state_dict"]], "quark.torch.export.nn.modules.realquantizer": [[178, 1, 1, "", "NonScaledRealQuantizer"], [178, 1, 1, "", "RealQuantizerBase"], [178, 1, 1, "", "ScaledRealQuantizer"]], "quark.torch.export.nn.modules.realquantizer.NonScaledRealQuantizer": [[178, 2, 1, "", "to_real_quantize_params"]], "quark.torch.export.nn.modules.realquantizer.ScaledRealQuantizer": [[178, 2, 1, "", "to_real_quantize_params"]], "quark.torch.export.utils": [[180, 3, 1, "", "preprocess_import_info"], [180, 3, 1, "", "split_params_for_DbrxExperts"]], "quark.torch.extensions": [[184, 0, 0, "-", "brevitas"]], "quark.torch.extensions.brevitas": [[181, 0, 0, "-", "algos"], [182, 0, 0, "-", "api"], [183, 0, 0, "-", "config"], [185, 0, 0, "-", "mapping"], [186, 0, 0, "-", "verification"]], "quark.torch.extensions.brevitas.algos": [[181, 1, 1, "", "ActivationEqualization"], [181, 1, 1, "", "BiasCorrection"], [181, 1, 1, "", "GPFA2Q"], [181, 1, 1, "", "GPFQ"], [181, 1, 1, "", "GPTQ"], [181, 1, 1, "", "Preprocess"]], "quark.torch.extensions.brevitas.api": [[182, 1, 1, "", "ModelExporter"], [182, 1, 1, "", "ModelQuantizer"]], "quark.torch.extensions.brevitas.api.ModelExporter": [[182, 2, 1, "", "export_onnx_model"]], "quark.torch.extensions.brevitas.api.ModelQuantizer": [[182, 2, 1, "", "quantize_model"]], "quark.torch.extensions.brevitas.config": [[183, 1, 1, "", "Backend"], [183, 1, 1, "", "Config"], [183, 1, 1, "", "ParamType"], [183, 1, 1, "", "QuantType"], [183, 1, 1, "", "QuantizationConfig"], [183, 1, 1, "", "QuantizationSpec"]], "quark.torch.extensions.brevitas.verification": [[186, 1, 1, "", "ConfigVerifier"]], "quark.torch.kernel": [[192, 1, 1, "", "DeQuantizeFunction"], [192, 1, 1, "", "DequantE4M3Function"], [192, 1, 1, "", "DequantE5M2Function"], [192, 1, 1, "", "LSQQuantize"], [192, 1, 1, "", "NonScaledFakeQuantizeFunction"], [192, 1, 1, "", "NonScaledRealQuantizeFunction"], [192, 1, 1, "", "QuantE4M3Function"], [192, 1, 1, "", "QuantE5M2Function"], [192, 1, 1, "", "ScaledFakeQuantizeFunction"], [192, 1, 1, "", "ScaledRealQuantizeFunction"], [192, 1, 1, "", "TQTQuantize"], [191, 0, 0, "-", "hw_emulation"]], "quark.torch.kernel.DeQuantizeFunction": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.DequantE4M3Function": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.DequantE5M2Function": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.LSQQuantize": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.NonScaledFakeQuantizeFunction": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.NonScaledRealQuantizeFunction": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.QuantE4M3Function": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.QuantE5M2Function": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.ScaledFakeQuantizeFunction": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.ScaledRealQuantizeFunction": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.TQTQuantize": [[192, 2, 1, "", "backward"], [192, 2, 1, "", "forward"]], "quark.torch.kernel.hw_emulation": [[189, 0, 0, "-", "extensions"], [190, 0, 0, "-", "hw_emulation_interface"]], "quark.torch.pruning": [[193, 0, 0, "-", "api"], [194, 0, 0, "-", "config"], [196, 0, 0, "-", "model_transformation"], [197, 0, 0, "-", "utils"]], "quark.torch.pruning.api": [[193, 1, 1, "", "ModelPruner"]], "quark.torch.pruning.api.ModelPruner": [[193, 2, 1, "", "pruning_model"]], "quark.torch.pruning.config": [[194, 1, 1, "", "AlgoConfig"], [194, 1, 1, "", "AlgoConfigBase"], [194, 1, 1, "", "BlockwiseTuningConfig"], [194, 1, 1, "", "Config"], [194, 1, 1, "", "ConfigBase"], [194, 1, 1, "", "OSSCARConfig"]], "quark.torch.quantization": [[198, 0, 0, "-", "api"], [201, 0, 0, "-", "config"], [204, 0, 0, "-", "constants"], [205, 0, 0, "-", "debug"], [208, 0, 0, "-", "graph"], [230, 0, 0, "-", "model_transformation"], [231, 0, 0, "-", "nn"], [239, 0, 0, "-", "observer"], [243, 0, 0, "-", "tensor_quantize"], [244, 0, 0, "-", "utils"]], "quark.torch.quantization.api": [[198, 1, 1, "", "ModelQuantizer"], [198, 3, 1, "", "load_params"]], "quark.torch.quantization.api.ModelQuantizer": [[198, 2, 1, "", "freeze"], [198, 2, 1, "", "quantize_model"]], "quark.torch.quantization.config": [[199, 0, 0, "-", "config"], [200, 0, 0, "-", "config_verification"], [202, 0, 0, "-", "type"], [203, 0, 0, "-", "utils"]], "quark.torch.quantization.config.config": [[199, 1, 1, "", "AWQConfig"], [199, 1, 1, "", "AlgoConfig"], [199, 1, 1, "", "AlgoConfigBase"], [199, 1, 1, "", "AutoSmoothQuantConfig"], [199, 1, 1, "", "BFP16Spec"], [199, 1, 1, "", "Bfloat16Spec"], [199, 1, 1, "", "Config"], [199, 1, 1, "", "ConfigBase"], [199, 1, 1, "", "DataTypeSpec"], [199, 1, 1, "", "FP8E4M3PerChannelSpec"], [199, 1, 1, "", "FP8E4M3PerGroupSpec"], [199, 1, 1, "", "FP8E4M3PerTensorSpec"], [199, 1, 1, "", "FP8E5M2PerChannelSpec"], [199, 1, 1, "", "FP8E5M2PerGroupSpec"], [199, 1, 1, "", "FP8E5M2PerTensorSpec"], [199, 1, 1, "", "Float16Spec"], [199, 1, 1, "", "GPTQConfig"], [199, 1, 1, "", "Int4PerChannelSpec"], [199, 1, 1, "", "Int4PerGroupSpec"], [199, 1, 1, "", "Int4PerTensorSpec"], [199, 1, 1, "", "Int8PerChannelSpec"], [199, 1, 1, "", "Int8PerGroupSpec"], [199, 1, 1, "", "Int8PerTensorSpec"], [199, 1, 1, "", "MX6Spec"], [199, 1, 1, "", "MX9Spec"], [199, 1, 1, "", "MXSpec"], [199, 1, 1, "", "PreQuantOptConfig"], [199, 1, 1, "", "QuaRotConfig"], [199, 1, 1, "", "QuantizationConfig"], [199, 1, 1, "", "QuantizationSpec"], [199, 1, 1, "", "RotationConfig"], [199, 1, 1, "", "SmoothQuantConfig"], [199, 1, 1, "", "TQTSpec"], [199, 1, 1, "", "Uint4PerChannelSpec"], [199, 1, 1, "", "Uint4PerGroupSpec"], [199, 1, 1, "", "Uint4PerTensorSpec"], [199, 1, 1, "", "Uint8PerChannelSpec"], [199, 1, 1, "", "Uint8PerGroupSpec"], [199, 1, 1, "", "Uint8PerTensorSpec"], [199, 3, 1, "", "load_pre_optimization_config_from_file"], [199, 3, 1, "", "load_quant_algo_config_from_file"]], "quark.torch.quantization.config.config.Config": [[199, 2, 1, "", "add_pre_optimization_config"], [199, 2, 1, "", "set_algo_config"]], "quark.torch.quantization.config.type": [[202, 1, 1, "", "DeviceType"], [202, 1, 1, "", "Dtype"], [202, 1, 1, "", "QSchemeType"], [202, 1, 1, "", "QuantizationMode"], [202, 1, 1, "", "RoundType"], [202, 1, 1, "", "ScaleType"], [202, 1, 1, "", "TQTThresholdInitMeth"], [202, 1, 1, "", "ZeroPointType"]], "quark.torch.quantization.config.utils": [[203, 3, 1, "", "dataclass_pretty_string"]], "quark.torch.quantization.debug": [[205, 3, 1, "", "activation_stats_hook"], [205, 3, 1, "", "barplot"], [205, 3, 1, "", "collect_quantization_statistics"], [205, 3, 1, "", "distribution_plot"], [205, 3, 1, "", "insert_stats_hooks"], [205, 3, 1, "", "save_distribution_histogram"], [205, 3, 1, "", "summarize_activation"], [205, 3, 1, "", "summarize_weight"], [205, 3, 1, "", "weight_stats_hook"]], "quark.torch.quantization.graph": [[207, 0, 0, "-", "fx"], [210, 0, 0, "-", "optimization"], [224, 0, 0, "-", "processor"], [228, 0, 0, "-", "torch_utils"]], "quark.torch.quantization.graph.fx": [[206, 0, 0, "-", "base"]], "quark.torch.quantization.graph.fx.base": [[206, 1, 1, "", "GraphTransform"], [206, 1, 1, "", "Transform"]], "quark.torch.quantization.graph.optimization": [[209, 0, 0, "-", "activate_dropout"], [211, 0, 0, "-", "model_optimization"], [212, 0, 0, "-", "modify_reshape_param"], [213, 0, 0, "-", "opt_pass_manager"], [214, 0, 0, "-", "post_quant"], [216, 0, 0, "-", "pre_quant"], [222, 0, 0, "-", "remove_dropout_node"], [223, 0, 0, "-", "utils"]], "quark.torch.quantization.graph.optimization.activate_dropout": [[209, 1, 1, "", "ActivateDropoutNode"]], "quark.torch.quantization.graph.optimization.model_optimization": [[211, 3, 1, "", "trans_opsfunc_2_quant_module"]], "quark.torch.quantization.graph.optimization.modify_reshape_param": [[212, 3, 1, "", "modify_reshape_param"]], "quark.torch.quantization.graph.optimization.opt_pass_manager": [[213, 1, 1, "", "OptPassBase"], [213, 1, 1, "", "OptPassManager"]], "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassBase": [[213, 2, 1, "", "call"], [213, 2, 1, "", "ensures"], [213, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.opt_pass_manager.OptPassManager": [[213, 2, 1, "", "add_checks"], [213, 2, 1, "", "add_pass"]], "quark.torch.quantization.graph.optimization.post_quant": [[215, 0, 0, "-", "opt_pass_after_quant"]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant": [[215, 1, 1, "", "ConvertClip2ReLUQOPass"]], "quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant.ConvertClip2ReLUQOPass": [[215, 2, 1, "", "call"], [215, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant": [[217, 0, 0, "-", "opt_pass_before_quant"], [218, 0, 0, "-", "replace_conv2d_to_qtconv2d"], [219, 0, 0, "-", "replace_conv_bn_to_qt_model"], [220, 0, 0, "-", "replace_convtranspose2d_to_qtconvtranspose2d"], [221, 0, 0, "-", "replace_linear_to_qtlinear"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant": [[217, 1, 1, "", "ConvertBn2D2ConvQOPass"], [217, 1, 1, "", "ConvertReduceMean2GapQOPass"], [217, 1, 1, "", "SplitQuantModuleCalledOverOnce"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertBn2D2ConvQOPass": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.ConvertReduceMean2GapQOPass": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant.SplitQuantModuleCalledOverOnce": [[217, 2, 1, "", "call"], [217, 2, 1, "", "requires"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d": [[218, 3, 1, "", "replace_conv2d_qtconv2d"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model": [[219, 3, 1, "", "replace_conv2dbn_quantizedconv_module"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d": [[220, 3, 1, "", "replace_convtranspose2d_qtconvtranspose2d"]], "quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear": [[221, 3, 1, "", "replace_linear_qtlinear"]], "quark.torch.quantization.graph.optimization.remove_dropout_node": [[222, 1, 1, "", "RemoveDropoutNode"]], "quark.torch.quantization.graph.processor": [[225, 0, 0, "-", "insert_quantizer"], [226, 0, 0, "-", "processor"], [227, 0, 0, "-", "processor_utils"]], "quark.torch.quantization.graph.processor.insert_quantizer": [[225, 3, 1, "", "insert_quantizer"]], "quark.torch.quantization.graph.processor.processor": [[226, 3, 1, "", "freeze_model"], [226, 3, 1, "", "mark_exclude_nodes"], [226, 3, 1, "", "transform_for_annotation"]], "quark.torch.quantization.graph.torch_utils": [[228, 5, 1, "", "BATCHNORM_OPS"], [228, 5, 1, "", "CAT_OPS"], [228, 3, 1, "", "allow_exported_model_train_eval"], [228, 3, 1, "", "is_batchnorm2d_node"], [228, 3, 1, "", "is_cat_node"], [228, 3, 1, "", "is_conv1d_node"], [228, 3, 1, "", "is_conv2d_node"], [228, 3, 1, "", "is_conv3d_node"], [228, 3, 1, "", "is_convtranspose2d_node"], [228, 3, 1, "", "is_dropout_node"]], "quark.torch.quantization.model_transformation": [[230, 3, 1, "", "in_place_replace_layer"], [230, 3, 1, "", "process_model_transformation"], [230, 3, 1, "", "setup_config_per_layer"]], "quark.torch.quantization.nn": [[232, 0, 0, "-", "modules"], [238, 0, 0, "-", "utils"]], "quark.torch.quantization.nn.modules": [[233, 0, 0, "-", "mixin"], [234, 0, 0, "-", "quantize_conv"], [235, 0, 0, "-", "quantize_conv_bn_fused"], [236, 0, 0, "-", "quantize_embed"], [237, 0, 0, "-", "quantize_linear"]], "quark.torch.quantization.nn.modules.mixin": [[233, 1, 1, "", "QuantMixin"]], "quark.torch.quantization.nn.modules.quantize_conv": [[234, 1, 1, "", "QuantConv2d"], [234, 1, 1, "", "QuantConvTranspose2d"]], "quark.torch.quantization.nn.modules.quantize_conv_bn_fused": [[235, 1, 1, "", "QuantizedConvBatchNorm2d"]], "quark.torch.quantization.nn.modules.quantize_embed": [[236, 1, 1, "", "QuantEmbedding"], [236, 1, 1, "", "QuantEmbeddingBag"]], "quark.torch.quantization.nn.modules.quantize_embed.QuantEmbeddingBag": [[236, 2, 1, "", "forward"]], "quark.torch.quantization.nn.modules.quantize_linear": [[237, 1, 1, "", "QuantLinear"]], "quark.torch.quantization.nn.utils": [[238, 3, 1, "", "check_min_max_valid"]], "quark.torch.quantization.observer": [[240, 0, 0, "-", "lsq_observer"], [241, 0, 0, "-", "observer"], [242, 0, 0, "-", "tqt_observer"]], "quark.torch.quantization.observer.lsq_observer": [[240, 1, 1, "", "LSQObserver"]], "quark.torch.quantization.observer.observer": [[241, 1, 1, "", "ObserverBase"], [241, 1, 1, "", "PerBlockBFPObserver"], [241, 1, 1, "", "PerBlockMXObserver"], [241, 1, 1, "", "PerChannelMinMaxObserver"], [241, 1, 1, "", "PerGroupMinMaxObserver"], [241, 1, 1, "", "PerTensorHistogramObserver"], [241, 1, 1, "", "PerTensorHistogramObserverPro"], [241, 1, 1, "", "PerTensorMSEObserver"], [241, 1, 1, "", "PerTensorMinMaxObserver"], [241, 1, 1, "", "PerTensorPercentileObserver"], [241, 1, 1, "", "PlaceholderObserver"], [241, 1, 1, "", "UniformScalingObserver"]], "quark.torch.quantization.observer.observer.PerGroupMinMaxObserver": [[241, 2, 1, "", "calculate_qparams"]], "quark.torch.quantization.observer.observer.PerTensorHistogramObserver": [[241, 2, 1, "", "forward"]], "quark.torch.quantization.observer.observer.PerTensorMSEObserver": [[241, 2, 1, "", "get_min_max_by_mse"]], "quark.torch.quantization.observer.observer.PerTensorMinMaxObserver": [[241, 2, 1, "", "forward"]], "quark.torch.quantization.observer.observer.PerTensorPercentileObserver": [[241, 2, 1, "", "get_min_max_by_percentile"]], "quark.torch.quantization.observer.observer.PlaceholderObserver": [[241, 2, 1, "", "extra_repr"]], "quark.torch.quantization.observer.observer.UniformScalingObserver": [[241, 2, 1, "", "calculate_qparams"], [241, 2, 1, "", "extra_repr"], [241, 2, 1, "", "reset_min_max_vals"]], "quark.torch.quantization.observer.tqt_observer": [[242, 1, 1, "", "TQTObserver"]], "quark.torch.quantization.observer.tqt_observer.TQTObserver": [[242, 2, 1, "", "get_fix_position"]], "quark.torch.quantization.tensor_quantize": [[243, 1, 1, "", "FakeQuantizeBase"], [243, 1, 1, "", "FreezedScaledFakeQuantize"], [243, 1, 1, "", "NonScaledFakeQuantize"], [243, 1, 1, "", "ScaledFakeQuantize"]], "quark.torch.quantization.tensor_quantize.FakeQuantizeBase": [[243, 2, 1, "", "update_buffer"]], "quark.torch.quantization.tensor_quantize.ScaledFakeQuantize": [[243, 2, 1, "", "extra_repr"]], "quark.torch.quantization.utils": [[244, 3, 1, "", "set_op_by_name"], [244, 3, 1, "", "t_exponent"]], "quark.torch.utils": [[246, 0, 0, "-", "pack"]]}, "objnames": {"0": ["py", "module", "Python module"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "function", "Python function"], "4": ["py", "property", "Python property"], "5": ["py", "data", "Python data"]}, "objtypes": {"0": "py:module", "1": "py:class", "2": "py:method", "3": "py:function", "4": "py:property", "5": "py:data"}, "terms": {"": [1, 3, 9, 13, 14, 15, 16, 17, 22, 25, 32, 39, 54, 55, 90, 93, 118, 140, 150, 177, 182, 193, 198, 213, 217, 230, 236, 241, 250, 251, 252, 257, 258, 259, 260, 261, 265, 267, 279, 280, 285, 291, 295, 296, 297, 298, 299, 301, 303, 304, 308, 312, 316, 318, 319, 320, 335, 336, 337, 339, 341, 342], "0": [1, 3, 5, 6, 7, 13, 15, 27, 33, 39, 54, 55, 57, 99, 110, 159, 181, 192, 194, 199, 202, 203, 215, 218, 219, 220, 234, 235, 236, 241, 250, 251, 252, 253, 255, 257, 259, 265, 267, 272, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 291, 292, 293, 295, 296, 297, 298, 299, 301, 302, 303, 307, 309, 312, 316, 317, 318, 319, 320, 333, 336, 337, 338, 339, 340, 341], "000": [266, 267, 268, 269, 271, 275, 277], "0000": 236, "000001": [259, 295], "00001": [257, 286, 287, 292, 296, 297], "0001": 299, "001": [299, 341], "0010": 315, "0044": 236, "00456": 278, "01": [3, 27, 199, 257, 279, 280, 299, 341], "02": 278, "0251": 236, "0284": 315, "0309": 236, "0317": [273, 274, 276, 281], "03236": 320, "03302": 320, "03372": 320, "0364": 236, "042": 271, "05": 235, "052": 267, "0523": 236, "06": [55, 144], "0635": 236, "06d": 265, "07": [5, 99], "0748": 236, "0755": 315, "076": 267, "07941": 202, "08": [315, 316, 340, 342], "08066": 202, "08342": 235, "084": 319, "085": 319, "09": [278, 340], "090": 267, "0b": 315, "1": [1, 3, 5, 6, 7, 9, 13, 27, 39, 54, 55, 57, 82, 85, 95, 99, 124, 177, 181, 182, 192, 194, 198, 199, 202, 212, 217, 218, 219, 220, 233, 234, 235, 236, 241, 242, 243, 250, 251, 252, 253, 257, 260, 262, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 283, 284, 285, 286, 287, 290, 291, 292, 296, 297, 298, 299, 303, 304, 307, 309, 311, 313, 314, 331, 332, 333, 336, 337, 339, 340], "10": [236, 250, 257, 265, 277, 278, 291, 303, 315, 317, 319, 339], "100": [3, 27, 241, 259, 295, 296, 297, 299], "1000": [27, 252, 257, 266, 267, 268, 269, 271, 275, 277, 284, 285, 294, 296, 297, 299, 338], "10000": [99, 110, 286, 287, 292, 293], "1024": [251, 291, 318], "10518": 27, "10568": [26, 27], "108": 339, "11": [9, 250, 251, 278, 296, 315], "1110": 251, "1111": 251, "114": [267, 275], "1151": 236, "1195": 315, "11b": [310, 311], "12": [250, 267, 284, 286, 289, 292, 337, 339], "120": 273, "123": 339, "125m": [198, 255, 274, 281, 289, 304, 316, 318, 333, 338], "127": [202, 257, 303], "128": [3, 66, 198, 202, 257, 303, 305, 315, 316, 317, 324, 336, 337, 341], "12_quantiz": [267, 275, 286, 292], "13": [291, 296, 315], "1306": 236, "135": 339, "13503277": 138, "1370": 315, "1382": 315, "13908052444458": 339, "13b": [150, 322], "14": 315, "140": 339, "148": 339, "14b": 315, "15": [202, 340], "1535": 236, "156": 339, "16": [39, 55, 181, 202, 250, 251, 257, 259, 278, 286, 287, 292, 294, 295, 296, 298, 300, 301, 303, 316, 340], "16406": 278, "1655": 236, "1685": 236, "16b": [316, 342], "17": [250, 277, 291, 301], "1705472343": [259, 295, 296, 297], "172": 339, "18": [257, 267, 319, 342], "1805": 202, "1806": 235, "181324005126953": 339, "182": 267, "18944": 339, "19": [257, 301, 340, 342], "1903": 202, "1959": 315, "1d": 236, "1e": [55, 99, 144, 199, 235, 294, 337, 341], "1k": [266, 267, 268, 269, 271, 275, 277], "1st": 303, "2": [1, 6, 7, 9, 27, 39, 55, 57, 82, 85, 99, 192, 194, 199, 202, 218, 219, 220, 235, 236, 242, 250, 251, 254, 257, 260, 265, 267, 268, 269, 275, 278, 280, 283, 284, 287, 289, 291, 296, 297, 298, 299, 303, 304, 307, 309, 311, 313, 314, 333, 339, 340], "20": [9, 27, 124, 177, 181, 233, 243, 250, 312, 314, 339, 340], "200": 284, "2004": [26, 27], "2006": 27, "2012": [266, 267, 268, 269, 271, 275, 277], "2014": 309, "2017": 270, "2020": [26, 27], "2022": [250, 301], "2024": [270, 276, 278, 294, 295, 296, 297, 299, 315, 316, 342], "2025": 298, "2045": 241, "2048": [3, 272, 273, 274, 276, 278, 281, 282, 283, 316], "21": 309, "2145": 236, "2155": 340, "22": [287, 340], "224": [257, 260, 265, 266, 291, 319], "23": [251, 309, 338, 340], "232": 271, "2404": 278, "2405": 278, "2462": 315, "248": 267, "24969482421875": 339, "25": [212, 275], "253": 339, "254": 319, "255": [202, 251, 257], "256": [241, 267], "26": [272, 278, 282, 283, 340], "26559": 309, "2678": 236, "269912719726": 339, "27": [273, 274, 276, 281, 340], "274": 275, "278": 266, "28": [273, 274, 276, 281, 340], "29": [309, 315, 340], "291": 319, "2988730": 138, "2b": 315, "2d": [217, 236, 339], "2e": [257, 317], "2gb": [3, 22, 61, 71, 76, 99, 257, 290, 291], "2nd": 303, "2x1": 339, "3": [1, 39, 57, 194, 199, 202, 235, 236, 242, 250, 257, 260, 265, 266, 268, 269, 280, 282, 284, 291, 298, 299, 304, 307, 311, 315, 319, 333, 336, 337, 339, 340, 342], "30": [274, 340], "3000": [286, 287, 292], "301": 319, "31": [251, 291, 307, 309], "32": [150, 251, 257, 297, 298, 302, 303, 315, 316, 320, 322, 324, 333, 337], "322": 268, "33": 277, "3448": 236, "34745": 309, "34854": 309, "35": 340, "36": [315, 339], "3600": 299, "3604": 274, "3616": 236, "3640": 339, "3677": 236, "37": 340, "3794": 315, "38": 340, "384": [241, 274, 276, 281], "385": 281, "3892": 337, "3b": 315, "3ex": 303, "4": [1, 39, 57, 66, 105, 194, 199, 202, 236, 250, 251, 257, 265, 266, 268, 269, 272, 274, 278, 279, 280, 284, 304, 305, 315, 317, 319, 320, 333, 337, 339, 340], "40": [339, 340], "4004": 236, "4025": 315, "406": 274, "4096": [254, 257, 278, 318], "41": [267, 269, 309], "42": [309, 315, 340], "420": 269, "424": [266, 268, 269], "4315": 281, "4350": 236, "4362": 236, "43882751464844": 339, "44": [316, 317], "440": 277, "444": 275, "446": 267, "44639": 309, "45": [309, 339], "4532": 319, "4537": 319, "456": 271, "46": [315, 340], "469": 319, "47": 267, "4721": 315, "4759": 319, "480": [273, 274, 276, 281], "4838": 315, "484": 319, "486": 277, "4970": 236, "4b": [66, 257], "4bit": 282, "4k": 317, "5": [5, 9, 27, 39, 55, 72, 119, 124, 177, 181, 202, 233, 236, 243, 250, 251, 255, 257, 259, 266, 268, 269, 271, 272, 275, 277, 278, 280, 282, 283, 284, 295, 299, 304, 309, 315, 317, 318, 319, 320, 337, 340, 341], "50": [266, 267, 268, 269, 271, 275, 277, 319], "500": [299, 309, 317], "5000": 309, "502": 271, "5081": 315, "51": 340, "510": 251, "512": [267, 316, 317, 324], "52": [268, 339], "5210": 337, "53238": 309, "53386": 309, "536": 277, "544": 159, "562": 319, "5651": 315, "56758": 309, "5734": 274, "5798": 236, "58": 340, "580": 271, "5803": 236, "59": 271, "5994": 315, "5b": [315, 316], "5e": 337, "6": [9, 33, 39, 55, 250, 253, 257, 272, 278, 280, 282, 283, 284, 290, 294, 299, 303, 315, 318, 319, 320, 339], "60": [266, 339], "6006": 273, "602": 277, "61475": 309, "618": 277, "62": 275, "622321128845215": 339, "6236": 319, "6239": 319, "6251": 236, "6267": [272, 282, 283], "63": [278, 284, 286, 287, 292], "64": [266, 267, 269, 278, 337, 338], "6400": 212, "642": 267, "6431": 236, "6466": 319, "648": [266, 319], "65": [266, 268, 269, 319], "652": 266, "658": 267, "662": 267, "664": 271, "67": 267, "6778": 236, "68": 319, "684": 267, "6846": [274, 276, 281], "69": 319, "690": [268, 269], "6902": 236, "6969": 236, "6984167098999": 339, "6986": 315, "6b": [315, 316, 317, 336, 342], "7": [39, 61, 202, 236, 250, 257, 266, 278, 283, 294, 303, 315, 338, 339, 340], "70": [267, 271], "708": [268, 269], "7082": 236, "7089": 236, "70b": [150, 322], "71": [267, 319], "716": [267, 275], "7172": 236, "7224": 315, "7265": 236, "73": [267, 275], "74": [267, 275, 277], "742": 267, "74845": 309, "75": 268, "754": 251, "756": 268, "7590": 315, "76": 277, "764": [267, 319], "766": 267, "77445": 309, "7782": 339, "784": 182, "788": [266, 268, 269], "7895": 236, "79": 271, "7964": 320, "7b": [150, 254, 278, 283, 289, 302, 311, 313, 314, 315, 316, 318, 320, 322, 323, 339, 341, 342], "8": [39, 192, 202, 251, 257, 266, 268, 269, 272, 274, 278, 279, 280, 298, 300, 301, 303, 315, 317, 338, 339, 340], "80": 212, "802": 269, "806": 266, "8074": 315, "82": [266, 267, 275], "8255": 283, "83": 271, "83954": 309, "85": [266, 268, 269, 340], "854": 271, "85444": 309, "86": 319, "8602": 315, "87": [267, 340], "872": 319, "87ba8cb8a6a4f6525f26255fa513d902b17ab060": 309, "88": [267, 271, 319], "881": 319, "883856773376465": 339, "8859": 272, "8861": 236, "89": [267, 319], "891325950622559": 339, "8945": 315, "8952": 320, "8958": 315, "8b": [150, 307, 315, 322, 336, 339, 340, 342], "8bit": 283, "8x7b": [315, 316, 342], "9": [236, 250, 251, 315, 337, 340], "90": [267, 319], "90b": [310, 311], "91": [267, 275], "9124": 236, "92": 277, "920": 267, "9210": 282, "922": 267, "9228": 282, "9274": 315, "93": 277, "9315": 236, "938": 277, "94": [271, 279, 280], "9400": 236, "948962688446045": 339, "95": [279, 280, 319, 340], "9560": 315, "96": [257, 271], "9685": 236, "97": [267, 275], "9897": 236, "99": [3, 257, 299], "996": 267, "999": [3, 257, 299], "9994": 320, "9b": 315, "9e": 5, "A": [5, 6, 8, 13, 14, 15, 16, 17, 21, 32, 33, 39, 50, 53, 54, 55, 57, 71, 76, 107, 110, 111, 112, 118, 151, 181, 182, 183, 194, 199, 213, 235, 236, 241, 251, 252, 257, 265, 279, 280, 291, 303, 308, 309, 311, 338, 339], "And": [249, 284, 340], "As": [9, 124, 177, 233, 243, 279, 280, 298, 303, 307, 308, 320, 337, 339], "At": 298, "Be": 303, "But": [99, 316, 326, 336, 339], "By": [3, 7, 32, 61, 251, 252, 257, 259, 285, 295, 296, 298, 299, 301, 302, 323, 326], "For": [9, 39, 53, 118, 150, 212, 217, 236, 240, 241, 242, 249, 251, 252, 253, 254, 255, 257, 258, 259, 262, 263, 266, 267, 272, 276, 278, 279, 280, 282, 283, 284, 285, 286, 287, 290, 291, 292, 293, 298, 301, 302, 303, 304, 305, 308, 309, 310, 315, 316, 317, 318, 319, 320, 323, 324, 325, 326, 332, 333, 335, 336, 337, 339, 340, 341], "If": [3, 6, 7, 32, 39, 53, 55, 56, 61, 95, 118, 140, 150, 177, 183, 192, 198, 199, 236, 241, 244, 250, 252, 253, 254, 257, 259, 265, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 284, 285, 291, 295, 296, 297, 298, 299, 302, 303, 304, 305, 307, 316, 320, 336, 337, 339, 341], "In": [22, 39, 55, 150, 212, 243, 249, 250, 251, 252, 254, 257, 258, 259, 270, 279, 280, 284, 285, 294, 295, 296, 297, 298, 300, 301, 302, 303, 304, 307, 316, 318, 319, 320, 323, 325, 326, 331, 333, 335, 336, 337, 338, 339, 341], "It": [3, 6, 7, 32, 33, 39, 56, 61, 82, 85, 95, 99, 150, 177, 183, 186, 192, 193, 198, 205, 213, 215, 217, 252, 254, 255, 257, 259, 260, 267, 274, 278, 279, 280, 281, 284, 285, 286, 287, 291, 292, 295, 299, 336, 337, 340, 342], "Its": 257, "No": [260, 291], "Not": [31, 32, 33, 50], "Of": 249, "On": [178, 315], "One": [3, 303, 320, 337], "Such": [159, 303], "THE": [289, 330], "That": 9, "The": [1, 5, 6, 7, 27, 32, 33, 39, 50, 53, 54, 55, 61, 71, 76, 82, 85, 95, 107, 108, 110, 118, 119, 147, 150, 151, 154, 177, 180, 182, 183, 192, 193, 198, 199, 202, 205, 213, 215, 217, 236, 241, 243, 244, 248, 249, 250, 251, 252, 253, 254, 255, 257, 259, 260, 261, 262, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 286, 287, 289, 291, 292, 293, 294, 297, 298, 299, 300, 301, 302, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 319, 320, 322, 324, 330, 332, 333, 336, 337, 338, 339, 341, 342], "Then": [6, 7, 9, 192, 266, 267, 268, 269, 270, 271, 275, 277, 320, 338, 339], "There": [1, 6, 7, 192, 241, 251, 267, 280, 284, 294, 299, 320, 337, 341], "These": [151, 250, 251, 252, 297, 299, 301, 307], "To": [6, 7, 55, 93, 149, 192, 213, 241, 243, 250, 251, 260, 263, 266, 267, 268, 269, 271, 275, 277, 294, 296, 299, 301, 303, 309, 310, 315, 316, 317, 320, 324, 333, 339], "Will": [61, 301], "With": 303, "_": [265, 291, 307, 319, 325], "_2": 303, "_3sd": 202, "__getitem__": [149, 265], "__getitems__": 149, "__init__": [9, 32, 124, 177, 233, 243, 258, 260, 265, 337], "__len__": [149, 265], "__name__": 316, "__quantize_input": 39, "_avgpool_globalaveragepool_output_0_float": 291, "_conv1_conv_output_0_dequantizelinear_output": 291, "_data_load": [139, 143], "_dim": 236, "_export": 319, "_fix_neuron_v2_devic": 242, "_float": 291, "_ll_j": 202, "_load_calibration_data": 258, "_native_batch_norm_legit": 228, "_native_batch_norm_legit_no_train": 228, "_pass": 213, "_preprocess_imag": 260, "_quant": 320, "_size_2_t": [234, 235], "_val": 320, "_weight": 236, "a16w8": [102, 286, 287, 292, 300], "a18w8": 293, "a1h_in1k": 271, "a2": [316, 342], "a2q": 181, "a8w8": [102, 286, 287, 292, 300, 307], "a8w8_cpu": 77, "a8w8_npu": 77, "ab": [55, 278, 337], "abbrevi": [267, 314, 316], "abc": [121, 122, 127, 128, 131, 135, 136, 139, 143, 156, 178, 194, 199, 206, 209, 222, 241], "abil": 310, "about": [186, 205, 236, 251, 263, 320, 333, 341], "abov": [9, 39, 118, 124, 177, 199, 233, 243, 257, 302, 310, 316, 319, 320, 322, 335, 336, 338], "absmax": 55, "absolut": [261, 302, 306, 307], "abstract": [3, 33, 34, 149, 213], "acc1_freez": 319, "acc1_quant": 319, "acceler": [119, 249, 252, 257, 259, 266, 294, 295, 296, 297, 301, 340, 342], "accept": [1, 6, 7, 149, 177, 192, 193, 198, 241, 243, 257, 279, 280, 284, 305], "access": [205, 252, 253, 254, 255, 266, 267, 268, 269, 271, 272, 275, 276, 277, 278, 282, 283, 302, 308, 315, 316, 317, 318, 319, 324, 339], "accommod": 251, "accompani": 298, "accord": [1, 7, 55, 150, 236, 257, 270, 284, 296, 302, 320], "accordingli": [193, 198, 257, 308, 338], "account": 303, "accraci": 18, "accumul": 181, "accumulator_bit_width": 181, "accur": [181, 199, 249, 257, 263, 296, 333, 337], "accuraci": [3, 61, 98, 183, 199, 249, 251, 252, 253, 254, 255, 257, 261, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 284, 285, 286, 287, 292, 298, 299, 300, 301, 302, 306, 319, 320, 336, 339, 342], "accuracy_improv": [252, 253, 254, 255, 266, 267], "accuracy_level": 66, "accuracylevel": 257, "achiev": [228, 249, 251, 252, 259, 263, 267, 278, 287, 291, 295, 298, 299, 300, 301, 303, 308, 333], "across": [57, 151, 183, 194, 199, 202, 248, 251, 252, 257, 286, 287, 292, 336, 338], "act": [9, 25, 159, 215, 287], "act_ord": 181, "act_quant": 199, "activ": [3, 9, 26, 39, 54, 55, 57, 61, 76, 101, 111, 140, 177, 181, 183, 199, 205, 215, 251, 254, 255, 257, 258, 263, 278, 279, 280, 285, 286, 287, 291, 292, 293, 294, 297, 298, 300, 301, 307, 308, 309, 316, 319, 320, 324, 333, 336, 339, 340, 341, 342], "activatedropoutnod": 209, "activation_lay": 318, "activation_qtyp": [4, 39, 54, 63], "activation_stats_hook": 205, "activation_tensor_nam": 298, "activation_tensor_name1": 298, "activation_tensor_name2": 298, "activation_typ": [3, 36, 55, 57, 252, 253, 254, 255, 257, 259, 279, 280, 284, 285, 294, 295, 296, 297, 298, 299, 300, 301], "activationequ": 181, "activationsc": [257, 294], "activationsymmetr": [61, 252, 253, 254, 255, 257, 279, 280, 284, 285, 299, 301], "actord": 257, "acttargetquanttyp": [257, 298], "actual": [203, 257, 258, 261, 263, 306, 320, 333], "ad": [177, 257, 299, 301, 308, 323, 337, 339, 342], "ada": 7, "adagrad": 236, "adapt": [26, 27, 193, 198, 252, 253, 257, 259, 286, 287, 292, 295, 298], "adaptive_avg_pool2d": 217, "adaptiveavgpool2d": 217, "adaqu": [21, 22, 27, 249, 257, 258, 259, 286, 287, 289, 292, 294, 295, 296, 297, 299, 300, 342], "adaround": [7, 21, 22, 26, 27, 249, 257, 258, 259, 284, 285, 286, 287, 289, 292, 294, 299, 300, 342], "adaroundconst": 7, "adaroundintquant": 7, "add": [54, 55, 108, 138, 140, 150, 199, 213, 250, 257, 299, 301, 315, 316, 319, 324], "add_check": 213, "add_export_info_for_hf": 150, "add_pass": 213, "add_pre_optimization_config": 199, "add_qdq_pair_to_weight": 54, "add_qk_rotation_after_function_call_in_forward": 140, "add_wrapper_after_function_call_in_method": 138, "addit": [3, 22, 32, 39, 54, 55, 57, 236, 257, 272, 278, 282, 283, 291, 301, 309, 310, 315, 316, 323], "addition": [151, 251, 257, 259, 295, 296, 297, 317], "addqdqpairtoweight": 257, "addqdqtoqoptransform": 50, "address": [257, 294, 296, 319, 336], "adeptli": 251, "adequ": 265, "adher": 251, "adjust": [39, 72, 199, 251, 252, 257, 261, 267, 279, 280, 285, 293, 296, 297, 299, 306, 309, 319], "adjust_bias_scal": 72, "adjust_hard_sigmoid": 72, "adjust_quantize_info": 72, "adjust_shift_bia": 72, "adjust_shift_cut": 72, "adjust_shift_read": 72, "adjust_shift_swish": 72, "adjust_shift_writ": 72, "adjustbiasscal": 257, "adjusthardsigmoid": 257, "adjustshiftbia": 257, "adjustshiftcut": 257, "adjustshiftread": 257, "adjustshiftswish": 257, "adjustshiftwrit": 257, "adopt": 342, "advanc": [249, 251, 252, 258, 270, 276, 278, 294, 295, 296, 297, 298, 299, 300, 304, 308, 318, 320, 341, 342], "advantag": [251, 279, 280], "affect": [138, 140, 251], "afford": [303, 337], "after": [20, 31, 53, 138, 140, 183, 194, 199, 213, 215, 226, 251, 252, 254, 256, 257, 259, 260, 278, 279, 280, 285, 289, 291, 293, 295, 296, 297, 298, 299, 307, 315, 316, 319, 320, 322, 323, 326, 330, 336, 337, 339], "again": [55, 331], "against": 55, "aggreg": 236, "aggress": [199, 259, 295, 298], "agnost": 320, "ahead": [263, 333, 337], "ai": [39, 61, 150, 258, 297, 298, 316, 338, 342], "aid": 199, "ailab": 140, "aim": [150, 193, 198, 252, 259, 286, 287, 292, 295, 296, 297, 299, 303, 308, 320, 341], "akanametov": 287, "al": [26, 27, 181, 202], "algo": 184, "algo_config": [66, 128, 183, 194, 199, 341], "algoconfig": [194, 199], "algoconfigbas": [194, 199], "algorithm": [3, 22, 26, 27, 181, 183, 194, 199, 202, 249, 251, 252, 254, 255, 257, 258, 261, 270, 274, 278, 279, 280, 281, 282, 285, 286, 287, 292, 294, 299, 300, 304, 316, 318, 320, 324, 339, 341, 342], "algorithm_config": 341, "align": [72, 217, 257, 294, 320, 342], "align_concat": 72, "align_pad": 72, "align_pool": 72, "align_quantize_info": 72, "align_reshap": 72, "align_slic": 72, "align_transpos": 72, "alignconcat": 257, "aligneltwisequanttyp": 257, "alignpad": 257, "alignpool": 257, "alignreshap": 257, "alignslic": 257, "aligntranspos": 257, "all": [1, 3, 6, 7, 9, 25, 31, 56, 61, 82, 85, 95, 99, 118, 119, 124, 149, 177, 192, 193, 198, 205, 217, 233, 236, 243, 250, 252, 257, 270, 276, 278, 284, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 307, 308, 310, 311, 316, 319, 320, 337, 339, 340, 341, 342], "all_config": [279, 280, 284], "all_tensors_to_one_fil": [282, 283, 291], "alloc": [251, 299, 303], "allow": [9, 33, 57, 118, 119, 124, 151, 177, 183, 194, 199, 228, 233, 243, 251, 257, 259, 263, 264, 267, 286, 287, 291, 292, 295, 296, 297, 298, 302, 303, 312, 318, 333, 334], "allow_exported_model_train_ev": 228, "allow_mulit_consum": 33, "allow_multi_consum": 33, "almost": 302, "alon": 337, "along": [302, 309, 320], "alpha": [7, 26, 55, 76, 181, 199, 336, 337, 340, 341], "alpha_valu": 55, "alreadi": [61, 82, 85, 99, 193, 198, 266, 267, 268, 269, 271, 275, 277, 291, 293, 320], "also": [6, 7, 9, 32, 33, 50, 61, 124, 149, 156, 177, 183, 192, 199, 233, 236, 243, 251, 252, 254, 255, 257, 259, 266, 267, 274, 278, 281, 291, 298, 299, 300, 301, 302, 303, 310, 319, 320, 321], "altern": [251, 257, 291, 311, 313, 314], "alwai": [22, 32, 61, 251, 257, 297, 302, 337, 338], "amax": 241, "amd": [217, 248, 250, 253, 254, 255, 256, 257, 261, 262, 263, 265, 270, 279, 280, 285, 291, 293, 300, 303, 305, 306, 327, 330, 332, 333, 335, 336, 337, 342], "amd_quark": [250, 289, 319, 330], "among": [3, 53], "amount": [285, 303], "amplifi": 294, "an": [3, 6, 7, 9, 22, 32, 39, 53, 54, 55, 56, 57, 61, 99, 104, 107, 108, 110, 118, 121, 122, 124, 127, 128, 131, 135, 136, 139, 140, 143, 149, 150, 156, 177, 178, 182, 192, 193, 194, 198, 199, 205, 206, 209, 213, 217, 222, 228, 233, 236, 241, 243, 249, 251, 252, 255, 257, 258, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 282, 283, 284, 285, 287, 289, 290, 291, 293, 295, 296, 297, 298, 299, 302, 303, 304, 305, 308, 314, 315, 316, 317, 320, 323, 324, 336, 337, 338, 339, 342], "analysi": 257, "analyz": [307, 340], "ani": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 54, 55, 57, 61, 63, 66, 71, 72, 74, 78, 86, 91, 106, 107, 108, 114, 119, 121, 122, 136, 138, 139, 140, 143, 147, 150, 169, 177, 180, 183, 186, 192, 199, 203, 205, 213, 236, 237, 241, 243, 251, 279, 280, 284, 298, 302, 320, 337, 340], "annot": [55, 226, 319], "annotate_tensor": 55, "anoth": [236, 255, 303, 308, 320], "apart": [1, 298], "api": [57, 60, 133, 151, 160, 181, 183, 184, 194, 195, 199, 229, 243, 250, 258, 263, 291, 299, 301, 304, 308, 319, 320, 324, 333, 335], "apl": [249, 327, 330, 342], "appar": 251, "appear": 251, "append": [118, 337], "appli": [6, 7, 31, 32, 33, 34, 36, 39, 50, 51, 53, 55, 57, 99, 140, 141, 144, 150, 177, 181, 183, 192, 199, 202, 250, 251, 252, 257, 258, 259, 260, 294, 295, 296, 297, 298, 299, 300, 301, 303, 308, 337, 338, 339], "applic": [118, 202, 251, 252, 279, 280, 293, 298, 299, 308, 316], "approach": [193, 198, 248, 251, 252, 263, 265, 267, 296, 297, 298, 299, 303, 333, 336, 337], "appropri": [118, 298, 317, 320, 336], "approxim": [55, 228, 251, 257, 336], "ar": [3, 6, 7, 9, 32, 33, 39, 50, 55, 61, 99, 110, 118, 138, 140, 150, 156, 159, 177, 186, 192, 193, 198, 199, 202, 205, 217, 228, 236, 238, 241, 243, 244, 250, 252, 253, 254, 255, 257, 263, 266, 267, 276, 278, 280, 282, 284, 289, 290, 291, 292, 293, 294, 296, 297, 298, 299, 300, 301, 304, 307, 308, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 323, 325, 326, 330, 332, 333, 335, 337, 338, 339, 340, 341, 342], "arbitrari": [6, 7, 118, 192, 236, 302], "arbitrarili": 298, "architectur": [150, 335, 337, 339, 342], "area": 294, "aren": 183, "arg": [1, 5, 6, 7, 32, 33, 34, 39, 50, 51, 53, 54, 55, 56, 61, 71, 76, 119, 150, 154, 177, 182, 192, 193, 198, 205, 213, 215, 217, 218, 219, 220, 221, 233, 236, 244, 280, 284, 311, 314, 319, 323, 325, 326, 335], "argmax": [265, 301], "argument": [6, 7, 32, 118, 150, 154, 177, 192, 236, 241, 257, 291, 307, 315, 316], "aris": 290, "arithmet": 251, "around": [140, 199, 257, 299, 309], "arrai": [1, 22, 25, 99, 114, 296, 297], "art": [249, 320], "artifici": 296, "arxiv": [26, 27, 202, 235, 278], "as_text": 114, "asctim": 118, "assembl": 1, "assembleidx": 1, "assert": 32, "assess": 299, "assign": [9, 124, 150, 177, 233, 243, 259, 280, 284, 295, 297, 298, 299, 319], "assist": 251, "associ": [39, 55, 252, 341], "assum": [56, 193, 198, 310], "astyp": 265, "asym": [311, 313, 314], "asymetr": 150, "asymmetr": [39, 183, 249, 251, 257, 258, 262, 264, 300, 304, 320, 322, 324, 332, 334, 336, 342], "aten": [211, 217, 218, 219, 220, 221, 228], "attach": [205, 225, 226, 235], "attempt": 338, "attent": [199, 339], "attr": [6, 7, 11, 192, 236], "attr_path": 147, "attribut": [6, 7, 9, 11, 32, 54, 55, 57, 118, 124, 147, 159, 177, 192, 213, 217, 233, 236, 243, 257, 296, 297, 298, 341], "attributeproto": 11, "augment": [3, 199], "augment_graph": 3, "augmented_model": 3, "augmented_model_path": 3, "author": 32, "auto": [257, 270, 299, 342], "auto_merg": 291, "auto_search": 299, "auto_search_config": [279, 280, 284, 299], "auto_search_conig": [280, 284], "auto_search_in": [279, 280, 284], "auto_search_inst": 299, "auto_search_model": 270, "autoawq": [150, 323, 342], "autofp8": [150, 342], "autograd": [6, 7, 177, 192], "automat": [6, 7, 32, 36, 57, 192, 249, 279, 280, 291, 336, 340, 342], "automixprecis": [257, 298], "automixusefastft": 257, "automodelforcausallm": [198, 302, 304, 320, 333, 338], "autosearch": 299, "autosearchconfig": 299, "autosearchconfig_default": [280, 284], "autosmoothqu": [199, 336], "autosmoothquantconfig": 199, "autosmoothquantprocessor": 121, "autotoken": [198, 302, 304, 316, 320, 333, 338], "avail": [118, 150, 198, 250, 251, 252, 253, 254, 255, 257, 260, 266, 267, 276, 282, 295, 296, 297, 299, 308, 314, 315, 316, 317, 318, 319, 341, 342], "averag": [3, 20, 177, 217, 236, 252, 254, 257, 261, 306], "average_l2": 20, "averagepool": 301, "averaging_const": 3, "avgpool": 291, "avoid": [55, 93, 177, 181, 257, 279, 280, 300, 315, 316, 341], "awai": [251, 257], "awar": [181, 199, 235, 249, 251, 252, 317, 319, 336, 342], "awq": [150, 199, 249, 304, 311, 313, 314, 320, 323, 336, 341, 342], "awq_auto_config_help": 316, "awq_config": 316, "awqconfig": [199, 341], "awqprocessor": [122, 341], "axi": [39, 54, 192, 199, 257, 262, 265, 296, 297, 302, 332, 341], "b": [33, 39, 55, 61, 118, 236, 242, 257, 279, 280, 303, 316], "b_beta": 13, "back": 251, "backbon": [199, 339], "backend": [181, 183, 249, 263, 298, 320, 333, 342], "backpropag": 319, "backward": [6, 7, 192, 236, 308], "bad": [279, 280], "bag": 236, "balanc": [249, 251, 259, 279, 280, 286, 287, 292, 295, 296, 298, 303, 307, 336, 337, 342], "bandwidth": [259, 295], "bar": [205, 307], "barplot": 205, "base": [3, 6, 7, 9, 22, 32, 33, 34, 39, 50, 53, 54, 55, 56, 110, 118, 124, 138, 147, 159, 177, 182, 192, 193, 198, 202, 213, 217, 225, 226, 230, 233, 243, 249, 251, 252, 257, 261, 270, 274, 276, 278, 279, 280, 281, 284, 285, 291, 293, 299, 301, 303, 306, 307, 309, 318, 320, 326, 328, 336, 337], "base_input": 1, "base_path": 159, "basealgoprocessor": 136, "baselin": 1, "basevocab": 159, "batch": [53, 149, 212, 236, 252, 257, 265, 266, 267, 268, 269, 270, 271, 275, 277, 291, 307], "batch_idx": 265, "batch_it": 325, "batch_norm": 228, "batch_siz": [27, 260, 265, 291, 305, 311, 313, 314, 319, 337], "batchfeatur": [198, 205], "batchnorm": [33, 53, 228, 235, 257, 301], "batchnorm2d": [228, 235], "batchnorm_op": 228, "batchsiz": [252, 257, 259, 285, 295, 296, 297], "bb": 118, "bdcb8f42221bc40c411150a009a3d3a30fa74722": 320, "becaus": [1, 39, 251, 257, 291, 293, 300, 320, 331, 341], "becom": [199, 251, 254, 278, 298], "been": [33, 34, 250, 252, 257, 304, 338], "befor": [9, 53, 101, 118, 124, 140, 177, 183, 213, 215, 217, 226, 233, 236, 243, 250, 254, 257, 266, 278, 279, 280, 291, 293, 301, 307, 315, 317, 318, 320, 337, 339, 341], "begin": [308, 320], "behalf": 251, "behav": 13, "behavior": [6, 61, 228, 252, 257, 324], "behind": 303, "being": [177, 205, 250, 251, 257, 299, 307, 308, 320], "below": [39, 55, 118, 249, 257, 279, 280, 284, 285, 287, 303, 310, 311, 312, 313, 314, 316, 320, 323, 326, 336, 337], "benchmark": 342, "benefici": [251, 302], "benefit": [249, 337], "besid": [252, 257, 300, 301, 340], "best": [6, 7, 192, 199, 263, 289, 291, 299, 333], "beta": [39, 54], "beta_rang": 27, "better": [3, 34, 140, 183, 217, 249, 251, 252, 257, 278, 286, 287, 291, 292, 294, 299, 301, 302, 336, 337, 339, 342], "between": [1, 3, 20, 98, 107, 140, 183, 193, 198, 205, 228, 241, 249, 251, 257, 259, 261, 279, 280, 284, 294, 295, 296, 306, 307, 310, 312, 320, 337], "between_op": 107, "bewar": 337, "bf16": [257, 286, 287, 291, 292, 293, 298, 300, 311, 313, 314, 324], "bf16qdqtocast": 257, "bf16withclip": 257, "bfloat16": [7, 80, 83, 101, 104, 109, 202, 241, 249, 251, 257, 258, 286, 287, 292, 294, 298, 300, 303, 304, 324, 325, 336, 341, 342], "bfloat16_spec": 341, "bfloat16spec": 199, "bfloat_16_onnx_model_path": [80, 83, 293], "bfloat_format": 293, "bfp": [54, 257, 259, 267, 286, 287, 289, 291, 292, 296, 297, 338, 342], "bfp16": [81, 84, 249, 257, 286, 287, 292, 298, 300, 318, 342], "bfp16_adaqu": 266, "bfp16spec": 199, "bfp_16_onnx_model_path": [81, 84, 293], "bfp_method": [257, 296, 301], "bfpattribut": [257, 296, 301], "bfpfixneuron": [6, 257, 259, 295, 296, 298, 301, 342], "bfpprimequantdequantfunct": 6, "bfpquantdequantfunct": 6, "bfpquantiz": 6, "bia": [8, 9, 13, 14, 15, 16, 17, 22, 39, 54, 140, 150, 177, 178, 181, 183, 199, 205, 218, 219, 220, 221, 228, 234, 235, 237, 252, 253, 257, 258, 319, 335, 337, 341, 342], "bias": [9, 183, 251, 319], "bias_nam": [39, 54], "bias_scal": 54, "bias_to_quant": 54, "bias_zero_point": 54, "biascorrect": [181, 249, 342], "biastargetquanttyp": 257, "bigger": [22, 252, 257], "bin": [3, 199, 205, 241, 273, 274, 276, 281, 291, 303, 337], "bin_edg": 241, "binari": [82, 85, 99, 320], "bit": [61, 66, 178, 183, 202, 251, 257, 259, 274, 279, 280, 286, 287, 292, 294, 295, 296, 297, 298, 299, 300, 303, 308, 316, 336, 338, 339], "bit_width": [6, 183, 257, 296, 301], "bitwidth": 303, "blob": [228, 284, 286, 287, 292], "block": [54, 199, 202, 249, 257, 267, 286, 287, 289, 292, 295, 296, 297, 298, 301, 303, 305, 316, 320, 332, 336, 339, 342], "block_q4_1": 320, "block_recon": 27, "block_siz": [6, 66, 192, 257, 272, 273, 274, 276, 278, 281, 282, 283, 296, 297, 301], "blocksiz": 257, "blockwisetuningconfig": [128, 194], "blockwisetuningprocessor": 128, "blow": 105, "bn": [33, 217, 228], "bool": [3, 4, 5, 7, 9, 13, 27, 33, 39, 53, 54, 55, 57, 61, 63, 66, 71, 72, 74, 76, 99, 114, 118, 124, 141, 144, 150, 156, 177, 178, 180, 181, 198, 199, 219, 228, 233, 234, 235, 236, 237, 238, 241, 243, 257, 291, 341], "boolean": [6, 7, 9, 124, 177, 192, 199, 233, 243, 252, 253, 254, 255, 257, 291], "boost": 285, "both": [33, 39, 72, 140, 150, 177, 226, 241, 243, 248, 249, 257, 262, 263, 266, 285, 286, 287, 291, 292, 296, 298, 301, 303, 308, 310, 314, 318, 319, 320, 325, 332, 333, 335, 337, 339, 342], "boundari": [257, 298], "boundless": 291, "bpevocab": 159, "brain": 294, "break": [33, 203, 304], "brecq": [249, 318, 342], "brevita": [249, 327, 330, 337, 342], "brevitasquant": 308, "broaden": 308, "broadli": 251, "broken": 302, "buffer": [177, 243], "buffer_nam": 243, "bug": [32, 33, 50], "build": [1, 150, 177, 267, 279, 280, 284, 298, 299, 320, 331], "build_all_config": [279, 280, 284], "builder": [310, 312], "buildin": [1, 270], "buildin_eval_func": 1, "built": [267, 279, 280, 299], "bunch": [302, 320], "byte": 251, "c": [33, 118, 159, 217, 250, 266, 267, 268, 269, 270, 271, 275, 276, 277, 278, 279, 280, 294, 295, 296, 297, 298, 299, 301], "c4ai": [315, 316, 342], "cach": [20, 22, 55, 199, 249, 280, 284, 304, 324, 331, 336, 339, 342], "cacheddataread": [20, 55], "cacheddataset": 22, "calc_recon_loss": 26, "calc_round_loss": 26, "calcuat": [280, 284], "calcul": [1, 3, 20, 26, 39, 55, 199, 241, 251, 253, 257, 270, 279, 280, 299, 310, 319, 320, 338], "calculate_qparam": [241, 243], "calculate_quantization_param": 39, "calib": 260, "calib_": 265, "calib_000001": [260, 265], "calib_000002": [260, 265], "calib_000003": [260, 265], "calib_000004": [260, 265], "calib_000005": [260, 265], "calib_bin_edg": 241, "calib_data": [136, 266, 267, 268, 269, 271, 275, 277, 284, 286, 292], "calib_data_fold": 258, "calib_data_path": [111, 260, 279, 280, 284, 286, 287, 292], "calib_data_read": 258, "calib_dataload": [182, 198, 302, 304, 305, 320, 325, 333, 337, 338], "calib_dataread": 260, "calib_hist": 241, "calib_imag": 287, "calib_length": 319, "calib_load": [182, 319], "calib_prompt": 309, "calib_s": 309, "calibdataread": 258, "calibdatas": 257, "calibmovingaverag": [254, 257, 279, 280, 299], "calibmovingaverageconst": [257, 279, 280, 299], "calibr": [22, 27, 39, 54, 55, 56, 57, 61, 76, 182, 193, 198, 226, 249, 251, 254, 257, 258, 259, 263, 265, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 288, 293, 294, 295, 296, 297, 298, 299, 304, 318, 319, 320, 333, 337, 338, 342], "calibrate_method": [3, 4, 39, 54, 55, 57, 63, 252, 253, 254, 255, 257, 259, 279, 280, 284, 285, 294, 295, 296, 297, 298, 299, 300, 301], "calibration_cache_dir": 265, "calibration_data": [258, 260, 265], "calibration_data_path": [55, 56, 260], "calibration_data_read": [55, 56, 252, 253, 254, 255, 257, 260, 285, 299, 300, 301], "calibration_dataset_path": [266, 267, 268, 269, 271, 275, 277, 279, 280], "calibration_image_fold": 260, "calibration_method": 257, "calibrationdataread": [3, 22, 55, 56, 111, 258, 260, 291], "calibrationmethod": [3, 55, 57, 254, 257, 259, 279, 280, 284, 290, 294, 295, 296, 297, 298, 299, 301], "calibtensorrangesymmetr": 257, "call": [3, 6, 7, 9, 22, 32, 39, 54, 118, 124, 138, 140, 177, 178, 192, 213, 215, 217, 228, 233, 236, 243, 250, 251, 284, 308, 318, 320, 337, 338, 339], "call_modul": 225, "callabl": [74, 138, 140, 213], "can": [1, 3, 4, 6, 7, 9, 32, 33, 39, 55, 63, 99, 118, 124, 140, 151, 159, 177, 182, 183, 192, 193, 198, 199, 205, 212, 213, 215, 217, 233, 236, 241, 242, 243, 250, 251, 252, 253, 254, 255, 257, 259, 264, 265, 266, 267, 268, 269, 271, 272, 275, 277, 278, 279, 280, 282, 283, 284, 285, 286, 287, 289, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 305, 307, 308, 309, 310, 314, 315, 316, 317, 319, 320, 323, 326, 330, 334, 336, 337, 339, 341, 342], "candid": 299, "candidate_lay": 34, "candidate_nod": [31, 51], "canni": 342, "cannot": [99, 150, 236, 257, 291, 325, 331, 339], "capabl": [249, 262, 298, 307, 308, 332, 342], "capi": 290, "caption": 309, "captions_sourc": 309, "capture_pre_autograd_graph": 319, "card": 316, "care": [33, 337], "carefulli": [298, 317], "carri": 118, "case": [32, 39, 61, 180, 212, 236, 251, 252, 253, 254, 255, 257, 298, 303, 304, 337, 339], "cast": [82, 85, 104, 109, 257, 293, 301], "cast_cast": 325, "cat": 228, "cat_op": 228, "categor": 251, "categori": 251, "cauchy_": 337, "caus": [252, 279, 280, 290, 291, 294], "caution": 308, "cd": [309, 336], "ceil": 242, "center": 299, "central": [3, 257], "certain": [118, 251, 252, 257, 291, 298, 299, 300], "ch_axi": [7, 192, 198, 199, 262, 302, 320, 332, 333, 337, 338, 341], "chain": 236, "challeng": 336, "chang": [32, 99, 199, 212, 217, 228, 236, 250, 301, 308, 311, 313, 314], "channel": [39, 54, 57, 61, 71, 118, 140, 144, 183, 199, 202, 249, 251, 257, 258, 262, 291, 304, 332, 336, 340, 341, 342], "channel_axi": 39, "channel_splitting_ratio": 181, "channel_splitting_split_input": 181, "chapter": [263, 333], "characterist": [251, 252, 298], "chart": 205, "chat": [311, 313, 314, 316, 342], "chatglm": [336, 342], "chatglm3": [315, 316, 317], "check": [9, 39, 55, 159, 213, 215, 217, 238, 250, 266, 267, 268, 269, 271, 275, 277, 299, 319, 331, 337], "check_hard_sigmoid_condit": 55, "check_min_max_valid": 238, "check_model_quantiz": 55, "check_onnx_model": 55, "check_reduce_mean_condit": 55, "check_relu_like_nod": 55, "checker": 159, "checkout": 309, "checkpoint": [272, 278, 282, 283, 311, 313, 314, 315, 316, 317, 323, 324, 326], "child": [9, 124, 177, 233, 243], "choic": [150, 198, 252, 259, 295, 311, 339], "choos": [249, 250, 251, 267, 302, 303, 319, 320, 326, 336], "chosen": [20, 302, 303], "circumst": 39, "citi": 309, "ckpt_path": 316, "cl": [250, 301], "clamp": 320, "class": [258, 265, 266, 267, 268, 269, 271, 275, 277, 280, 284, 300, 305, 308, 337, 338, 341], "classic": [257, 296, 303], "classif": [249, 267, 275, 284, 286, 292, 298, 342], "classifi": 251, "classmethod": [25, 26, 177], "cle": [57, 249, 258, 270, 286, 287, 289, 292, 342], "cle_balance_method": 5, "cle_pair_typ": 5, "cle_scale_append_bia": 5, "cle_scale_use_threshold": 5, "cle_step": 5, "cle_total_layer_diff_threshold": 5, "cle_transform": 5, "cle_weight_threshold": 5, "clean": [280, 284, 342], "clescaleappendbia": [253, 257], "clestep": [253, 257], "cletotallayerdiffthreshold": 257, "cli": [272, 273, 274, 276, 281, 282, 283], "click": [250, 301, 310], "clip": [9, 39, 53, 55, 101, 215, 242, 253, 257, 301, 309], "clip_max": 215, "clip_min": 215, "clip_nod": 55, "clockwis": 339, "clone": [236, 309], "close": [252, 261, 279, 280, 306], "closer": [252, 339], "closest": [55, 257], "cloud": 320, "cmd": [250, 301], "cnn": [54, 57, 249, 257, 290, 300, 342], "cnn_dailymail": 314, "cnn_dm": 314, "cnv": 95, "co": [266, 268, 269, 270, 271, 273, 274, 275, 276, 277, 279, 280, 281, 284], "coars": [296, 297], "coco": [270, 309], "coco2014": 309, "coco2017": 270, "code": [32, 140, 217, 250, 259, 284, 286, 287, 289, 290, 291, 292, 295, 296, 297, 299, 304, 308, 316, 320, 322, 330, 336, 340], "coher": 316, "cohereforai": [315, 316, 342], "colbert": 181, "collaps": 251, "collat": 316, "collate_fn": 305, "collect": [1, 3, 205, 213, 241, 243, 307, 337], "collect_data": 3, "collect_quantization_statist": 205, "color": 303, "column": [257, 303, 336], "com": [138, 267, 275, 284, 286, 287, 292, 309], "combin": [1, 6, 7, 32, 33, 50, 192, 235, 257, 259, 279, 280, 284, 295, 318, 324, 339, 340], "come": 251, "comma": 314, "command": [250, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 287, 291, 301, 315, 316, 320, 324, 336, 342], "comment": [257, 301], "commit": 320, "common": [33, 257, 259, 266, 286, 287, 292, 295, 296, 297, 298, 303, 320, 337, 340], "common_typ": [234, 235], "commonli": [266, 267, 268, 269, 270, 271, 275, 277], "compar": [55, 252, 257, 259, 291, 293, 295, 297, 303, 307, 336, 337], "comparison": [267, 312], "compat": [6, 7, 56, 150, 192, 193, 198, 250, 252, 257, 259, 295, 305, 308, 309, 316, 340, 342], "compil": [72, 95, 250, 257, 298, 301, 307, 316, 340, 342], "complementari": 241, "complet": [150, 299, 304], "complex": [193, 198, 251, 257, 259, 266, 279, 280, 286, 287, 292, 295, 298], "compli": 308, "compon": [57, 183, 194, 199, 279, 280, 308], "compos": 177, "composit": 251, "comprehens": [57, 151, 183, 194, 199, 249, 310], "compress": [150, 198, 251, 307], "compromis": [249, 251, 337], "comput": [3, 6, 7, 8, 25, 39, 54, 55, 93, 118, 183, 192, 205, 236, 243, 250, 251, 252, 254, 257, 259, 261, 266, 267, 286, 287, 291, 292, 294, 295, 296, 297, 298, 302, 303, 306, 312, 320], "computation": 298, "compute_collection_result": 3, "compute_rang": 3, "compute_scale_loss": 199, "compute_scale_zp": 55, "compute_scale_zp_fp": 55, "concat": [33, 53, 257, 301], "concat_5": 287, "concaten": [53, 236, 284], "concentr": [254, 278], "concept": [251, 267, 296], "conclud": 299, "concret": [243, 267, 279, 280], "conda": 310, "condit": [55, 215, 270, 279, 280, 284, 299], "conduct": [253, 257, 319], "conf": 1, "config": [1, 4, 33, 34, 51, 56, 63, 127, 128, 131, 135, 139, 143, 150, 169, 177, 178, 182, 184, 186, 193, 195, 198, 230, 234, 235, 236, 237, 240, 241, 242, 243, 252, 253, 254, 255, 257, 258, 259, 260, 266, 267, 268, 269, 270, 272, 273, 274, 276, 277, 278, 281, 282, 283, 284, 285, 286, 287, 292, 294, 295, 296, 297, 298, 299, 300, 302, 304, 309, 316, 319, 320, 322, 323, 325, 326, 333, 337, 338, 339, 340, 342], "configbas": [194, 199], "configur": [3, 27, 32, 34, 55, 56, 57, 150, 151, 183, 193, 194, 198, 199, 241, 249, 251, 252, 258, 259, 263, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 285, 287, 288, 295, 296, 297, 298, 299, 301, 304, 308, 319, 320, 323, 324, 326, 329, 331, 333, 335, 337, 342], "configuration_prepar": 316, "configverifi": 186, "conflict": 291, "conform": 305, "confus": [251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337], "congratul": 302, "conjunct": 278, "connect": 33, "consecut": 253, "consid": [3, 39, 251, 257, 261, 291, 292, 303, 336, 339], "consider": [236, 291], "consist": [25, 154, 257, 291, 338], "const": [61, 257], "constant": [3, 7, 66, 150, 236, 257], "constrain": [252, 259, 295, 303, 319], "constraint": [32, 33, 50, 53, 72, 74, 299, 303, 336], "construct": [32, 118, 149, 213, 236, 254, 257, 338], "constructor": 32, "consum": [32, 33, 50, 54, 251, 298, 300], "consumpt": [298, 299], "contain": [9, 32, 33, 39, 50, 55, 56, 107, 124, 150, 154, 177, 193, 198, 199, 213, 215, 217, 233, 236, 241, 243, 244, 250, 251, 252, 253, 254, 255, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 281, 283, 284, 286, 287, 291, 292, 301, 315, 320, 324, 325, 326], "content": 342, "context": [6, 7, 192, 261, 306], "continu": [298, 299], "contrib": 55, "contribut": 236, "control": [57, 150, 151, 183, 194, 199, 255, 257, 279, 280, 291, 298, 302, 336, 338], "control_v11p_sd15_canni": 309, "controlnet": 342, "controlnet_id": 309, "conv": [9, 33, 53, 61, 95, 102, 140, 257, 287, 298, 301], "conv1": [9, 124, 177, 233, 243, 257, 291], "conv1d": 228, "conv2": [9, 124, 177, 233, 243, 257], "conv2d": [9, 33, 124, 177, 199, 217, 218, 219, 228, 230, 233, 234, 235, 243, 251, 304, 342], "conv2d_85": 212, "conv3d": 228, "conv_501_dequantizelinear": 291, "conv_501_dequantizelinear_output": 291, "conv_664": 257, "conv_665": 257, "conv__224": [61, 257], "conv__252": [61, 257], "conv_out": 291, "conv_transpose2d": [220, 228], "conveni": [252, 293], "converg": [252, 257], "convers": [50, 51, 82, 85, 95, 99, 257, 293, 303], "convert": [8, 9, 22, 51, 53, 55, 57, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 99, 106, 114, 118, 124, 150, 154, 177, 182, 215, 233, 243, 257, 266, 282, 283, 291, 294, 301, 305, 320], "convert_a8w8_npu_to_a8w8_cpu": [100, 293], "convert_act": 9, "convert_avg_pool_to_dpu_vers": 74, "convert_bn_to_conv": 53, "convert_clip_to_relu": 53, "convert_conv": 9, "convert_customqdq_to_qdq": 100, "convert_dynamic_to_fix": [100, 266], "convert_exported_model_to_gguf": 154, "convert_float16_to_float": [82, 99], "convert_float32_to_float16": 85, "convert_float_to_float16": [85, 99], "convert_float_to_float16_model_path": 99, "convert_fp16_to_bf16": [100, 293], "convert_fp16_to_bfp16": [100, 293], "convert_fp16_to_fp32": [55, 57, 100, 257, 293, 300, 301], "convert_fp32_to_bf16": [100, 293], "convert_fp32_to_bfp16": [100, 293], "convert_fp32_to_fp16": [100, 282, 283], "convert_gemm": 9, "convert_hard_sigmoid_to_dpu_vers": 74, "convert_hf_to_gguf": 320, "convert_initializers_to_float": 92, "convert_instance_norm_to_dpu_vers": 74, "convert_leaky_relu_to_dpu_vers": 74, "convert_lstm_to_customlstm": 100, "convert_matmul": 9, "convert_nchw_to_nhwc": [55, 57, 100, 257, 293, 300], "convert_norm": 9, "convert_np_to_float": 99, "convert_np_to_float16": 99, "convert_onnx_to_onnxtxt": 100, "convert_onnx_to_torch": 22, "convert_onnxtxt_to_onnx": 100, "convert_ops_to_modul": 9, "convert_opset_vers": 100, "convert_qdq_to_qop": 100, "convert_quant_to_float": 100, "convert_reduce_mean_to_dpu_vers": 74, "convert_reduce_mean_to_global_avg_pool": 53, "convert_resize_fs_to_pof2": 100, "convert_s8s8_to_u8s8": 100, "convert_shared_initializer_to_uniqu": 100, "convert_sigmoid_to_hard_sigmoid": 74, "convert_softmax_to_dpu_vers": 74, "convert_split_to_slic": 53, "convert_tensor_float16_to_float": 99, "convert_tensor_float_to_float16": 99, "convert_to_bfloat_before_bfp": 257, "convert_torch_to_onnx": 22, "convert_transform": 52, "convert_transforms_pipelin": 52, "convert_u16s8_to_s16s8": 100, "convert_u16u8_to_u8u8": [100, 293], "convertavgpooltodpuvers": 257, "convertbn2d2convqopass": 217, "convertbntoconv": 257, "convertclip2reluqopass": 215, "convertcliptorelu": 257, "converthardsigmoidtodpuvers": 257, "convertleakyrelutodpuvers": 257, "convertopsetvers": 257, "convertqdqtoqoptransformspipelin": 51, "convertreducemean2gapqopass": 217, "convertreducemeantodpuvers": 257, "convertreducemeantoglobalavgpool": 257, "convertsigmoidtohardsigmoid": 257, "convertsoftmaxtodpuvers": 257, "convertsplittoslic": 257, "convet": 51, "convient": 1, "convolut": [202, 253, 257, 291, 296, 326], "convqdqtoqoptransform": 50, "convtranspos": [9, 102, 257, 293, 298, 301], "convtranspose2d": [234, 304, 342], "copi": [95, 177, 337], "copy_func_with_new_glob": 138, "copyright": [270, 276, 278, 294, 295, 296, 297, 298, 299], "copysharedinit": 257, "core": [33, 299, 308], "correct": [6, 7, 39, 55, 181, 192, 258, 302], "correctli": 251, "correspond": [6, 7, 55, 93, 151, 177, 192, 205, 217, 236, 241, 257, 266, 268, 269, 271, 277, 279, 280, 307, 337], "cos_metr": 1, "cosin": [1, 98, 293], "cost": [251, 259, 295, 303], "could": [32, 33, 50, 61, 95, 149, 150, 252, 257, 279, 280, 320, 340], "couldn": 320, "count": [257, 299], "counterclockwis": 339, "counterpart": 251, "coupl": 118, "cover": 285, "cp": [272, 273, 274, 276, 278, 281, 282, 283], "cpp": 342, "cpu": [4, 27, 53, 63, 199, 202, 234, 236, 243, 249, 250, 251, 252, 257, 258, 259, 284, 286, 287, 291, 292, 294, 295, 296, 297, 301, 304, 310, 311, 313, 314, 315, 324, 331, 342], "cpuexecutionprovid": [3, 57, 76, 257, 259, 294, 295, 296, 297, 300, 301], "crash": [32, 33, 50], "creat": [3, 6, 7, 20, 31, 32, 39, 54, 55, 61, 118, 121, 122, 127, 128, 131, 135, 136, 139, 143, 156, 177, 178, 192, 194, 199, 206, 209, 222, 241, 257, 258, 260, 266, 267, 268, 269, 271, 275, 277, 298, 305, 308, 316, 337, 339], "create_calibrator_float_scal": 3, "create_calibrator_power_of_two": 3, "create_sess": 20, "creation": [32, 33, 50, 118], "criteria": 299, "criterion": 319, "critic": [57, 118, 194, 199, 251, 257, 296, 298], "cross": [249, 286, 287, 289, 292], "crossentropyloss": 319, "crosslayerequ": 257, "crucial": 317, "csr": 236, "csv": [313, 314], "ctx": [6, 7, 192], "cuda": [119, 178, 236, 242, 243, 250, 252, 257, 258, 259, 266, 279, 280, 284, 285, 294, 295, 296, 297, 301, 304, 311, 313, 314, 342], "cuda_visible_devic": [317, 318], "cudaexecutionprovid": [257, 259, 285, 294, 295, 296, 297, 301], "cudnn_batch_norm": [219, 228], "cudnn_en": 228, "cuh": 242, "cumbersom": 150, "cur_it": 26, "current": [6, 7, 26, 39, 55, 61, 118, 150, 177, 192, 213, 228, 236, 249, 250, 251, 252, 257, 260, 278, 291, 293, 299, 301, 307, 308, 310, 311, 312, 313, 314, 316, 319, 322, 323, 325, 326, 339, 340, 342], "custom": [6, 7, 55, 78, 86, 149, 150, 192, 199, 241, 243, 250, 257, 258, 259, 265, 279, 280, 284, 291, 294, 295, 296, 297, 299, 301, 316, 323, 342], "custom_mod": [150, 177, 180, 323, 324, 326], "custom_op": 250, "custom_ops_infer_shap": [78, 86], "customer_defined_evalu": [280, 284], "customformatt": 118, "customiz": 342, "customqdq_to_contribqdq": 55, "cut": [9, 249, 257], "cv3": 287, "d": [33, 118, 236, 262, 320, 332], "d0": [262, 332], "d_1": 303, "d_2": 303, "daisi": [284, 286, 287, 292], "damp_perc": [199, 341], "dampen": [199, 257], "dao": 140, "data": [1, 3, 6, 7, 20, 22, 25, 27, 39, 55, 56, 57, 61, 76, 111, 121, 122, 127, 128, 131, 135, 136, 149, 151, 177, 181, 182, 183, 192, 193, 198, 199, 202, 205, 241, 243, 249, 252, 254, 257, 258, 259, 261, 263, 264, 279, 280, 285, 290, 291, 294, 295, 296, 297, 298, 304, 305, 306, 309, 316, 318, 319, 320, 324, 333, 334, 336, 337, 338, 341, 342], "data_batch": 305, "data_dir": 319, "data_fold": 258, "data_it": 258, "data_list": 260, "data_load": [27, 121, 122, 127, 128, 131, 135, 265], "data_num": 20, "data_path": [55, 111], "data_prepar": 305, "data_read": [1, 3, 20, 21, 22, 55, 294, 296, 297, 298], "data_s": [27, 55], "data_tensor": 71, "data_typ": 324, "databrick": [316, 342], "dataclass": [203, 300, 341], "dataclass_inst": 203, "dataclass_pretty_str": 203, "dataload": [1, 27, 76, 121, 122, 127, 128, 131, 135, 136, 149, 182, 193, 198, 205, 257, 265, 279, 280, 284, 302, 304, 307, 320, 333, 337, 338], "dataread": [279, 280, 299], "datas": [257, 259, 284, 295, 296, 297, 299], "dataset": [25, 149, 258, 263, 265, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 299, 311, 313, 314, 316, 317, 319, 320, 324, 333, 336, 337, 338], "dataset_path": 318, "datashap": 212, "datatyp": [1, 55, 178, 199, 316], "datatypespec": [199, 342], "datefmt": 118, "dbrx": [180, 316, 342], "dbrx_experts_group": 180, "de": 55, "deal": 251, "debug": [57, 118, 194, 199, 257, 291, 336], "debug_mod": [55, 57, 61], "decid": [284, 299], "decim": 251, "decod": [147, 199, 316, 337, 339, 340], "decompos": 303, "decompress": 251, "deconstruct": 32, "decor": [119, 159], "decreas": [252, 257, 337], "dedic": [54, 257], "dedicate_dq_nod": 53, "dedicated_qdq_pair": 54, "dedicatedqdqpair": 257, "deem": [32, 118, 298], "deep": [56, 150, 182, 193, 198, 249, 251, 294, 296, 302, 320, 338], "deepcopi": 337, "deepseek": [316, 342], "def": [6, 7, 9, 124, 159, 177, 192, 233, 243, 258, 260, 265, 280, 284, 305, 316, 337], "default": [3, 7, 32, 39, 56, 57, 61, 95, 110, 118, 149, 150, 151, 177, 183, 194, 198, 199, 212, 217, 235, 236, 252, 253, 254, 255, 257, 270, 279, 280, 284, 291, 293, 298, 301, 307, 309, 323, 324, 326, 339, 341, 342], "default_bfp16_per_block": 338, "default_config": [279, 280], "default_int8_per_tensor_sym_spec": 304, "default_mx_fp_8_per_block": 302, "default_uint4_per_group_asym_spec": [198, 320, 333], "default_w_bfp16_per_block_config": 338, "default_w_fp8_a_fp8_per_tensor_config": 333, "default_w_int8_a_int8_per_tensor_dynamic_config": 333, "default_w_int8_per_tensor_config": 304, "default_w_mx_fp8_per_block_config": 302, "default_w_uint4_per_group_config": [198, 320, 333], "defer": 199, "defin": [1, 5, 6, 7, 31, 32, 33, 50, 55, 56, 71, 159, 162, 183, 185, 192, 193, 198, 199, 202, 213, 251, 257, 258, 260, 265, 267, 279, 280, 284, 296, 299, 300, 302, 304, 305, 307, 308, 320, 333, 337, 341], "definit": [320, 338], "degrad": [293, 298, 303, 336], "degre": [336, 339], "delet": [55, 119, 331], "delete_directory_cont": 119, "delimit": 312, "deliv": 336, "delta": 320, "demand": [249, 251, 279, 280, 293], "demonstr": [252, 253, 254, 255, 263, 265, 285, 300, 309, 318, 319, 333, 336, 338], "denot": [151, 303, 320], "dens": 294, "densenet121": 277, "depend": [118, 193, 198, 236, 251, 308, 317, 324, 336, 337], "deploi": [249, 252, 257, 259, 291, 295, 298, 299], "deploy": [53, 202, 217, 248, 249, 251, 252, 257, 259, 279, 280, 293, 295, 298, 300, 336, 340, 342], "deprec": [177, 257], "depth": 342, "depthtospac": 301, "depthwise_conv": 253, "depthwiseconv": 33, "depthwiseconv2d": 33, "dequant": [7, 54, 177, 178, 251, 303, 320], "dequante4m3funct": 192, "dequante5m2funct": 192, "dequantize_data": 55, "dequantizefunct": 192, "dequantizelinear": [7, 92, 107, 257, 301], "dequantizerlinear": 325, "deriv": [5, 31, 55, 162, 243], "desc_act": [199, 341], "descend": 199, "descent": 319, "describ": [32, 33, 50, 118, 264, 276, 282, 316, 334, 341], "descript": 341, "design": [7, 177, 249, 251, 252, 279, 280, 286, 287, 292, 293, 298, 299, 318, 320, 336, 337], "desir": [118, 183, 251, 299, 303], "destin": 177, "destruct": [307, 337], "detach": 177, "detail": [6, 7, 57, 61, 151, 159, 183, 192, 194, 199, 235, 236, 242, 251, 258, 259, 260, 263, 272, 278, 282, 283, 285, 291, 295, 296, 298, 299, 303, 304, 305, 311, 313, 314, 315, 316, 319, 324, 333, 340, 341], "detect": [287, 289, 342], "detection_imag": 287, "determ": 183, "determin": [3, 32, 118, 183, 199, 236, 241, 251, 252, 253, 254, 255, 257, 261, 263, 291, 299, 302, 303, 333, 336, 337, 341], "determinist": 236, "develop": [249, 250, 301, 308, 318, 320], "deviat": 252, "devic": [149, 178, 183, 193, 198, 199, 202, 234, 236, 237, 240, 241, 242, 243, 252, 257, 259, 266, 267, 270, 276, 278, 294, 295, 296, 297, 298, 299, 301, 305, 311, 313, 314, 315, 316, 319], "device_id": 319, "devicetyp": [199, 202], "di": [262, 332], "diag": 257, "diagon": 257, "dict": [1, 3, 4, 6, 13, 21, 22, 31, 33, 34, 39, 51, 54, 55, 57, 61, 63, 66, 71, 92, 111, 136, 138, 150, 169, 177, 180, 193, 198, 199, 205, 230, 257, 279, 280, 284, 291, 341], "dictat": 299, "dictionari": [3, 39, 54, 55, 57, 61, 118, 177, 193, 198, 199, 205, 236, 252, 253, 254, 255, 257, 298, 305, 316, 337], "diff": 32, "differ": [1, 9, 39, 55, 57, 61, 151, 183, 194, 199, 202, 205, 251, 252, 253, 254, 255, 257, 259, 261, 262, 266, 267, 268, 269, 279, 280, 293, 294, 295, 298, 299, 301, 303, 306, 307, 310, 316, 320, 332, 333, 337, 342], "differenti": [6, 7, 192, 236], "difficult": 307, "difficulti": [76, 255, 257, 337], "difflib": 32, "diffus": [249, 289, 330, 342], "diffusers_root": 309, "dilat": [218, 219, 220, 234, 235], "dim": [217, 236], "dimens": [39, 141, 254, 257, 262, 291, 307, 332], "dir": 324, "directionari": 270, "directli": [6, 7, 110, 138, 140, 192, 193, 198, 213, 217, 244, 250, 251, 252, 257, 266, 267, 268, 269, 270, 271, 275, 277, 284, 286, 292, 293, 294, 308, 319, 323, 326, 339], "directori": [55, 119, 150, 154, 205, 250, 257, 260, 289, 291, 301, 308, 310, 316, 319, 320, 324, 330, 331, 342], "disabl": [61, 252, 307], "disable_shape_inf": [82, 85, 99, 282, 283], "disappear": 294, "discard": [252, 257], "discrep": 320, "discret": [254, 278], "discuss": 278, "disk": [114, 205], "displai": 304, "distanc": [20, 284], "distinct": [263, 333], "distribut": [3, 111, 112, 205, 254, 257, 261, 263, 265, 278, 307, 319, 333, 337], "distribution_plot": 205, "div": [257, 301], "div_1": 287, "divid": [140, 332], "dll": [259, 294, 295, 296, 297, 301], "dm": 320, "dml": 324, "dn": [262, 332], "dnn": 338, "do": [6, 7, 22, 61, 95, 140, 177, 180, 192, 236, 251, 252, 257, 261, 270, 279, 280, 291, 293, 294, 296, 297, 298, 301, 303, 304, 307, 316, 331, 335, 336, 337, 339], "do_constant_fold": 150, "do_onnx_ev": [272, 273, 274, 276, 278, 281, 282, 283], "doc": [61, 205, 228, 250, 320], "document": [205, 250, 251, 252, 285, 291, 294, 296, 297, 298, 303, 304, 307, 308, 310, 316, 317, 324, 331, 335, 336, 337, 341], "doe": [56, 140, 198, 228, 241, 252, 257, 287, 291, 308, 331], "doesn": [241, 320], "domain": [192, 254, 278], "don": [39, 140, 316], "done": [140, 257, 320], "dot": 244, "doubl": 236, "down": [26, 27, 302, 339], "down_proj": [307, 336, 341], "download": [250, 266, 267, 268, 269, 270, 271, 272, 275, 277, 278, 279, 280, 282, 283, 284, 286, 287, 289, 292, 309, 315, 316, 317, 320, 324, 330], "downstream": [150, 321, 323, 326], "dpu": [55, 74, 75, 257, 291], "dpu_leaky_relu_alpha": 55, "dq": [7, 36, 107, 108, 257, 294, 301, 342], "dr": [18, 36, 55], "drop": [252, 257, 279, 280, 284, 302, 336], "drop_ratio": 27, "dropout": 228, "dropratio": 257, "dtype": [7, 114, 178, 198, 199, 202, 236, 243, 302, 304, 311, 313, 314, 319, 333, 336, 337, 338, 341], "dual": 296, "dualquantnod": 298, "duck": 159, "due": [251, 254, 257, 278, 282, 283, 303, 337], "dummi": [303, 304, 337], "dummy_path": 27, "dump": [55, 309], "dump_data_fold": 309, "dump_data_read": [55, 291], "dump_float": [55, 291], "dump_model": [55, 291], "dump_result": [55, 291], "duplic": [95, 257], "duplicatefilt": 118, "dure": [6, 7, 34, 39, 54, 55, 93, 192, 193, 198, 199, 202, 236, 249, 250, 251, 252, 255, 257, 260, 263, 265, 278, 279, 280, 285, 291, 299, 301, 305, 319, 333, 336, 341], "dynam": [57, 61, 79, 198, 199, 249, 252, 257, 258, 259, 263, 266, 286, 287, 291, 292, 294, 295, 297, 298, 300, 304, 320, 323, 333, 336, 338, 340, 341, 342], "dynamic_quantized_model": [272, 273], "dynmaic": 257, "e": [6, 7, 33, 71, 95, 118, 140, 150, 177, 183, 192, 199, 205, 217, 226, 230, 236, 241, 243, 244, 257, 279, 280, 303, 307, 310, 312, 323, 324, 326, 336], "e2m1": [267, 297, 302], "e2m3": [267, 297, 302], "e3m2": [267, 297, 302], "e4m3": [267, 297, 302], "e4m3fn": [249, 342], "e5m2": [249, 267, 297], "e8m0": [199, 297], "e_": 303, "each": [3, 6, 7, 39, 54, 57, 107, 150, 183, 192, 199, 205, 213, 217, 230, 236, 241, 244, 248, 252, 257, 259, 260, 261, 262, 265, 279, 280, 291, 294, 295, 296, 297, 299, 302, 303, 307, 310, 312, 316, 320, 332, 336, 337, 338, 341], "eager": [150, 198, 202, 228, 249, 304, 307, 342], "eager_mod": [150, 198, 199, 202], "earli": [252, 257], "earlier": 250, "early_stop": 27, "earlystop": [257, 259, 284, 295, 296, 297], "eas": [251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337, 342], "easi": [249, 260, 320, 337], "easier": [181, 253, 257, 291, 299], "easili": [318, 337], "ebit": 6, "ecosystem": 308, "edg": [241, 249, 252], "edit": [250, 301], "effect": [186, 199, 228, 251, 257, 259, 291, 295, 296, 297, 303, 308, 337, 339], "effici": [181, 236, 248, 251, 252, 257, 259, 266, 267, 286, 287, 291, 292, 293, 295, 296, 297, 298, 299, 319, 320, 336, 337, 340], "eg": 150, "either": [1, 6, 7, 118, 192, 193, 198, 236, 251, 299, 305, 314], "element": [55, 199, 202, 236, 241, 244, 249, 251, 257, 259, 267, 295, 296, 297, 298, 302, 303, 338, 342], "element_dtyp": [257, 297], "element_typ": 55, "eleutherai": 316, "elif": [294, 295, 296, 297], "elimin": [291, 336], "ellipsi": [138, 140, 150, 178, 205], "els": [294, 295, 296, 297, 316, 325], "elucid": 251, "emax": 6, "embed": [183, 236, 304, 326], "embedding_dim": 236, "embedding_sum": 236, "embeddingbag": [236, 304], "embeddingtyp": 162, "emerg": 298, "emit": 118, "emploi": [251, 336], "empow": 249, "empti": [3, 57, 95, 118, 150, 199, 205, 236, 257, 291, 337], "enabl": [57, 61, 199, 213, 248, 251, 253, 254, 255, 260, 267, 286, 287, 292, 294, 301, 307, 308, 310, 318, 320, 340, 342], "enable_data_cach": 265, "enable_dpu": 257, "enable_npu_cnn": [55, 57, 252, 253, 255, 257, 279, 280, 285, 300, 301], "enable_npu_transform": [55, 57, 254, 257, 300], "enablesubgraph": [61, 257], "enablevaimlbf16": 257, "encapsul": [57, 151, 183, 194, 199, 308], "encod": [150, 154, 320], "encount": [251, 252, 279, 280, 294, 296, 297, 298, 303, 304, 307, 317, 319, 331, 335, 336, 337], "encourag": 310, "end": [57, 61, 177, 252, 257, 310, 312, 320, 324], "end_node1": 257, "end_node2": 257, "end_node3": 257, "energi": [254, 278], "enforc": [6, 7, 54, 55, 177, 192], "engin": 202, "enhanc": [199, 249, 251, 259, 267, 295, 296, 297, 298, 308, 319, 336, 342], "ensur": [6, 7, 39, 56, 186, 192, 193, 198, 213, 243, 250, 252, 257, 260, 261, 279, 280, 285, 286, 287, 291, 292, 293, 299, 301, 302, 308, 310, 317, 318, 319], "entir": [57, 183, 199, 202, 257, 267, 296, 300, 336], "entri": [31, 236, 339], "entropi": [3, 249, 257, 258, 299, 342], "entropycalibrat": 3, "enum": 156, "enum_data": 260, "enumer": [5, 31, 55, 162, 257, 265], "env": [6, 7, 192], "environ": [32, 250, 252, 257, 259, 266, 295, 301, 307, 310], "eor": 312, "ep": [144, 228, 235, 241], "epsilon": [55, 219, 228, 307], "equal": [55, 181, 199, 217, 236, 242, 250, 251, 253, 257, 286, 287, 289, 292, 298, 337], "equalize_iter": 181, "equalize_merge_bia": 181, "equanl": 5, "equat": [55, 251, 320], "equival": [6, 7, 192, 230, 236, 251, 320, 337], "erf": 301, "error": [32, 57, 118, 183, 193, 194, 198, 199, 205, 250, 252, 254, 257, 261, 278, 286, 287, 290, 292, 296, 297, 303, 306, 307, 312, 336, 337], "especi": [61, 253, 254, 255, 257, 263, 267, 287, 333], "essenti": [56, 193, 198, 279, 280], "estim": [261, 306], "et": [26, 27, 181, 202], "etc": [9, 118, 124, 177, 180, 205, 230, 233, 243, 257, 279, 280, 284, 320, 341], "eval": [1, 198, 228, 291, 302, 304, 310, 311, 313, 314, 318, 320, 333, 337, 338], "eval_mod": 312, "eval_task": 317, "evalaut": 310, "evalu": [1, 9, 100, 124, 177, 233, 243, 279, 280, 284, 298, 299, 319, 320, 328, 330, 342], "evaluatefunct": 257, "evalut": 1, "evalutor": 284, "even": [182, 202, 251, 257, 293, 301, 303], "event": 118, "everi": [55, 302, 339], "evluat": 270, "ex": [250, 301], "exact": 257, "exactli": [236, 320], "exampl": [1, 6, 7, 9, 33, 39, 61, 77, 82, 85, 87, 93, 95, 99, 110, 111, 112, 118, 124, 150, 159, 177, 182, 192, 198, 212, 233, 236, 240, 241, 242, 243, 244, 249, 250, 251, 257, 263, 265, 272, 273, 274, 275, 276, 277, 279, 280, 281, 282, 284, 285, 286, 287, 291, 292, 294, 298, 299, 300, 301, 302, 303, 305, 307, 308, 309, 311, 312, 313, 314, 315, 316, 317, 320, 324, 333, 336, 337, 338, 339, 340, 341, 342], "example_input": [150, 319, 335], "example_quark_torch_llm_eval_harness_offlin": 310, "exce": [279, 280, 290, 299], "exceed": [290, 291, 299], "except": [32, 118, 300, 341], "excess": 53, "exclud": [5, 39, 53, 54, 55, 57, 61, 199, 226, 236, 257, 286, 287, 292, 307, 314, 316, 324, 340, 341], "exclude_lay": [150, 324, 336, 341], "exclude_nod": [286, 287, 292], "exclude_subgraph": 287, "excludeindic": 257, "excut": [1, 270], "exec": 272, "execut": [3, 32, 57, 150, 250, 253, 257, 266, 267, 284, 285, 291, 293, 298, 299, 301, 316, 317, 320, 324], "execution_provid": [3, 55, 57, 257, 285, 300], "exemplifi": 251, "exercis": 308, "exhaust": 299, "exist": [39, 56, 95, 199, 238, 244, 257, 298, 299, 316], "exist_ok": 265, "exp": [6, 7, 192], "expand": [257, 301], "expect": [149, 150, 182, 183, 193, 198, 251, 299, 308, 337], "expens": 183, "experi": [252, 253, 254, 255, 336], "experiment": [249, 257, 298, 301, 308, 319, 342], "explain": [54, 265, 279, 280, 294, 296, 302, 339], "explan": 319, "explicit": [32, 251, 294], "explicitli": [53, 251, 257], "explor": [249, 279, 280, 299, 302], "expon": [183, 202, 244, 249, 257, 259, 266, 267, 286, 287, 292, 295, 296, 297, 298, 302, 303, 338], "exponent_bit_width": 183, "exponential_average_factor": 219, "export": [182, 198, 199, 226, 249, 251, 266, 268, 269, 271, 272, 273, 274, 276, 277, 278, 279, 280, 281, 282, 283, 287, 304, 311, 314, 318, 319, 329, 335, 342], "export2oga": 278, "export_config": [150, 322, 323, 325, 326], "export_dir": [150, 319, 320, 322, 325, 326, 335], "export_gguf_model": [150, 320, 322], "export_hf_model": [323, 326], "export_import_hf_model": [323, 326], "export_onnx": [266, 268, 269, 271, 277], "export_onnx_model": [150, 182, 304, 319, 325], "export_path": [150, 182, 304, 320, 322, 323, 325, 326], "export_quark_model": [150, 326], "export_yolo_to_onnx": 287, "exported_model": 319, "exporterconfig": [150, 151, 319, 320, 322, 323, 325, 326], "expos": 308, "express": [33, 251], "extend": [6, 7, 54, 192, 267, 296], "extens": [249, 250, 252, 257, 318, 330, 342], "extent": 76, "extern": [3, 57, 118, 291, 308, 318], "external_data_loc": 291, "external_data_size_threshold": 291, "extra": [22, 61, 241, 243, 257, 294], "extra_op_types_to_quant": [55, 57, 257], "extra_opt": [3, 4, 18, 21, 22, 36, 39, 54, 55, 57, 61, 63, 66, 252, 253, 254, 255, 257, 259, 260, 279, 280, 284, 285, 294, 295, 296, 297, 298, 299, 300, 301], "extra_repr": [241, 243], "extract": [9, 11, 236, 320], "extract_attr_valu": 11, "extract_padding_param": 9, "extract_padding_params_for_conv": 9, "extract_weight_and_bia": 9, "extrem": 336, "f": [9, 33, 118, 124, 138, 177, 233, 243, 265, 272, 316, 320], "face": [287, 288, 302, 305, 316, 317, 329], "face_quant": 287, "facebook": [198, 273, 274, 276, 281, 304, 315, 316, 333, 338], "facilit": [265, 308, 324, 340], "fact": [118, 294, 298, 307, 326, 337], "factor": [3, 39, 151, 199, 202, 251, 257, 262, 264, 299, 303, 319, 323, 332, 334, 336, 337], "fail": 32, "failur": [32, 291], "failureexcept": 32, "fair": 312, "fake": [215, 243, 319, 326], "fake_datatyp": 199, "fake_dtyp": 199, "fake_qu": 319, "fake_quant": 215, "fakequant": [178, 198, 225, 235, 307, 319], "fakequantizebas": [178, 205, 243], "fals": [3, 7, 27, 39, 53, 54, 55, 57, 61, 66, 99, 114, 118, 141, 150, 156, 177, 181, 183, 198, 199, 234, 235, 236, 241, 252, 253, 254, 255, 257, 265, 270, 279, 280, 284, 290, 291, 299, 300, 304, 305, 316, 319, 320, 325, 333, 337, 339, 341], "famili": 302, "fashion": 307, "fast": [18, 22, 57, 140, 250, 257, 289, 294, 295, 296, 297, 300, 320, 339, 342], "fast_finetun": [259, 295, 296, 297], "faster": [140, 250, 251, 252, 257, 259, 266, 267, 285, 290, 294, 295, 298], "fastfinetun": [95, 252, 257, 259, 279, 280, 284, 285, 294, 295, 296, 297, 299], "fatal": [57, 194, 199, 257], "favor": [150, 251], "fc1": 337, "feasibl": [251, 320], "featur": [140, 217, 252, 260, 261, 285, 291, 301, 302, 306, 307, 320, 339, 340], "feature_extraction_util": [198, 205], "fed": 251, "feed": [20, 22, 33, 55, 111, 257], "feel": 336, "fetch": 149, "few": [236, 250, 251, 298, 303, 311, 320], "fewer": [259, 295, 303], "fewshot": 311, "fid": 309, "field": [71, 154], "figur": [284, 286, 287, 292, 320, 336, 337], "file": [22, 56, 61, 82, 85, 99, 110, 118, 150, 154, 198, 199, 250, 252, 253, 254, 255, 257, 259, 260, 265, 266, 267, 276, 279, 280, 282, 284, 286, 287, 289, 291, 292, 294, 295, 296, 297, 301, 307, 308, 310, 314, 315, 316, 317, 318, 319, 320, 322, 323, 326, 330, 335, 336, 339, 340, 342], "file_path": [199, 265], "filenam": 118, "fill": 236, "filter": [118, 299], "final": [32, 33, 50, 217, 251, 257, 275, 302, 303, 338], "final_layer_norm": 337, "find": [1, 32, 33, 39, 50, 55, 107, 108, 140, 150, 244, 272, 279, 280, 299, 305, 320, 336], "find_best_candid": 299, "find_int16_scal": 55, "find_n_candid": 299, "find_node_by_output": [107, 108], "find_quant_scale_zp": 39, "find_quantized_valu": 39, "fine": [57, 202, 249, 257, 261, 267, 284, 286, 287, 292, 296, 302, 306, 317, 336, 342], "finer": [267, 296, 297, 303], "finetun": [257, 289, 294, 295, 296, 297, 300, 336], "finetune_batchs": 317, "finetune_checkpoint": 317, "finetune_dataset": 317, "finetune_datasubset": 317, "finetune_epoch": 317, "finetune_it": 317, "finetune_lr": 317, "finetune_seqlen": 317, "finetune_w_uint4_asym": 317, "finfo": 241, "finit": 99, "first": [6, 7, 39, 192, 198, 241, 250, 257, 270, 291, 301, 304, 307, 308, 320, 337], "five": 270, "fix": [54, 55, 79, 93, 99, 236, 249, 252, 257, 259, 264, 266, 291, 295, 319, 334, 337], "fix_shap": 266, "fixed_se": 27, "fixedse": [257, 259, 295, 296, 297], "fixneuron": [257, 291], "fixshap": 257, "fixtur": 32, "flag": [39, 53, 55, 57, 150, 151, 199, 213, 252, 253, 254, 255, 257, 291, 324, 339], "flaki": 119, "flexibl": [252, 259, 263, 295, 296, 297, 298, 299, 308, 333], "float": [1, 3, 4, 5, 6, 7, 8, 9, 13, 20, 22, 25, 26, 27, 39, 54, 55, 63, 72, 76, 82, 83, 84, 85, 92, 93, 99, 110, 144, 150, 181, 183, 192, 198, 199, 202, 205, 219, 235, 236, 241, 249, 251, 252, 255, 257, 258, 263, 265, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 289, 292, 293, 294, 295, 296, 297, 298, 299, 303, 304, 316, 319, 320, 333, 337, 339, 341, 342], "float16": [3, 7, 80, 81, 82, 85, 202, 241, 249, 251, 257, 258, 282, 283, 298, 303, 304, 324, 325, 340, 341, 342], "float16_convert": 99, "float16spec": 199, "float32": [1, 3, 8, 13, 82, 83, 84, 85, 99, 111, 114, 202, 241, 251, 257, 258, 265, 303, 304, 311, 313, 314, 320, 324], "float8": 251, "float_16_onnx_model_path": [80, 81, 82, 85, 293], "float_32_onnx_model_path": [82, 83, 84, 85, 293], "float_bia": 22, "float_dtyp": 178, "float_model": [258, 265, 319], "float_model_path": [18, 36, 258], "float_onnx_model_path": [293, 299], "float_output": 26, "float_quant": 183, "float_result": 20, "float_weight": 22, "floor": [202, 341], "flow": 312, "fmodel_path": 21, "fmt": 118, "fn_attr": 54, "fn_type": 54, "fname_out": 156, "focus": 310, "fold": [22, 53, 54, 150, 257, 337], "fold_batch_norm": 53, "fold_batch_norm_after_concat": 53, "fold_relu": 54, "foldbatchnorm": 257, "folder": [250, 260, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 289, 292, 301, 307, 309, 315, 316, 319, 324, 330, 331, 339], "folder1": 98, "folder2": 98, "foldrelu": 257, "follow": [6, 7, 26, 27, 32, 33, 39, 50, 181, 192, 199, 215, 236, 243, 248, 250, 251, 257, 258, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 283, 285, 291, 296, 298, 299, 300, 301, 302, 303, 304, 306, 307, 309, 312, 314, 315, 316, 317, 319, 320, 324, 333, 336, 338, 339, 340, 342], "footprint": [251, 259, 295], "forc": [39, 61, 257, 296, 297], "forcequantizenoinputcheck": [61, 257], "form": [251, 326, 339], "format": [3, 9, 54, 57, 88, 89, 118, 150, 151, 202, 236, 249, 251, 257, 258, 259, 260, 266, 268, 269, 270, 271, 275, 277, 279, 280, 286, 287, 289, 291, 292, 293, 294, 295, 296, 297, 302, 304, 305, 312, 319, 320, 321, 324, 342], "formatexcept": 118, "formatt": 118, "formattim": 118, "former": 298, "formul": 319, "formula": [6, 7, 54, 192, 199], "forthcom": 319, "forward": [6, 7, 8, 9, 124, 140, 144, 177, 178, 192, 217, 233, 236, 241, 243, 337], "found": [33, 39, 107, 108, 182, 250, 257, 299, 303, 319, 337], "four": [150, 293, 316], "fp": 242, "fp16": [57, 257, 291, 294, 309, 320, 324, 331, 336, 342], "fp16_model": [282, 283], "fp32": [57, 92, 257, 286, 287, 292, 294, 302, 318, 319, 320], "fp4": [202, 249, 267, 302, 341, 342], "fp4_e2m1": [257, 297], "fp6": [202, 267, 302], "fp6_e2m3": [202, 249, 257, 297, 341, 342], "fp6_e3m2": [202, 249, 257, 297, 341, 342], "fp8": [150, 180, 199, 202, 240, 241, 242, 249, 267, 302, 304, 309, 323, 325, 336, 342], "fp8_attention_qu": 150, "fp8_e4m3": [202, 257, 297, 302, 333, 341, 342], "fp8_e4m3fn": [249, 342], "fp8_e5m2": [202, 249, 257, 297, 341, 342], "fp8_per_tensor_spec": [333, 341], "fp8e4m3perchannelspec": 199, "fp8e4m3pergroupspec": 199, "fp8e4m3pertensorspec": 199, "fp8e5m2perchannelspec": 199, "fp8e5m2pergroupspec": 199, "fp8e5m2pertensorspec": 199, "fp8\u2460": 316, "fp_model": 128, "fpquantiz": 7, "fpx": 336, "frac": [303, 307, 320, 337, 339], "framework": [32, 248, 251, 252, 279, 280, 291, 294, 318, 320], "frantar": 181, "free": [181, 336, 339], "freez": [150, 198, 304, 319, 320], "freeze_bn_stat": 235, "freeze_model": 226, "freezed_model": 319, "freezed_quantized_model": [304, 320], "freezedfakequant": 198, "freezedscaledfakequant": 243, "frequenc": [236, 241], "friendli": [303, 318, 340], "frobeniu": 26, "from": [1, 4, 5, 8, 9, 20, 22, 25, 26, 27, 31, 39, 53, 54, 55, 57, 61, 63, 76, 82, 85, 95, 99, 105, 107, 108, 114, 140, 149, 150, 151, 162, 177, 181, 198, 199, 202, 205, 228, 235, 236, 243, 249, 251, 252, 253, 254, 255, 257, 258, 259, 260, 261, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 306, 307, 308, 310, 311, 314, 316, 319, 322, 323, 325, 326, 333, 335, 336, 337, 338, 341, 342], "from_float": 241, "from_modul": 177, "from_pretrain": [198, 236, 302, 304, 316, 320, 333, 338], "from_subgraph": 39, "frozen": 337, "ft": 317, "full": [55, 118, 251, 317, 323], "fulli": 257, "func": [99, 140, 159], "funcnam": 118, "function": [6, 7, 31, 33, 50, 118, 124, 177, 186, 192, 193, 213, 215, 217, 233, 243, 251, 252, 257, 258, 280, 284, 291, 294, 305, 316, 319, 323, 325, 326, 335, 342], "function_nam": [138, 140], "functool": 205, "fundament": [32, 33, 50, 183, 339], "further": [226, 251, 257, 278, 279, 280, 302, 319, 339], "furthermor": 296, "fuse": [53, 235, 257, 291, 301, 337], "fuse_gelu": 53, "fuse_instance_norm": 53, "fuse_l2_norm": 53, "fuse_layer_norm": 53, "fusegelu": 257, "fuseinstancenorm": 257, "fusel2norm": 257, "fuselayernorm": 257, "fusion": [257, 291], "futur": [61, 177, 228, 257, 308], "fx": [198, 202, 211, 212, 213, 215, 217, 218, 219, 220, 221, 225, 226, 228, 242, 249, 304, 307, 330, 342], "fx_graph": [150, 198, 335], "fx_graph_mod": [150, 198, 199, 202, 319, 335], "g": [6, 7, 33, 95, 118, 150, 177, 183, 192, 199, 205, 217, 226, 230, 243, 244, 257, 272, 278, 279, 280, 282, 283, 303, 323, 326, 336], "g128": [311, 313, 314, 317], "gain": [249, 307], "gate": [302, 316], "gate_proj": [307, 341], "gather": 301, "gb": 291, "gelu": [53, 257], "gemm": [9, 95, 102, 254, 257, 278, 293, 298, 301], "genai": 324, "genai_config": 310, "gener": [1, 3, 5, 31, 32, 33, 50, 55, 61, 78, 82, 85, 86, 93, 114, 140, 141, 154, 159, 162, 181, 198, 199, 205, 254, 257, 260, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 290, 291, 298, 299, 302, 303, 305, 310, 312, 314, 319, 324, 326, 334, 335, 340, 342], "generate_initi": 114, "generate_input_initi": 32, "generation_config": [273, 274, 276, 281, 323], "genproto": 159, "get": [3, 8, 9, 22, 34, 55, 111, 141, 144, 150, 215, 242, 244, 257, 273, 274, 276, 281, 289, 302, 304, 320, 330, 338, 339], "get_annotate_tensor": 55, "get_available_provid": [294, 295, 296, 297], "get_bia": 8, "get_clip_min_max": 55, "get_config": [34, 150], "get_datatype_shap": 55, "get_default_config": [252, 258, 260, 300], "get_exclude_nod": 55, "get_export_model": 150, "get_fix_posit": 242, "get_hadamard_matric": 141, "get_input": 265, "get_library_path": [259, 285, 291, 294, 295, 296, 297, 301], "get_min_max_by_ms": 241, "get_min_max_by_percentil": 241, "get_model_typ": 316, "get_modules_optimized_bia": 9, "get_modules_optimized_weight": 9, "get_nested_attr_from_modul": 147, "get_next": [55, 111, 258, 260], "get_output": 265, "get_provid": 285, "get_qdq_to_remov": 55, "get_qmin_qmax_for_qtyp": 55, "get_qrange_for_qtyp": 55, "get_rotation_matrix": 144, "get_tensor_valu": 114, "get_token": 316, "get_weight": 8, "getmessag": 118, "gettempdir": 150, "ggml": 320, "ggml_common_aggr": 320, "ggml_half": 320, "ggml_half2": 320, "gguf": [150, 154, 249, 304, 316, 321, 342], "git": 309, "github": [267, 275, 284, 286, 287, 292, 309], "give": [1, 337], "given": [3, 6, 7, 9, 39, 55, 56, 61, 95, 141, 144, 147, 149, 159, 182, 192, 193, 198, 205, 213, 215, 217, 236, 238, 243, 244, 251, 254, 257, 293, 307], "global": [3, 57, 138, 140, 151, 199, 217, 230, 257, 300, 303, 341], "global_config": [182, 337], "global_quant_config": [57, 182, 183, 198, 199, 230, 252, 253, 254, 255, 257, 258, 259, 260, 285, 294, 295, 296, 297, 298, 300, 302, 304, 319, 320, 333, 337, 338, 341], "globalaveragepool": [53, 217, 291, 301], "goal": [261, 279, 280, 291, 303, 306], "goe": 303, "gold": 310, "good": [287, 302, 336], "gpfa2q": 181, "gpfq": 181, "gpt": [316, 342], "gpt2": 150, "gptj": 150, "gptnext": 150, "gptq": [181, 199, 249, 257, 258, 289, 304, 316, 320, 336, 341, 342], "gptq_config": 316, "gptq_quantized_model": 274, "gptqconfig": [131, 199, 341], "gptqparam": 257, "gptqprocessor": [131, 341], "gpu": [199, 249, 251, 257, 259, 266, 267, 279, 280, 291, 294, 295, 296, 297, 298, 301, 309, 310, 311, 313, 314, 315, 316, 341, 342], "gqa": 342, "grad": [6, 7, 192], "grad_factor": 192, "grad_output": [6, 7, 192], "gradcheck": [6, 7, 192], "gradient": [6, 7, 192, 236, 319], "gradual": 326, "grain": [267, 296, 297, 302], "grant": 324, "granular": [183, 297, 298, 303, 336, 342], "graph": [3, 31, 32, 33, 50, 55, 150, 182, 202, 249, 252, 257, 291, 304, 307, 325, 330, 337, 339, 340, 342], "graph_model": 319, "graph_modul": [213, 215, 217], "graphmodul": [198, 211, 212, 213, 215, 217, 218, 219, 220, 221, 225, 226, 228, 307, 319], "graphtransform": 206, "greater": [99, 252, 257, 279, 280, 291], "greatli": 285, "greedi": 181, "grid": [299, 336], "grok": [316, 342], "group": [150, 151, 199, 202, 218, 219, 220, 234, 235, 251, 257, 259, 266, 286, 287, 292, 295, 296, 297, 302, 303, 304, 320, 324, 332, 336, 341, 342], "group_siz": [150, 192, 198, 199, 302, 316, 317, 320, 322, 324, 332, 333, 336, 337, 338, 341], "groupsiz": 257, "grow": 298, "gs128": [311, 313, 314], "gsm8k": 312, "guarante": [181, 308, 320], "guess": 291, "guess_output_rank": 291, "guid": [251, 279, 280, 285, 289, 299, 302, 305, 320, 330, 336, 338], "guidanc": [260, 284, 286, 287, 291, 292, 316, 336], "guidelin": 310, "guidenc": 309, "gz": [266, 267, 268, 269, 271, 275, 277], "h": [217, 236, 320], "h1": 141, "h2": 141, "ha": [6, 7, 9, 22, 34, 55, 56, 82, 85, 99, 183, 192, 193, 198, 199, 236, 250, 252, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 279, 280, 281, 283, 286, 287, 292, 294, 297, 302, 304, 310, 320, 332, 337, 338, 339], "had": [199, 339], "hadamard": [140, 199, 254, 257, 278, 336, 339, 340], "hadamard_multipli": 140, "hadamard_transform": 140, "half": [9, 257, 298, 303], "half_even": [198, 199, 202, 304, 319, 320, 333, 337, 338, 341], "half_to_even": 338, "halt": [279, 280], "halv": 251, "hand": [251, 297], "handi": 320, "handl": [4, 6, 7, 33, 56, 63, 192, 193, 198, 199, 205, 251, 262, 332], "handler": 118, "har": [313, 314], "hard": [55, 257, 337], "hardmard_transform": 141, "hardsigmoid": 301, "hardwar": [140, 217, 248, 249, 251, 259, 263, 295, 296, 298, 310, 312, 320, 333, 336, 339, 340], "harmon": [254, 278], "hat": 320, "have": [6, 7, 9, 32, 33, 124, 150, 177, 181, 186, 192, 199, 205, 228, 233, 236, 241, 243, 251, 257, 266, 267, 268, 269, 271, 275, 277, 279, 280, 291, 293, 296, 297, 298, 301, 302, 303, 318, 320, 337, 339, 342], "head": 290, "heavi": 183, "hello": [198, 302, 304, 320, 333], "help": [251, 252, 261, 291, 298, 300, 306, 336, 337, 340, 341], "helper": [32, 55, 121, 122, 127, 128, 131, 135, 136, 139, 143, 156, 178, 186, 194, 199, 206, 209, 222, 241], "henc": [254, 278, 303, 337], "here": [1, 180, 182, 251, 252, 253, 254, 255, 258, 259, 267, 285, 289, 294, 295, 296, 297, 298, 303, 305, 307, 311, 312, 316, 317, 330, 333, 337, 339, 340, 341], "hessian": 257, "hf": [254, 272, 278, 282, 283, 311, 313, 314, 315, 316, 321, 324], "hf_format": [311, 313, 314, 316, 317], "hf_format_export": 150, "hf_token": 302, "hidden": [144, 303], "hidden_s": [144, 278], "hidden_st": 144, "hierarch": [57, 183, 194, 199], "hierarchi": 118, "high": [177, 202, 252, 266, 279, 280, 286, 287, 292, 298, 299, 303, 304, 320, 326], "higher": [199, 251, 252, 291, 298, 300, 301, 316, 317, 319], "highli": 340, "highlight": 186, "hip": [295, 296, 297], "histdataread": 111, "histogram": [3, 205, 241, 257, 261, 306, 307], "histogramobserv": 241, "histori": 299, "hold": [308, 337], "hook": 205, "how": [6, 7, 20, 32, 33, 50, 57, 118, 183, 192, 198, 199, 251, 253, 254, 255, 257, 260, 265, 274, 276, 278, 279, 280, 281, 282, 285, 287, 299, 304, 305, 311, 312, 313, 314, 318, 319, 326, 333, 336, 341], "howev": [140, 177, 236, 251, 296, 302, 310, 320, 337, 339, 340], "hpcai": [316, 342], "hqq": 257, "hspace": 303, "html": [61, 205], "http": [61, 138, 202, 205, 235, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 284, 286, 287, 292, 309], "hubara": 27, "hug": [302, 305, 316, 317], "huggingfac": [150, 266, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 311, 315, 316, 320, 321, 324, 338], "human": 118, "hw_emul": 303, "hw_emulation_interfac": 303, "hyper": [279, 280, 341], "hyperparamet": [202, 337, 340], "i": [1, 3, 4, 6, 7, 9, 22, 26, 27, 31, 32, 33, 39, 50, 53, 54, 55, 56, 57, 61, 63, 71, 76, 82, 85, 93, 95, 99, 105, 118, 124, 140, 149, 150, 151, 154, 177, 180, 181, 182, 183, 186, 192, 193, 194, 198, 199, 202, 203, 205, 212, 213, 215, 217, 233, 235, 236, 238, 241, 243, 244, 249, 250, 252, 253, 254, 255, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 275, 277, 278, 279, 280, 284, 285, 286, 287, 289, 290, 291, 292, 293, 294, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 314, 316, 317, 318, 319, 323, 324, 325, 326, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342], "id": [118, 316], "idea": [254, 278, 299, 337, 338, 339], "ideal": [140, 259, 295], "ident": [257, 307, 337, 342], "identifi": [199, 244, 270, 276, 278, 294, 295, 296, 297, 298, 299, 316, 336, 340], "idx": [236, 265], "ieee": 251, "ignor": [159, 236, 342], "ignore_w": 236, "ignore_warn": 57, "igpu": 298, "illustr": [251, 298], "ilsvrc": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00000236": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00000262": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00000293": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00001079": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00002138": [266, 267, 268, 269, 271, 275, 277], "ilsvrc2012_val_00002663": [266, 267, 268, 269, 271, 275, 277], "imag": [98, 217, 257, 258, 260, 266, 267, 268, 269, 271, 275, 277, 284, 286, 287, 291, 292, 298, 309], "image_classif": [284, 286, 287, 289, 292, 330], "image_fold": 260, "image_folder_1_path": 98, "image_folder_2_path": 98, "imagedataread": 260, "imagenet": [249, 266, 267, 268, 269, 270, 271, 275, 277, 319, 342], "immedi": [39, 251], "impact": [252, 257, 298], "implement": [6, 7, 32, 51, 55, 140, 149, 192, 213, 215, 217, 235, 241, 243, 257, 258, 293, 296, 297, 301, 303, 308, 319, 320, 337], "impli": 257, "implicit": 338, "implicitli": 320, "import": [9, 32, 82, 85, 95, 99, 124, 150, 177, 178, 193, 198, 233, 243, 250, 252, 253, 254, 255, 257, 258, 259, 260, 265, 279, 280, 285, 291, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 307, 319, 320, 322, 325, 333, 335, 337, 338, 341], "import_file_format": [311, 312, 313, 314], "import_hf_model": 323, "import_model": 150, "import_model_dir": [150, 311, 312, 313, 314, 317, 323, 326], "import_model_info": [150, 326], "improv": [18, 27, 61, 183, 186, 193, 198, 252, 253, 254, 255, 257, 267, 285, 286, 287, 291, 292, 298, 299, 319, 336, 339, 342], "in_channel": [234, 235], "in_feat": 337, "in_featur": [237, 337], "in_place_replace_lay": 230, "inc": [270, 276, 278, 294, 295, 296, 297, 298, 299], "includ": [1, 7, 32, 53, 55, 57, 95, 150, 177, 198, 242, 244, 249, 250, 252, 253, 257, 270, 279, 280, 282, 285, 291, 294, 296, 297, 298, 299, 300, 301, 308, 309, 316, 319, 321, 322, 324, 335, 336, 338, 341, 342], "include_auto_mp": [57, 257, 298, 299], "include_cl": [55, 57, 253, 257, 271, 279, 280, 299, 300], "include_fast_ft": [55, 57, 252, 257, 259, 279, 280, 284, 285, 295, 296, 297, 299], "include_last_offset": 236, "include_rot": [55, 254, 257, 278], "include_sq": [55, 57, 255, 257, 278, 281, 299, 300], "incom": [261, 306], "incorpor": [309, 339], "increas": [293, 303, 337], "incur": [302, 339], "indent": 203, "independ": [267, 332], "index": [149, 236, 257, 298, 323], "indic": [39, 53, 149, 150, 151, 154, 199, 236, 257, 285, 291, 298, 303, 307], "individu": [32, 259, 267, 295, 296, 297, 298, 310], "industri": 338, "inf": [4, 63, 99, 110, 244], "infer": [20, 55, 99, 150, 193, 198, 202, 249, 251, 257, 259, 263, 279, 280, 284, 286, 290, 291, 292, 294, 295, 296, 297, 298, 299, 301, 302, 309, 310, 320, 324, 333, 337, 339], "infer_shap": [55, 99], "inferdevic": [257, 259, 279, 280, 284, 285, 294, 295, 296, 297], "inferenc": [295, 296, 297], "inference_model": 20, "inferencesess": [20, 55, 259, 265, 285, 291, 294, 295, 296, 297, 301], "infin": [55, 93], "influenc": 199, "info": [57, 72, 78, 86, 118, 150, 169, 194, 199, 257, 304, 320, 323, 331], "inform": [3, 9, 39, 55, 82, 85, 99, 118, 150, 241, 243, 251, 252, 253, 254, 255, 258, 261, 266, 267, 276, 282, 291, 304, 306, 308, 315, 316, 317, 318, 319, 320, 323, 324, 326, 341], "inherit": [4, 39, 54, 63, 121, 122, 127, 128, 131, 135, 136, 139, 143, 156, 178, 194, 199, 206, 209, 222, 241], "init_list": 55, "initi": [3, 4, 7, 32, 39, 55, 63, 92, 95, 105, 114, 118, 177, 202, 235, 236, 251, 257, 279, 280, 297, 299, 316, 319, 320, 338], "initial_lr": 27, "initialize_alpha": 7, "initializers_to_convert": 92, "inp": [265, 337, 341], "inp_data_float": [22, 25], "inp_data_qu": [22, 25], "input": [1, 6, 7, 8, 13, 14, 15, 16, 17, 22, 25, 32, 33, 39, 50, 54, 55, 56, 57, 61, 71, 77, 80, 81, 82, 83, 84, 85, 87, 90, 95, 99, 104, 105, 107, 108, 110, 140, 141, 144, 150, 177, 178, 182, 183, 192, 193, 198, 199, 205, 212, 218, 219, 220, 221, 225, 228, 236, 243, 244, 251, 257, 258, 262, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 282, 283, 284, 285, 287, 290, 291, 298, 299, 301, 304, 305, 307, 309, 312, 314, 320, 332, 338, 339], "input1": 265, "input1_nam": [260, 265], "input2": 265, "input2_nam": [260, 265], "input_1": 257, "input_2": 257, "input_arg": [150, 304, 325], "input_data": [22, 265, 285], "input_data_rang": 55, "input_dict": 305, "input_fe": 265, "input_folder_path": 265, "input_height": 260, "input_histogram": 307, "input_id": [198, 302, 304, 305, 320, 333], "input_idx": 1, "input_imag": [287, 309], "input_layernorm": 341, "input_list": 305, "input_model": [71, 76, 93, 110, 111, 112, 293], "input_model_path": [93, 104, 110, 111, 112, 252, 253, 254, 255, 260, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 296, 297, 298, 300], "input_nam": [32, 39, 54, 150, 258, 260, 265, 291], "input_nod": [33, 55, 57, 257, 300], "input_node_map": 55, "input_onnx_model_path": 90, "input_path": [77, 87, 293], "input_qdq_histogram": 307, "input_ref_histogram": 307, "input_ref_histogram_absmean_ch0": 307, "input_ref_histogram_absmean_ch1": 307, "input_scal": [39, 54], "input_shap": [55, 111, 257, 260], "input_tensor": [183, 192, 199, 305, 319, 333, 337, 341], "input_tensor_devic": 243, "input_tensor_nam": 260, "input_width": 260, "inputdataset": 265, "insal": [279, 280], "insensit": 298, "insert": [101, 140, 205, 225, 251, 254, 257, 278, 293, 294, 298, 307, 319, 320, 339], "insert_clip_bfloat16_qdq": 100, "insert_stats_hook": 205, "insid": 33, "inside_layer_modul": [199, 316, 341], "insight": 307, "inspect": 186, "inspir": 336, "instabl": 294, "instad": 199, "instal": [251, 257, 259, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 285, 286, 287, 289, 292, 293, 295, 296, 297, 301, 309, 310, 317, 318, 319, 330], "instanc": [9, 22, 32, 39, 53, 55, 118, 213, 217, 251, 257, 258, 270, 280, 284, 291, 300, 302, 305, 341], "instancenorm": [53, 257, 301], "instanti": [32, 198, 236, 258, 308, 335], "instead": [3, 6, 7, 192, 199, 251, 257, 259, 270, 293, 295, 296, 297, 302, 303, 307, 319, 323], "instruct": [265, 285, 307, 309, 310, 311, 312, 315, 316, 317, 336, 340, 342], "int": [1, 3, 5, 6, 7, 13, 15, 20, 22, 26, 27, 32, 39, 54, 55, 57, 66, 72, 93, 111, 119, 141, 144, 150, 156, 159, 178, 181, 192, 194, 199, 202, 203, 205, 219, 234, 235, 236, 237, 240, 241, 242, 249, 252, 253, 254, 257, 260, 291, 341, 342], "int16": [55, 249, 257, 258, 277, 286, 287, 292, 298, 300, 342], "int16_cnn_accur": 300, "int16_cnn_default": 300, "int16_transformer_accur": 300, "int16_transformer_default": 300, "int16method": 55, "int16scal": 257, "int2": 341, "int32": [54, 150, 202, 249, 257, 258, 291, 298, 342], "int32_bia": 54, "int32bia": 257, "int4": [150, 199, 202, 249, 251, 257, 301, 304, 311, 313, 314, 316, 317, 320, 322, 324, 325, 336, 341, 342], "int4_matmul_nbit": 282, "int4perchannelspec": 199, "int4pergroupspec": 199, "int4pertensorspec": 199, "int8": [55, 150, 199, 202, 249, 251, 257, 258, 266, 267, 268, 269, 271, 274, 275, 276, 277, 278, 281, 283, 286, 287, 292, 294, 297, 298, 300, 301, 302, 303, 304, 309, 316, 318, 319, 325, 333, 336, 337, 338, 339, 340, 341, 342], "int8_cnn_accur": 300, "int8_cnn_default": 300, "int8_per_tenser_dynamic_spec": 333, "int8_per_tensor_spec": [319, 341], "int8_qdq": 276, "int8_transformer_accur": 300, "int8_transformer_default": [274, 276, 278, 281, 283, 300], "int8perchannelspec": 199, "int8pergroupspec": 199, "int8pertensorspec": [199, 341], "int_max": 291, "int_quant": 183, "intdequantfunct": 7, "integ": [7, 27, 55, 92, 183, 202, 241, 257, 264, 291, 298, 301, 302, 307, 334], "integr": [149, 249, 251, 252, 320, 327, 330, 342], "intellig": [296, 299], "intend": [6, 7, 192, 279, 280, 302], "intens": 298, "interact": 182, "intercept": 251, "interest": 303, "interfac": [213, 217, 235, 308, 318, 342], "intermedi": [55, 236, 339], "intern": [31, 33, 55, 199, 318, 342], "interpret": 118, "interv": 55, "inth": 205, "intk": 318, "intquant": 7, "intquantdequantfunct": 7, "intquantfunct": 7, "intric": 251, "introduc": [32, 33, 50, 267, 296, 297, 298, 303, 320, 336, 337, 338, 342], "introduct": [248, 303], "introductori": 258, "inttensor": 236, "intuit": 303, "int\u2461": 316, "invalid": [1, 56, 299], "invers": [71, 236, 339], "invoc": 342, "invok": [250, 312, 318], "involv": [199, 251, 258, 261, 264, 298, 304, 306, 308, 319, 320, 334], "ipu": [199, 202, 217], "ir_vers": 105, "irretriev": 251, "is_activation_symmetr": 54, "is_approximately_equ": 55, "is_batchnorm2d_nod": 228, "is_big_endian": 156, "is_cat_nod": 228, "is_clip_with_min_max": 55, "is_conv1d_nod": 228, "is_conv2d_nod": 228, "is_conv3d_nod": 228, "is_convtranspose2d_nod": 228, "is_dropout_nod": 228, "is_dynam": [198, 199, 302, 304, 319, 320, 333, 337, 338, 341], "is_kv_cach": 180, "is_larg": [71, 76], "is_layerwis": 181, "is_leaky_relu_with_alpha": 55, "is_node_needs_annot": 55, "is_ort_version_below": 55, "is_symmetr": 66, "is_weight_symmetr": 54, "isquanttyp": 257, "issu": [99, 118, 252, 291, 294, 296, 317, 319, 336], "itai": 27, "item": [1, 279, 280, 284, 316], "item_forward": 1, "iter": [1, 26, 55, 205, 252, 257, 258, 260, 299, 319, 325], "iter_x_": 1, "iteration_limit": 299, "itisquanttyp": 257, "its": [18, 25, 39, 141, 181, 193, 198, 205, 241, 243, 251, 252, 257, 258, 299, 301, 302, 303, 307, 308, 336, 337, 339], "itself": [32, 251, 326], "j": [316, 342], "jit": 22, "job": [257, 291], "join": [265, 319], "journei": 249, "jpeg": [266, 267, 268, 269, 271, 275, 277], "jpg": [284, 286, 287, 292, 293], "json": [150, 151, 154, 198, 199, 249, 254, 273, 274, 276, 278, 281, 282, 283, 304, 307, 310, 312, 316, 321, 323, 326, 335, 336, 340, 342], "json_export_config": [150, 151, 319, 320, 322, 323, 325, 326], "json_path": [154, 156, 198, 335], "jsonexporterconfig": [150, 151, 319, 320, 322, 323, 325, 326], "just": [6, 7, 118, 192, 294], "jvp": [6, 7, 192], "k": [140, 141, 249, 303, 316, 339, 342], "k_1": 303, "k_2": 303, "k_proj": [150, 316, 337, 341], "keep": [39, 95, 236, 251, 252, 279, 280, 320, 336], "keep_float_weight": 39, "keep_io_typ": 99, "keep_var": 177, "kei": [61, 149, 150, 177, 251, 252, 253, 254, 255, 257, 258, 298, 299, 302, 304, 308, 316, 320, 324, 336, 337], "kept": [298, 320], "kera": 31, "kernel": [53, 140, 250, 257, 303, 331, 336, 339, 342], "kernel_s": [234, 235], "kernel_vers": 6, "keyword": 177, "know": [118, 251], "knowledg": 118, "known": [251, 263, 266, 267, 268, 269, 270, 271, 275, 277, 291, 333], "kv": [199, 249, 304, 324, 336, 339, 342], "kv_cach": [150, 151, 180, 199, 316], "kv_cache_cfg": 341, "kv_cache_dtyp": [150, 316], "kv_cache_fp8": 341, "kv_cache_group": 151, "kv_cache_qu": 199, "kv_cache_quant_config": 199, "kv_group": 150, "kv_layer": 150, "kv_layer_nam": 150, "kv_layers_nam": 180, "kv_scale": 150, "kwarg": [6, 7, 13, 14, 15, 16, 17, 150, 177, 192, 233, 236, 237, 243], "l1": [1, 205, 279, 280, 284, 299, 337], "l1_metric": 1, "l2": [1, 20, 98, 257, 279, 280, 284, 293, 298, 299], "l2_metric": 1, "l2norm": [53, 257], "l2target": [257, 298], "label": [205, 265, 270], "lamb_in1k": [252, 259, 266, 268, 269], "lamb_in1k_adaquant_quant": [266, 268], "lamb_in1k_adaround_quant": 269, "lamb_in1k_fix": 266, "lamb_in1k_quant": [266, 268, 269], "languag": [150, 181, 249, 305, 315, 320, 324, 326, 330, 336, 337, 340, 341, 342], "language_model": [289, 305, 315, 316, 317, 330, 336, 337, 339], "larg": [3, 53, 61, 150, 181, 249, 254, 255, 257, 259, 291, 295, 299, 305, 307, 317, 324, 326, 336, 337, 340, 342], "larger": [71, 76, 236, 251, 252, 257, 302, 339], "last": [1, 236, 257, 307, 320], "latenc": [279, 280, 294], "latent": [61, 257], "later": [250, 251, 257, 291, 337], "latest": [150, 294], "latter": 298, "layer": [5, 9, 13, 14, 15, 16, 17, 27, 53, 54, 57, 140, 144, 147, 183, 198, 199, 205, 215, 217, 230, 244, 251, 252, 253, 257, 286, 287, 289, 292, 298, 299, 307, 316, 324, 326, 337, 339, 340, 341], "layer_metadata": 34, "layer_norm": 337, "layer_param": 9, "layer_qinfo": 9, "layer_quant_config": [199, 230, 341], "layer_type_quant_config": [199, 230, 341], "layernorm": [53, 257, 301, 318, 337], "layernormtyp": 162, "layerwis": [181, 183], "lead": [33, 193, 198, 251, 252, 259, 266, 267, 268, 269, 293, 295, 298, 303, 336], "leaderboard": 342, "leakag": 308, "leaki": [55, 257], "leakyrelu": [257, 301], "learn": [26, 56, 57, 150, 151, 182, 183, 193, 194, 198, 199, 249, 251, 252, 257, 286, 287, 292, 294, 295, 296, 297, 302, 319, 320, 338, 339, 342], "learnabl": 236, "learning_r": [286, 287, 292], "learningr": [252, 257, 259, 284, 285, 294, 295, 296, 297, 299], "left": [303, 307, 316], "length": [32, 236, 262, 314, 332], "less": [55, 99, 238, 251, 298, 302, 320, 337], "let": [217, 242, 302, 320, 337, 339], "leve11": 1, "level": [1, 39, 57, 118, 183, 199, 202, 244, 249, 251, 252, 257, 267, 291, 296, 298, 299, 304, 310, 339], "level1": 1, "level2": 1, "level3": 1, "levelnam": 118, "levelno": 118, "leverag": [249, 298, 299, 303, 308, 318], "lfloor": 320, "li": [251, 303], "librari": [20, 119, 150, 250, 251, 257, 291, 301, 302, 308, 318, 323, 326, 337, 338], "light": [249, 327, 330, 342], "lightweight": 318, "like": [39, 55, 61, 140, 199, 236, 249, 251, 254, 257, 284, 287, 291, 294, 299, 300, 302, 307, 308, 320, 331, 336, 342], "likelihood": 299, "limit": [55, 93, 228, 236, 257, 259, 291, 295, 298, 299, 311, 312, 315, 316, 342], "lin1": 337, "line": [118, 203, 241, 243, 250, 284, 286, 287, 292, 333, 339], "linear": [55, 140, 177, 199, 221, 230, 237, 251, 304, 316, 318, 326, 337, 340, 341, 342], "linear1": 337, "linear2": 337, "linear_lay": 318, "linear_mlp_fc1": 316, "linear_mlp_fc2": 316, "linear_o": 316, "linear_qkv": 316, "lineno": 118, "link": [250, 252, 259, 266, 268, 269, 271, 277, 295, 296, 297, 301, 305, 310, 325], "linux": [249, 258, 259, 294, 295, 296, 297, 301, 304, 341, 342], "list": [1, 3, 4, 5, 9, 20, 22, 25, 31, 32, 33, 34, 39, 50, 53, 54, 55, 57, 61, 63, 66, 74, 76, 95, 99, 107, 108, 111, 136, 149, 150, 151, 180, 193, 198, 199, 205, 213, 226, 236, 284, 298, 299, 301, 310, 311, 323, 326, 337, 339, 341], "littl": [255, 303, 320], "ll": [186, 339], "llama": [150, 254, 257, 278, 283, 289, 302, 307, 310, 311, 313, 314, 315, 318, 322, 324, 336, 337, 339, 340, 342], "llama2": [150, 272, 276, 278, 282, 283, 311, 313, 314, 316, 318, 320, 322, 323, 324, 341, 342], "llama2_checkpoint_fold": 315, "llama3": [150, 322, 342], "llamamodelwrit": 156, "llayernorm": 318, "llinear": 318, "lllyasviel": 309, "llm": [249, 254, 257, 310, 311, 313, 314, 320, 324, 336, 337, 339, 340, 342], "llm_eval": [310, 311, 312, 313, 314, 317], "llm_prune": 315, "llm_ptq": [316, 336, 337, 339], "llm_qat": 317, "llm_util": [323, 326], "lm": [313, 314], "lm_head": [316, 324, 336, 340, 341], "lmhead": [311, 313, 314], "load": [9, 55, 118, 149, 150, 180, 199, 236, 258, 260, 279, 280, 298, 304, 311, 313, 314, 319, 320, 326, 342], "load_model": [82, 85, 99], "load_param": [198, 335], "load_pre_optimization_config_from_fil": 199, "load_quant_algo_config_from_fil": 199, "load_state_dict": 319, "load_weight_and_bia": 9, "loader": [260, 304], "local": [250, 311, 313, 314, 320], "locat": [291, 301], "log": [150, 257, 291, 317, 319], "log2": 302, "log2t": 242, "log_dir": 205, "log_period": 27, "log_severity_level": [57, 194, 199, 257], "log_threshold": 192, "logger": 118, "logic": 308, "logit": 324, "logperiod": 257, "logrecord": 118, "long": [32, 236, 303, 320, 331, 337], "longer": [3, 6, 7, 192, 252, 257], "longmessag": 32, "longtensor": 236, "look": [32, 39, 107, 339], "lookup": 236, "loop": [257, 299], "lose": 255, "loss": [3, 22, 26, 98, 199, 236, 251, 252, 257, 259, 266, 268, 269, 279, 280, 286, 287, 292, 293, 295, 298, 302, 319, 336], "lossi": 251, "lost": 251, "lot": 291, "low": [36, 178, 202, 251, 267, 286, 287, 292, 296, 297, 303, 336], "lower": [183, 251, 259, 291, 295, 298, 303, 316, 337], "lower_op": 107, "lowercas": 316, "lpnormal": [53, 301], "lr_adjust": 27, "lradjust": 257, "lsoftmax": 318, "lsq": [249, 319, 342], "lsqobserv": 240, "lsqquantiz": 192, "lstm": 86, "lybrand": 181, "m": [55, 77, 87, 93, 110, 111, 112, 212, 215, 217, 218, 219, 220, 221, 236, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 292, 293, 301, 303, 320, 324, 332, 339], "m_": 303, "machin": [57, 61, 151, 183, 194, 199, 268, 269, 286, 287, 292], "made": [9, 124, 177, 233, 243], "mae": 199, "magnitud": [259, 295], "mai": [32, 39, 61, 82, 85, 118, 183, 199, 251, 257, 279, 280, 291, 307, 317, 319, 320, 323, 326, 336, 337, 340], "main": [31, 228, 254, 266, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 279, 280, 281, 284, 286, 287, 292, 308, 315, 317, 320, 338], "main_import": 150, "mainli": [278, 320], "maintain": [257, 259, 294, 295, 296, 298, 308], "mainten": 199, "major": 320, "make": [3, 6, 7, 118, 149, 181, 192, 251, 252, 253, 254, 257, 259, 260, 278, 291, 294, 295, 298, 299, 307, 312, 341], "makedir": 265, "manag": [251, 261, 306, 324], "mani": [6, 7, 20, 32, 149, 192, 205, 228, 252, 257, 284, 298, 302, 303, 337], "manipul": [31, 251], "manner": [199, 299], "mantissa": [183, 202, 257, 294, 296, 297, 303, 338], "mantissa_bit_width": 183, "manual": [250, 279, 280, 298, 301, 308], "manulli": 340, "map": [3, 31, 39, 54, 55, 99, 149, 150, 183, 184, 199, 251, 279, 280, 319, 323], "margin": [279, 280], "mark": [119, 230], "mark_exclude_nod": 226, "marku": [26, 27], "mask": 290, "match": [31, 32, 33, 50, 61, 107, 108, 140, 236, 243, 252, 265, 302, 320], "match_nod": [32, 33, 50], "mathcal": 236, "matmul": [9, 39, 61, 66, 254, 257, 278, 282, 283, 293, 298, 301, 342], "matmul10": 257, "matmul_4bit": 282, "matmul_4bits_hqq_quantized_model": 282, "matmul_4bits_quantized_model": 282, "matmul_nbit": [274, 282], "matmulconstbonli": [61, 257], "matmulnbit": [257, 274, 289, 342], "matmulnbitsparam": 257, "matmulnbitsquant": 66, "matmulqdqtoqoptransform": 50, "matric": [199, 339], "matrix": [71, 140, 141, 144, 236, 254, 257, 294, 296, 303, 339, 340], "max": [3, 5, 9, 39, 54, 55, 199, 236, 238, 241, 257, 261, 301, 303, 306, 307, 314, 320, 337, 338], "max_attempt": 119, "max_finite_v": 99, "max_loop_num": 72, "max_new_tok": 314, "max_norm": [6, 236], "max_q": 7, "max_seq_len": 316, "max_val": [238, 241], "max_valu": [33, 55], "maxdiff": 32, "maximum": [3, 32, 55, 150, 183, 238, 241, 251, 254, 257, 261, 279, 280, 290, 291, 299, 302, 303, 306, 319], "maxloopnum": 257, "maxpool": [61, 257, 301], "maybe_transpose_scal": 177, "mb": [266, 267, 268, 269, 271, 273, 274, 275, 276, 277, 281], "mbit": 6, "md": 250, "mean": [3, 55, 144, 183, 217, 236, 238, 257, 261, 262, 286, 287, 292, 294, 296, 297, 302, 303, 306, 307, 312, 316, 320, 332, 336, 337, 340], "meaning": 338, "measur": [98, 257, 279, 280, 298, 299], "mechan": 308, "meet": [53, 72, 74, 263, 284, 286, 287, 292, 299, 301, 333], "member": [156, 252, 257], "memori": [236, 251, 259, 282, 283, 286, 287, 292, 294, 295, 296, 297, 298, 300, 315, 316, 320, 342], "mention": [118, 320], "merg": [150, 151, 273, 274, 276, 281, 291, 336], "merge_batch_norm": 181, "messag": [32, 118, 257, 290], "met": 299, "meta": [272, 278, 282, 283, 302, 307, 311, 313, 314, 315, 316, 324, 336, 339, 340], "metadata": [31, 33, 225, 226, 320], "meteor": [310, 311, 313], "meth": [6, 7, 9, 124, 149, 159, 177, 192, 233, 243], "method": [3, 6, 7, 32, 39, 54, 55, 57, 138, 149, 150, 178, 183, 192, 199, 202, 236, 241, 243, 249, 250, 251, 252, 254, 257, 258, 259, 262, 263, 280, 284, 286, 287, 292, 295, 299, 304, 305, 307, 308, 318, 319, 320, 323, 332, 333, 336, 338, 339, 340, 341, 342], "method_nam": 138, "methodnam": 32, "methodologi": 336, "metirc": 270, "metric": [1, 205, 270, 279, 280, 284, 293, 298, 299, 307, 310, 312, 319], "metrics_output_dir": [310, 313, 314], "mi210": 309, "micro": [249, 257, 270, 276, 278, 294, 295, 296, 297, 298, 299], "microexpon": [267, 298], "microsc": [249, 296, 298, 316, 342], "microsoft": [284, 286, 287, 292, 315, 316, 317], "middl": 183, "might": [199, 250, 251, 252, 257, 260, 285, 290, 291, 293, 301, 302, 320, 337, 339], "migrat": [255, 257, 326], "millisecond": 118, "min": [3, 9, 39, 54, 55, 199, 238, 241, 257, 261, 301, 306, 307, 320], "min_max": 341, "min_norm": 6, "min_positive_v": 99, "min_q": 7, "min_val": [238, 241, 320], "min_valu": 55, "mind": [159, 236, 251, 336], "mini": [236, 257, 312, 315, 316, 317, 342], "minim": [3, 241, 251, 252, 257, 259, 261, 279, 280, 286, 287, 292, 295, 298, 303, 306, 320, 336, 337], "minimum": [3, 55, 183, 199, 238, 241, 250, 251, 254, 257, 261, 301, 306, 319, 320], "minmax": [3, 57, 249, 254, 257, 258, 259, 279, 280, 284, 286, 287, 292, 294, 295, 296, 297, 298, 299, 301, 304, 306, 318, 342], "minmaxcalibrat": 3, "minms": [3, 249, 252, 253, 255, 257, 258, 279, 280, 285, 286, 287, 290, 292, 299, 300, 301, 342], "minmse_mod": 3, "minmsemod": 257, "minor": [266, 267, 268, 269, 302], "minut": [250, 331], "miopen_batch_norm": 228, "misalign": [193, 198], "mistral": [316, 342], "mistralai": [315, 316, 342], "mit": [270, 276, 278, 294, 295, 296, 297, 298, 299], "mitig": [254, 266, 268, 269, 278], "mix": [39, 57, 150, 249, 257, 259, 289, 295, 300, 325, 342], "mixed_precis": 277, "mixedprecisiontensor": [257, 298], "mixtral": [315, 316, 342], "mkdir": [266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 292, 317], "mlcommon": 309, "mlp": [199, 307, 336, 337, 341], "mmlu_manag": 311, "mobil": 252, "mobilenet": [253, 319], "mobilenetv2": 319, "mobilenetv2_050": [252, 259, 266, 268, 269], "mode": [3, 4, 6, 7, 9, 25, 39, 54, 55, 57, 61, 63, 124, 150, 177, 192, 198, 199, 202, 228, 233, 236, 243, 249, 257, 298, 304, 307, 308, 310, 312, 330, 331, 341, 342], "model": [1, 3, 4, 5, 8, 9, 18, 20, 22, 31, 32, 33, 34, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 90, 91, 92, 95, 99, 104, 106, 107, 108, 110, 111, 112, 114, 121, 122, 124, 127, 128, 131, 135, 136, 139, 143, 147, 150, 151, 154, 177, 180, 181, 182, 183, 193, 194, 198, 199, 202, 205, 211, 215, 217, 225, 226, 228, 230, 233, 243, 248, 252, 253, 254, 255, 256, 257, 258, 261, 263, 294, 298, 300, 303, 304, 305, 306, 307, 308, 318, 322, 323, 325, 326, 330, 333, 336, 337, 339, 342], "model_arg": [311, 313, 314], "model_config": 150, "model_copi": 337, "model_decoder_lay": [199, 316, 337, 339, 341], "model_dir": [150, 315, 316, 320, 323, 324, 336, 339], "model_export": [316, 317, 319, 324], "model_file_path": [198, 319, 335], "model_id": 309, "model_info_dir": [150, 323, 326], "model_input": [55, 56, 61, 257, 301], "model_input_nam": 258, "model_max_length": 316, "model_nam": [154, 156, 266, 268, 269, 270, 271, 277, 319], "model_name_exclude_layers_map": 316, "model_name_kv_layers_map": 316, "model_name_or_path": [272, 273, 274, 276, 278, 281, 282, 283], "model_name_pattern_map": 316, "model_output": [55, 56, 61, 257, 301], "model_output_path": 291, "model_path": [3, 22, 55, 99, 111, 278, 318], "model_reload": [311, 313, 314, 317], "model_state_dict": [150, 180], "model_transform": 30, "model_transformer_test": 30, "model_trust_remote_cod": 317, "model_typ": [150, 316, 320, 322, 335], "model_util": 113, "modelexport": [150, 182, 304, 319, 320, 322, 325, 326], "modelimport": [150, 326], "modeloptim": 25, "modelproto": [4, 5, 8, 9, 20, 22, 31, 34, 39, 51, 53, 54, 55, 61, 63, 66, 71, 72, 74, 76, 78, 80, 81, 82, 83, 84, 85, 86, 91, 92, 95, 99, 106, 107, 108, 114, 290], "modelprun": 193, "modelquant": [56, 182, 198, 252, 253, 254, 255, 258, 259, 260, 285, 294, 295, 296, 297, 298, 300, 302, 304, 307, 319, 320, 333, 337, 338], "modeltransform": 31, "modeltransformertest": 32, "modelwrit": 156, "modern": [259, 295, 296], "modif": [336, 342], "modifi": [55, 107, 108, 110, 118, 140, 198, 205, 213, 236, 252, 257, 301, 316, 319], "modified_annotate_input": 55, "modul": [257, 258, 261, 291, 304, 306, 307, 316, 319, 320, 333, 337, 342], "module2inspect": 341, "module_config": 230, "module_nam": 205, "modulelist": [244, 337], "moe": [180, 316, 342], "momentum": [228, 235], "more": [6, 7, 61, 150, 183, 192, 193, 198, 236, 241, 242, 251, 252, 254, 257, 258, 259, 263, 267, 278, 279, 280, 284, 293, 295, 296, 297, 298, 299, 302, 303, 304, 310, 319, 320, 324, 333, 341, 342], "moreov": [205, 291, 323], "most": [182, 257, 259, 266, 267, 268, 269, 271, 275, 277, 291, 294, 295, 307, 310, 316, 320, 340], "mostcommon": 257, "motiv": 307, "move": [3, 254, 257, 270], "moving_averag": 3, "mqa": 342, "msbuild": [250, 301], "mse": [183, 199, 241, 249, 257, 304, 306, 336, 342], "msec": 118, "much": [140, 236, 250, 252, 255, 257, 320, 339], "mul": [108, 257, 287, 301, 337], "mulqdqtoqoptransform": 50, "multi": [31, 202, 241, 243, 249, 279, 280, 311, 312, 313, 314, 337], "multi_gpu": [311, 313, 314, 315, 316], "multilay": 199, "multimod": 311, "multipl": [3, 22, 33, 53, 119, 140, 236, 249, 257, 259, 295, 296, 314, 315, 316, 320, 342], "multipli": [54, 140, 257], "multiprocess": 205, "must": [6, 7, 9, 32, 54, 124, 149, 150, 156, 177, 192, 233, 236, 243, 254, 257, 291, 293, 301, 302, 305, 312, 322, 337], "mx": [54, 199, 202, 249, 251, 289, 296, 336, 341, 342], "mx4": [267, 296, 298], "mx6": [202, 249, 267, 296, 298, 341, 342], "mx6spec": 199, "mx9": [202, 249, 267, 298, 341, 342], "mx9spec": 199, "mx_element_dtyp": [192, 199, 302], "mxattribut": [257, 297], "mxfixneuron": [6, 257, 297], "mxfp4": [297, 298, 316], "mxfp4e2m1": 267, "mxfp6": 297, "mxfp6_e2m3": 298, "mxfp6_e3m2": 298, "mxfp6e2m3": [267, 316], "mxfp6e3m2": [267, 316], "mxfp8": 297, "mxfp8_e4m3": 298, "mxfp8_e5m2": [298, 342], "mxfp8e4m3": [267, 316], "mxfp8e5m2": [267, 316], "mxint8": [267, 297, 298, 316], "mxquantdequantfunct": 6, "mxquantiz": 6, "mxspec": 199, "mx\u2462": 316, "my_collate_fn": 305, "mymodel": 337, "mysubmodul": 337, "n": [55, 140, 141, 217, 228, 236, 257, 303, 309, 320, 339], "n01440764": [266, 267, 268, 269, 271, 275, 277], "n01443537": [266, 267, 268, 269, 271, 275, 277], "n15075141": [266, 267, 268, 269, 271, 275, 277], "n_bin": 205, "na": [319, 342], "nagel": [26, 27, 181], "naiv": 307, "name": [1, 3, 5, 32, 39, 53, 54, 55, 57, 61, 102, 107, 108, 114, 118, 138, 150, 151, 154, 177, 199, 205, 243, 244, 250, 251, 252, 257, 258, 260, 265, 266, 267, 268, 269, 270, 271, 272, 277, 291, 294, 296, 297, 298, 301, 303, 304, 307, 309, 315, 316, 317, 320, 323, 331, 335, 336, 337, 339, 341, 342], "name_to_arr": 3, "named_modul": 230, "nan": [99, 244], "narrow": 337, "nativ": [228, 257, 291, 294, 301, 320, 342], "native_batch_norm": 228, "native_funct": 228, "natur": 296, "navig": [244, 339], "nbit": 257, "nchw": [57, 87, 257], "nchw_onnx_model_path": 293, "ndarrai": [1, 3, 8, 9, 13, 20, 22, 25, 55, 71, 99, 111, 114, 205], "nearest": [202, 302], "necessari": [32, 55, 56, 193, 198, 225, 250, 254, 257, 258, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 291, 292, 293, 302, 308, 317, 339], "necessarili": 303, "necessit": [251, 263, 333], "need": [1, 6, 7, 9, 22, 32, 33, 55, 71, 82, 85, 95, 99, 118, 140, 150, 154, 180, 183, 186, 192, 213, 215, 217, 226, 236, 249, 251, 252, 257, 258, 259, 263, 265, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 284, 293, 294, 295, 296, 297, 298, 299, 301, 302, 304, 309, 316, 318, 319, 320, 326, 333, 337, 339, 340], "needs_input_grad": [6, 7, 192], "neg": [99, 110, 183], "nest": [9, 124, 147, 169, 177, 233, 243, 244], "network": [9, 124, 150, 177, 181, 198, 202, 233, 243, 244, 249, 251, 252, 259, 286, 287, 290, 292, 294, 295, 298, 335, 337, 338, 339], "neural": [9, 27, 54, 124, 177, 181, 198, 202, 233, 243, 244, 249, 251, 252, 259, 286, 287, 292, 294, 295, 298, 338], "neuron": 54, "new": [3, 5, 31, 32, 33, 34, 39, 50, 55, 162, 243, 244, 250, 251, 257, 267, 275, 284, 286, 292, 298, 301, 302, 308, 314, 316, 320, 339, 340], "new_model": [82, 85, 99], "new_modul": 244, "new_onnx_model": [82, 85, 95, 99], "new_tensor": 99, "new_valu": 243, "newer": 250, "newli": 236, "next": [1, 55, 111, 258, 260, 319, 320, 325], "nhwc": [57, 87, 257], "nhwc_onnx_model_path": 293, "nibbl": 320, "night": 309, "nn": [9, 22, 25, 121, 122, 124, 127, 128, 131, 135, 136, 138, 139, 140, 143, 144, 147, 150, 182, 193, 198, 199, 205, 217, 228, 230, 243, 244, 304, 316, 318, 319, 337, 342], "nnapi": 61, "nndct": 242, "nndct_fix_kernel": 242, "nndctfixneron": 242, "no_cuda": [272, 273, 274, 276, 281, 282, 283], "no_grad": [236, 337], "no_merge_realq_config": [150, 323, 326], "node": [3, 5, 9, 13, 31, 32, 33, 39, 50, 53, 54, 55, 57, 61, 82, 85, 92, 95, 107, 108, 150, 217, 225, 226, 228, 251, 257, 286, 287, 290, 291, 292, 293, 298, 301, 325, 337], "node_block_list": 99, "node_metadata": [31, 51], "node_nam": 54, "nodedef": 33, "nodeproto": [9, 33, 39, 55, 107, 108], "nodes_list": 55, "nodes_to_exclud": [4, 5, 39, 53, 54, 55, 57, 61, 63, 66, 74, 257, 300], "nodes_to_quant": [4, 5, 39, 53, 54, 55, 57, 61, 63, 74, 257, 300], "nodes_to_remov": 54, "nodetre": [32, 33, 50], "nodetyp": 31, "noinputqdqshar": 257, "non": [6, 7, 61, 149, 192, 205, 236, 290, 307, 337], "none": [1, 3, 4, 5, 6, 7, 8, 9, 13, 20, 22, 25, 27, 31, 33, 34, 39, 51, 53, 54, 55, 56, 61, 63, 66, 95, 99, 107, 108, 110, 111, 114, 118, 119, 124, 138, 140, 141, 144, 150, 151, 154, 169, 177, 178, 180, 182, 192, 193, 194, 198, 199, 205, 213, 215, 217, 218, 219, 220, 221, 230, 233, 235, 236, 240, 241, 242, 243, 244, 252, 253, 254, 255, 257, 258, 260, 284, 285, 291, 299, 300, 305, 316, 337, 341], "nonoverflow": [3, 55, 249, 257, 258, 299, 301, 342], "nonscaledfakequant": 243, "nonscaledfakequantizefunct": 192, "nonscaledrealquant": 178, "nonscaledrealquantizefunct": 192, "norm": [9, 26, 53, 236, 257, 298, 299], "norm_typ": 236, "normal": [53, 144, 252, 257, 260, 291, 305, 320, 337], "normalization_lay": 318, "notat": 244, "note": [1, 39, 53, 56, 150, 198, 217, 228, 236, 250, 257, 266, 268, 287, 291, 295, 296, 307, 308, 309, 310, 312, 319, 320, 326, 337, 338], "noteworthi": 311, "notic": [301, 339], "novelti": 303, "novocab": 159, "now": [193, 198, 250, 279, 280, 302, 312, 316, 320, 325, 335, 338, 340, 341, 342], "np": [1, 71, 265], "np_arrai": 99, "npu": [53, 54, 57, 257, 294, 298, 300, 301, 310, 312], "npu_cnn": [249, 257, 291, 301], "npu_transform": [249, 257, 301], "npulimitationcheck": 257, "npy": [260, 270, 293], "num": 309, "num1": 215, "num2": 215, "num3": 215, "num4": 215, "num_batch": 27, "num_bin": 3, "num_calib_data": [278, 315, 316, 324], "num_channel": 144, "num_embed": 236, "num_eval_data": 314, "num_fewshot": [311, 312], "num_it": [286, 287, 292], "num_iter": 27, "num_quantized_bin": 3, "num_sampl": 265, "num_work": 265, "numbatch": 257, "number": [3, 6, 7, 32, 33, 50, 118, 144, 183, 192, 202, 205, 236, 251, 252, 254, 255, 257, 259, 260, 266, 279, 280, 286, 287, 292, 294, 295, 296, 297, 299, 302, 303, 309, 311, 314, 338, 339], "numer": [118, 251, 257, 267, 294, 296, 297, 298], "numiter": [252, 257, 259, 284, 285, 294, 295, 296, 297, 299], "numpi": [1, 8, 9, 13, 20, 22, 25, 55, 71, 99, 111, 114, 205, 257, 265], "numtarget": 257, "nvidia": 285, "o": [199, 265, 284, 286, 287, 292, 319, 324, 339], "o_proj": [199, 339, 341], "obj": 147, "object": [3, 6, 7, 32, 55, 56, 82, 85, 95, 99, 147, 150, 177, 192, 193, 198, 213, 257, 266, 267, 268, 269, 271, 275, 277, 287, 289, 295, 296, 297], "observ": [183, 198, 199, 243, 251, 302, 304, 319, 320, 333, 337, 338, 341], "observer_cl": [198, 199, 302, 304, 319, 320, 333, 337, 338, 341], "observer_method": 341, "observerbas": [199, 241, 341], "obtain": [3, 22, 55, 93, 257, 261, 272, 278, 282, 283, 302, 315, 316, 319, 320], "occupi": 251, "occur": [251, 259, 291, 295], "ocp": [297, 342], "ocp_fp8_e4m3": 304, "ocp_fp8e4m3": 342, "ocp_mxfp4": 304, "ocp_mxfp6": 304, "ocp_mxfp8_e4m3": 304, "ocp_mxint8": 304, "off": [249, 250, 279, 280, 291], "offer": [251, 252, 257, 263, 285, 292, 324, 333, 340], "offici": [228, 305, 320], "offlin": [310, 339], "offset": [236, 251], "often": [236, 251, 261, 287, 293, 305, 306, 336, 339], "oga": [278, 310], "oga_fp32_model": 278, "oga_refer": 312, "oga_valid": 278, "ok": [250, 301], "onc": [257, 308, 324], "one": [1, 22, 39, 95, 150, 217, 236, 241, 244, 250, 251, 257, 262, 267, 279, 280, 284, 291, 293, 298, 303, 307, 308, 320, 323, 332, 333, 337, 338, 339, 340], "ones": [182, 236, 251, 299, 311], "onli": [1, 9, 39, 54, 61, 82, 85, 99, 118, 138, 140, 150, 159, 183, 198, 199, 205, 215, 228, 236, 241, 249, 250, 252, 257, 258, 259, 260, 263, 270, 278, 283, 291, 293, 294, 301, 303, 304, 307, 310, 312, 317, 319, 320, 321, 322, 323, 325, 326, 333, 335, 336, 337, 338, 339, 341, 342], "onlin": [199, 336, 339], "onnx": [150, 151, 182, 217, 226, 248, 250, 251, 253, 254, 255, 256, 257, 260, 261, 262, 263, 266, 267, 272, 273, 274, 275, 276, 277, 281, 282, 285, 299, 301, 304, 319, 320, 321, 330, 342], "onnx_aten": 150, "onnx_aten_fallback": 150, "onnx_evalu": 287, "onnx_export_config": [150, 151, 323, 326], "onnx_fallthrough": 150, "onnx_format": [311, 312, 313, 314], "onnx_ml_pb2": [11, 55], "onnx_model": [8, 9, 20, 22, 82, 85, 95, 99, 108, 274, 276, 281], "onnx_model_path": [71, 76, 259, 265, 294, 295, 296, 297, 301], "onnx_model_path_with_init_shar": 95, "onnx_model_path_without_init_shar": 95, "onnx_path": [1, 280, 284, 309], "onnx_pb": [13, 66, 99], "onnx_valid": [266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 281, 282, 283], "onnx_validate_with_custom_op": 266, "onnxexporterconfig": [150, 151, 323, 326], "onnxmltool": 99, "onnxquant": 39, "onnxquantizedmodel": 72, "onnxruntim": [3, 4, 20, 22, 39, 54, 55, 56, 61, 63, 250, 254, 257, 258, 259, 260, 265, 279, 280, 284, 285, 286, 287, 290, 291, 292, 294, 295, 296, 297, 299, 301, 324, 342], "onnxruntime_genai": 324, "onnxruntime_pybind11_st": 290, "onnxruntimeerror": 290, "onnxsim": [257, 293, 301], "onnxtxt": [88, 89], "onto": 298, "op": [3, 6, 7, 53, 54, 55, 78, 86, 93, 104, 107, 108, 109, 192, 211, 217, 218, 219, 220, 221, 228, 257, 259, 294, 296, 304, 337], "op_block_list": 99, "op_level_per_channel": 39, "op_typ": [33, 95, 257], "op_types_to_calibr": 3, "op_types_to_exclude_output_quant": 54, "op_types_to_quant": [4, 5, 39, 53, 54, 55, 57, 61, 63, 257, 300], "open": 342, "openllm": 317, "oper": [3, 5, 6, 7, 9, 39, 53, 54, 55, 57, 61, 82, 85, 106, 107, 108, 118, 140, 150, 151, 192, 199, 202, 205, 228, 236, 249, 250, 251, 257, 258, 259, 291, 293, 294, 295, 296, 297, 298, 301, 303, 304, 316, 318, 320, 323, 325, 337, 339, 342], "operand": 118, "operaton": 257, "operator_export_typ": 150, "operatorexporttyp": 150, "opset": [90, 150, 257, 291, 301], "opset_vers": [150, 291], "opt": [198, 255, 274, 281, 289, 304, 315, 316, 318, 333, 337, 338, 342], "optim": [3, 5, 7, 8, 9, 22, 25, 26, 39, 54, 57, 120, 140, 150, 181, 183, 193, 198, 199, 202, 226, 236, 249, 251, 252, 253, 254, 255, 257, 258, 259, 263, 278, 279, 280, 285, 286, 287, 288, 291, 292, 295, 296, 298, 299, 300, 304, 317, 318, 319, 333, 336, 337, 339, 340, 342], "optim_algo": 27, "optim_devic": 27, "optimalgorithm": [252, 257, 259, 284, 285, 294, 295, 296, 297, 299], "optimdevic": [252, 257, 259, 279, 280, 284, 285, 294, 295, 296, 297], "optimize_model": [55, 57, 257, 290, 300, 301], "optimize_modul": 22, "optimized_rotation_path": 199, "optimum": [272, 273, 274, 276, 281, 282, 283, 310], "option": [3, 22, 25, 39, 53, 54, 56, 57, 61, 118, 149, 150, 151, 177, 182, 183, 194, 199, 213, 218, 219, 220, 221, 236, 249, 250, 252, 253, 254, 255, 257, 259, 260, 282, 294, 295, 296, 297, 298, 300, 301, 304, 307, 308, 310, 316, 317, 319, 323, 336, 340], "optpassbas": 213, "optpassmanag": 213, "optypepattern": [32, 33, 50], "optypestoexcludeoutputquant": 257, "order": [22, 32, 140, 150, 177, 199, 257, 279, 280, 336, 337], "ordereddict": [33, 177], "org": [202, 205, 235, 278], "organ": [260, 266, 267, 268, 269, 270, 271, 275, 277], "origin": [8, 9, 22, 25, 26, 32, 33, 34, 39, 50, 55, 71, 150, 151, 198, 202, 251, 252, 257, 258, 263, 279, 280, 291, 303, 304, 312, 323, 333, 337, 340], "ort": [55, 257, 259, 265, 285, 291, 294, 295, 296, 297, 301], "orthogon": [71, 339], "ortonnxquant": 39, "ortqdqquant": 54, "os_cpu": 55, "osscar": [194, 315], "osscarconfig": [135, 194], "osscarprocessor": 135, "other": [6, 7, 9, 33, 53, 55, 124, 138, 140, 150, 177, 192, 233, 243, 251, 252, 257, 259, 264, 291, 294, 295, 296, 297, 298, 302, 303, 304, 305, 307, 308, 316, 320, 323, 324, 325, 326, 331, 334, 335, 336, 337, 342], "other_type_lay": 215, "otherwis": [33, 39, 55, 118, 177, 199, 244, 251, 252, 257, 270, 294, 296, 297, 298, 299, 303, 304, 307, 331, 335, 336, 337], "our": [320, 321], "out": [118, 236, 265, 284, 337], "out_channel": [234, 235], "out_data_float": [22, 25], "out_feat": 337, "out_featur": [237, 337], "out_proj": 337, "outlier": [254, 255, 261, 278, 306, 339], "outlin": [284, 286, 287, 292, 308, 336], "output": [1, 3, 6, 7, 8, 20, 25, 26, 39, 54, 55, 56, 57, 61, 71, 77, 78, 80, 81, 82, 83, 84, 85, 86, 87, 90, 95, 104, 107, 108, 110, 112, 138, 140, 144, 150, 177, 178, 183, 192, 199, 205, 225, 236, 251, 252, 257, 261, 265, 270, 279, 280, 282, 283, 284, 285, 287, 291, 293, 298, 299, 306, 307, 309, 310, 312, 324, 337], "output_dir": [55, 150, 291, 316, 317, 320, 322, 323, 324, 325, 326], "output_file_path": 154, "output_imag": 287, "output_index": 20, "output_model": [93, 110, 293], "output_model_path": [93, 104, 110, 252, 253, 254, 255, 260, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 285, 286, 287, 291, 292, 293, 294, 296, 297, 298, 300], "output_nam": [107, 108, 150, 265, 291], "output_nod": [55, 57, 257, 300], "output_onnx_model_path": 90, "output_pad": [220, 234], "output_path": [77, 87, 111, 112, 278, 293], "output_tensor": [183, 199, 319, 341], "outputindex": [257, 298], "outputqdq": 257, "outputs_path": 312, "outsid": [33, 205], "over": [57, 151, 183, 194, 199, 205, 217, 236, 296, 297, 303, 307, 331, 337], "overal": [252, 257, 297, 298, 337], "overfit": 257, "overflow": [181, 251, 257, 261, 294], "overhead": [282, 283, 339], "overrid": [6, 7, 32, 192, 324], "overridden": [6, 7, 57, 192, 199, 257], "overwrit": 149, "own": [32, 241, 243, 302, 305, 321], "p": [236, 266, 267, 268, 269, 273, 274, 275, 276, 281, 284, 286, 287, 292, 317, 324], "pack": [55, 150, 177, 178, 342], "pack_method": [150, 151, 177, 323, 326], "pack_qinfo": 177, "pack_zero_point": 177, "packag": [250, 252, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 292, 310, 317, 324], "pad": [9, 218, 219, 220, 234, 235, 236, 257, 301, 302], "padding_idx": 236, "padding_mod": [234, 235], "padding_sid": 316, "page": [272, 278, 282, 283, 315, 316, 324], "pair": [7, 51, 54, 61, 107, 252, 253, 254, 255, 257, 278, 294, 298, 320, 325], "paper": [26, 27, 181, 202, 252, 296, 337, 338], "para": 55, "param": [7, 9, 20, 22, 25, 26, 55, 78, 82, 85, 86, 90, 91, 93, 95, 99, 104, 106, 178, 194, 199, 212], "param_is_symmetr": 9, "param_typ": 183, "paramet": [3, 7, 9, 22, 25, 26, 27, 39, 53, 55, 56, 57, 99, 107, 108, 110, 124, 147, 150, 151, 177, 183, 185, 186, 193, 194, 198, 199, 202, 233, 236, 241, 243, 244, 251, 252, 253, 254, 255, 257, 258, 260, 261, 262, 263, 266, 279, 280, 284, 285, 286, 287, 291, 292, 296, 299, 301, 302, 306, 308, 319, 326, 332, 333, 335, 336, 342], "params_dict": 169, "paramt": 7, "paramtyp": 183, "parent": [9, 39, 124, 177, 178, 233, 243, 337], "pars": [228, 308], "parse_options_to_param": 22, "part": [3, 32, 180, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 277, 278, 281, 283, 298, 316, 320, 326, 336, 342], "partial": [205, 319, 342], "particular": [199, 249, 312], "particularli": [259, 261, 294, 295, 296, 298, 306], "pass": [6, 7, 32, 118, 140, 159, 192, 205, 213, 215, 217, 236, 241, 251, 267], "passmanag": [213, 217], "passresu": 213, "past": [250, 282, 301], "path": [1, 3, 20, 22, 55, 56, 61, 71, 76, 90, 99, 104, 110, 114, 119, 147, 150, 154, 156, 159, 181, 198, 199, 205, 250, 254, 257, 258, 265, 266, 267, 268, 269, 271, 275, 277, 291, 298, 301, 305, 307, 309, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 331, 342], "pathdataread": 55, "pathlib": [3, 55, 61, 150, 154, 156, 159, 205], "pathnam": 118, "pattern": [31, 32, 33, 50, 55, 226, 252, 340], "pdf": [202, 235], "peopl": 309, "pep": 159, "per": [9, 39, 54, 57, 61, 124, 177, 199, 233, 236, 243, 249, 251, 253, 257, 258, 262, 268, 269, 291, 299, 302, 303, 304, 307, 319, 320, 332, 336, 339, 340, 341, 342], "per_channel": [4, 39, 54, 55, 57, 61, 63, 199, 202, 257, 300, 341], "per_gpu_eval_batch_s": [272, 273, 274, 276, 281, 282, 283], "per_group": [150, 198, 199, 202, 302, 320, 322, 333, 338, 341], "per_sample_weight": 236, "per_tensor": [199, 202, 304, 319, 333, 337, 341], "perblockbfpobserv": [241, 338], "perblockmxobserv": [241, 251, 302, 341], "percdamp": 257, "percent": [257, 312], "percentag": [199, 257], "percentage_of_processed_input": 181, "percentil": [3, 241, 249, 257, 258, 279, 280, 284, 290, 299, 301, 304, 306, 342], "percentilecalibrat": 3, "perceptron": [199, 337], "perchannel": [112, 257, 316], "perchannelminmaxobserv": [241, 251, 320, 333, 341], "perfectli": 331, "perform": [6, 7, 34, 39, 54, 61, 66, 118, 140, 177, 178, 192, 193, 198, 199, 236, 249, 251, 252, 253, 257, 259, 260, 261, 265, 266, 268, 269, 279, 280, 290, 291, 295, 298, 299, 300, 302, 303, 304, 306, 307, 317, 318, 319, 320, 336, 338, 339, 342], "pergroup": 316, "pergroupminmaxobserv": [198, 241, 341], "perhap": [9, 307], "permiss": [272, 278, 282, 283, 315, 316, 324], "perplex": [310, 311, 314, 320, 339, 340], "persist": [177, 336], "pertensorhistogramobserv": 241, "pertensorhistogramobserverpro": 241, "pertensorminmaxobserv": [241, 251, 304, 319, 320, 333, 337, 341], "pertensormseobserv": [241, 341], "pertensorpercentileobserv": [241, 341], "peun": 193, "phase": [285, 331], "phi": [315, 316, 317, 342], "phi3": 312, "pick": 251, "pickl": 307, "pictur": 270, "pilev": 305, "pileval_for_awq_benchmark": [316, 324, 336], "pin_memori": 265, "pip": [250, 257, 293, 301, 310, 317, 324], "pipelin": [34, 51, 324], "place": [32, 118, 202, 236, 249, 258, 260, 291, 304, 320, 333, 342], "placeholderobserv": [241, 341], "plan": 257, "platform": [248, 249, 250, 257, 259, 279, 280, 291, 295, 301, 340], "pleas": [33, 61, 177, 198, 250, 251, 252, 257, 259, 266, 267, 268, 269, 271, 275, 277, 294, 295, 296, 297, 298, 303, 304, 305, 307, 310, 312, 317, 320, 331, 333, 335, 336, 337, 341], "plot": [205, 307], "plu": [316, 342], "png": [284, 286, 287, 292, 293, 307], "po": [55, 93], "pof2": [72, 93, 202, 341], "point": [3, 7, 31, 39, 54, 55, 93, 118, 183, 202, 249, 251, 252, 257, 258, 261, 264, 265, 267, 279, 280, 286, 287, 289, 291, 292, 294, 295, 296, 297, 298, 303, 304, 306, 307, 316, 319, 323, 326, 334, 339, 342], "pointwis": 337, "pool": [53, 217, 257], "poorli": [266, 268, 269], "popular": [308, 320, 321], "porint": 1, "portion": 118, "pos2scal": [55, 93], "pos_rang": 55, "posit": [55, 93, 99, 110, 177, 236, 257], "possess": 341, "possibl": [1, 150, 186, 228, 252, 255, 261, 284, 298, 299, 303, 306, 320], "post": [26, 27, 181, 199, 215, 249, 251, 252, 257, 263, 279, 280, 284, 285, 286, 287, 292, 319, 320, 328, 333, 337, 340, 342], "post_attention_layernorm": 341, "postcondit": 213, "postprocess": 287, "potenti": [193, 198, 251, 252, 279, 280, 299], "power": [3, 183, 199, 202, 249, 257, 261, 275, 286, 287, 292, 296, 298, 302, 303, 308, 319, 342], "poweroftwomethod": [3, 55, 57, 252, 253, 254, 255, 257, 279, 280, 285, 290, 299, 300, 301], "powoftwocalibrat": 3, "powoftwocollector": 3, "ppl": [272, 273, 274, 276, 278, 281, 282, 283, 315], "pprint": 203, "practic": [249, 252, 289, 298, 337, 338], "practition": 298, "pre": [7, 118, 120, 181, 183, 185, 199, 249, 251, 252, 257, 258, 260, 279, 280, 299, 304, 319, 337, 340, 342], "pre_optimization_config_file_path": 150, "pre_quant_opt_config": [139, 143, 183, 199, 337], "pre_quant_optim": 337, "pre_quantization_optim": [150, 336, 339], "prec": [271, 275, 277], "preced": [53, 337], "precis": [36, 39, 57, 177, 178, 183, 249, 251, 255, 257, 258, 259, 263, 266, 267, 286, 287, 289, 292, 294, 295, 296, 297, 299, 300, 303, 319, 324, 326, 333, 339, 342], "precondit": [213, 215, 217], "predefin": [279, 280, 299], "predict": [261, 265, 303, 306, 307, 310, 312], "prefer": 284, "prefix": [1, 177, 251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337], "prelu": [257, 301], "prepar": [150, 198, 226, 291, 318, 319], "preparatori": 118, "prepare_calib_dataset": 319, "prepare_data": [266, 267, 268, 269, 271, 275, 277], "prepare_data_load": 319, "prepared_model": 319, "preprocess": [180, 181, 258, 260, 284, 286, 287, 292, 342], "preprocess_import_info": 180, "preprocss": 270, "prequantoptconfig": 199, "presenc": 159, "present": [217, 285, 319], "preserv": [252, 259, 266, 286, 287, 292, 295, 296, 298, 300, 303, 307], "pretrain": [310, 311, 313, 314, 319], "pretrained_config": 150, "pretrainedconfig": 150, "pretrainedmodel": 150, "pretti": 203, "prev_op": [337, 341], "prevent": [199, 257], "previou": [252, 257, 263, 294, 298, 300, 333, 337], "primari": 308, "primarili": [159, 257, 264, 318, 334, 336], "print": [32, 55, 57, 102, 203, 241, 243, 257, 265, 285, 316, 337], "print_a16w8_a8w8_nod": [100, 293], "print_quantize_dynamic_info": 55, "print_quantize_info": 55, "print_summari": 57, "prior": 341, "priorit": [279, 280], "prioriti": [1, 298, 299], "problem": 294, "proceduc": 217, "process": [1, 7, 39, 54, 56, 118, 182, 183, 193, 194, 198, 199, 217, 250, 251, 252, 257, 260, 263, 266, 284, 285, 293, 295, 296, 297, 298, 299, 301, 308, 316, 319, 320, 324, 333, 337, 342], "process_model_transform": 230, "processed_data": 258, "prod": 236, "produc": [32, 33, 50, 107, 108, 193, 198, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 298, 299, 319], "product": [251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337], "profil": 298, "program": [27, 250, 301], "project": [150, 199, 254, 278, 337, 339, 342], "prompt": [250, 301], "proper": 336, "properli": [279, 280], "properti": 33, "proportion": [286, 287, 292], "propos": [26, 254, 278], "proprietari": 326, "protect": 257, "proto": [20, 159, 257], "protobuf": [290, 291], "protocol": 159, "provabl": 181, "provid": [3, 53, 55, 56, 57, 76, 121, 122, 127, 128, 131, 135, 136, 139, 143, 149, 150, 156, 177, 178, 182, 193, 194, 198, 199, 206, 209, 222, 228, 230, 241, 243, 244, 248, 249, 251, 252, 256, 257, 258, 259, 265, 266, 267, 268, 269, 271, 275, 277, 284, 285, 286, 287, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 305, 307, 308, 309, 310, 312, 316, 317, 318, 319, 320, 323, 324, 325, 336, 340, 341, 342], "prune": [128, 135, 328, 329, 330, 342], "pruner": 329, "pruning_algo": 315, "pruning_algo_config": 135, "pruning_model": 193, "psnr": [1, 98, 279, 280, 284, 293], "psnr_metric": 1, "pt": [198, 287, 302, 304, 307, 320, 333], "pth": [150, 198, 319, 321, 326, 335], "pth_path": [198, 335], "ptq": [182, 226, 249, 251, 252, 253, 254, 255, 256, 279, 280, 284, 285, 286, 287, 292, 320, 328, 330, 342], "pure": [211, 257, 301], "purpos": [199, 299, 301, 338], "push": 298, "put": [316, 337], "pwd": 309, "py": [80, 81, 82, 83, 84, 85, 90, 95, 98, 104, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 292, 303, 305, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 324, 336, 339], "py39": 331, "python": [77, 80, 81, 82, 83, 84, 85, 87, 90, 93, 95, 98, 104, 110, 111, 112, 250, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 291, 292, 293, 299, 300, 301, 309, 311, 312, 313, 314, 315, 316, 317, 318, 339, 341], "python3": [315, 316, 319, 324, 336], "pythonpath": 309, "pytorch": [9, 13, 119, 120, 150, 151, 182, 193, 194, 198, 199, 202, 205, 228, 241, 248, 250, 251, 252, 257, 285, 287, 305, 306, 307, 308, 309, 315, 316, 317, 319, 320, 325, 332, 333, 336, 342], "pytorch_exampl": 319, "pytorch_model": [273, 274, 276, 281], "pytorchlight": 318, "pytroch_light": 318, "q": [7, 36, 55, 107, 108, 140, 251, 257, 294, 301, 320, 342], "q4_0": 320, "q4_1": [304, 320], "q8_0": 320, "q8_1": 320, "q_fold": 7, "q_proj": [337, 341], "qat": [202, 219, 226, 249, 251, 328, 330, 342], "qbfloat16": [257, 294, 298, 301], "qbfp": [259, 295, 296, 298], "qbloat16": 298, "qconfig": 318, "qconv1d": 14, "qconv2d": 14, "qconv3d": 14, "qconvtranspose1d": 14, "qconvtranspose2d": 14, "qconvtranspose3d": 14, "qdq": [22, 51, 54, 55, 57, 78, 86, 91, 106, 108, 109, 150, 177, 251, 252, 253, 254, 255, 257, 258, 274, 285, 288, 291, 294, 298, 300, 301, 307, 342], "qdq_op_type_per_channel_support_to_axi": 54, "qdqnputransformerquant": 54, "qdqoptypeperchannelsupporttoaxi": 257, "qdqquantiz": 54, "qfloat16": [257, 301], "qgemm": [9, 15], "qinstancenorm1d": 17, "qinstancenorm2d": [9, 17], "qinstancenorm3d": 17, "qint16": [257, 279, 280, 284, 298, 299, 301], "qint32": [257, 301], "qint4": 257, "qint8": [3, 54, 57, 61, 252, 253, 254, 255, 257, 279, 280, 284, 285, 298, 299, 300, 301], "qk4_1": 320, "qkrotat": 140, "qkv": 337, "qlayernorm": [9, 17], "qlinearop": [4, 39, 54, 63], "qmatmul": [9, 16], "qmax": 55, "qmin": 55, "qmodel_path": 21, "qmx": 297, "qoper": [51, 91, 257, 258], "qparamslinear": 150, "qparamsoper": 177, "qscale_typ": 318, "qscheme": [183, 192, 198, 199, 302, 304, 319, 320, 333, 337, 338, 341], "qschemetyp": [198, 199, 202, 302, 304, 319, 320, 333, 337, 338, 341], "qserver": 336, "qspec": [178, 240, 241, 242], "qtype": [39, 55], "qualiti": [291, 303, 307], "quant": [20, 120, 199, 215, 217, 251, 255, 257, 258, 274, 289, 304, 309, 319, 333], "quant_algo": [150, 316, 324, 336], "quant_algo_config": [121, 122, 127, 131, 136], "quant_algo_config_file_path": 150, "quant_base_op": 9, "quant_config": [150, 177, 198, 234, 235, 236, 237, 252, 253, 254, 255, 257, 259, 260, 285, 294, 295, 296, 297, 298, 300, 302, 304, 307, 316, 319, 320, 323, 326, 333, 337, 338, 341], "quant_config_file_path": 309, "quant_dtyp": 192, "quant_format": [54, 55, 57, 252, 253, 254, 255, 257, 259, 285, 294, 295, 296, 297, 298, 300, 301], "quant_gemm_op": 9, "quant_info": 13, "quant_matmul_op": 9, "quant_max": 192, "quant_min": 192, "quant_mod": [150, 198, 199, 319, 335], "quant_model": [22, 182, 198, 293, 302, 304, 307, 320, 333, 338], "quant_model_nonsmooth": 337, "quant_model_path": [18, 36], "quant_model_smooth": 337, "quant_modul": [22, 25], "quant_norm_op": 9, "quant_output": 26, "quant_pre_process": 291, "quant_result": 20, "quant_schem": [150, 316, 317, 324, 325, 336, 339], "quant_spec": [243, 337], "quant_typ": [7, 183, 257, 301], "quant_util": [3, 4, 20, 39, 54, 61, 63, 72, 299], "quantconv2d": [218, 234, 251], "quantconvtranspose2d": [220, 234], "quante4m3funct": 192, "quante5m2funct": 192, "quantembed": 236, "quantembeddingbag": 236, "quantformat": [57, 252, 253, 254, 255, 257, 258, 285, 291, 300, 301], "quantformatqdq": 258, "quanti": 270, "quantiat": [55, 111], "quantif": 76, "quantiti": 102, "quantiz": [1, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 18, 20, 22, 25, 26, 27, 34, 36, 39, 51, 53, 54, 55, 72, 92, 95, 98, 104, 107, 108, 127, 131, 139, 140, 143, 150, 151, 177, 178, 181, 182, 183, 185, 186, 248, 250, 254, 255, 256, 258, 261, 265, 285, 288, 304, 305, 306, 308, 310, 311, 312, 313, 314, 317, 318, 322, 323, 325, 326, 328, 329, 330, 331, 337, 342], "quantization_annot": 225, "quantization_candid": 3, "quantization_config": [258, 299, 323, 326], "quantizationconfig": [57, 177, 182, 183, 198, 199, 230, 234, 235, 236, 237, 252, 253, 254, 255, 257, 259, 285, 294, 295, 296, 297, 298, 300, 302, 304, 319, 320, 333, 337, 338], "quantizationmod": [4, 39, 54, 63, 150, 198, 199, 202, 319, 335], "quantizationmodul": [9, 13], "quantizationparam": 39, "quantizationspec": [178, 182, 183, 198, 199, 240, 241, 242, 243, 302, 304, 319, 320, 333, 337, 338], "quantize_bia": 54, "quantize_bias_stat": 39, "quantize_bias_tensor": 54, "quantize_data_pof2": 55, "quantize_dequant": 7, "quantize_diffus": 309, "quantize_dynam": 61, "quantize_fp16": 55, "quantize_initi": 39, "quantize_model": [56, 182, 198, 252, 253, 254, 255, 258, 260, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 285, 294, 296, 297, 298, 300, 302, 304, 307, 319, 320, 333, 337, 338], "quantize_quark": [284, 286, 287, 292, 316, 318, 324, 336, 339], "quantize_stat": 301, "quantize_weight": 39, "quantize_weight_per_channel": 39, "quantizealloptyp": 257, "quantizebia": 257, "quantized_configur": 55, "quantized_model": [258, 274, 276, 278, 281, 283, 285, 291, 317, 319], "quantized_model_path": 258, "quantized_onnx_model_path": 293, "quantized_tensor_typ": [3, 4, 39, 54, 63], "quantized_value_map": 39, "quantizedconvbatchnorm2d": [219, 235], "quantizefp16": 257, "quantizelinear": [7, 22, 107, 257, 301], "quantizerlinear": 325, "quantizewrapp": [9, 13], "quantlinear": [150, 177, 221, 230, 237, 251], "quantmixin": 233, "quantschem": 316, "quanttyp": [3, 54, 57, 61, 183, 252, 253, 254, 255, 257, 259, 279, 280, 284, 285, 298, 299, 300, 301], "quark": [248, 250, 253, 254, 255, 256, 257, 261, 262, 263, 265, 266, 267, 272, 273, 274, 275, 276, 277, 281, 282, 285, 291, 293, 299, 300, 301, 303, 305, 306, 308, 311, 313, 314, 315, 318, 321, 322, 323, 325, 328, 332, 333, 335, 336, 339, 340, 341, 342], "quark_0": 250, "quark_debug": 307, "quark_debug_act_hist": 307, "quark_debug_input_pickl": 307, "quark_exported_model": 320, "quark_quant": [279, 280], "quark_safetensor": 324, "quarot": [199, 249, 257, 258, 289, 328, 336], "quarotconfig": [139, 199], "quarotprocessor": 139, "question": 320, "quick": [257, 284, 286, 287, 291, 292], "quicker": 299, "quickli": [300, 316], "quint16": [257, 301], "quint32": [257, 301], "quint4": 257, "quint8": [252, 253, 254, 255, 257, 279, 280, 285, 301], "quit": 320, "qunat": 39, "qweight": 323, "qwen": [315, 316, 339, 342], "qwen1": [316, 342], "qwen2": [315, 316, 339, 342], "qx": 242, "qzero": 323, "r": [6, 7, 55, 192, 199, 236, 251, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 290, 292, 310, 315, 316, 317, 342], "r1": [199, 278, 339, 340], "r2": [199, 278, 339], "r3": [140, 199, 278, 339], "r4": [199, 278, 339], "r4wrapper": 140, "r_config_path": 278, "r_matrix": 71, "ra_in1k": 277, "ra_in1k_mixed_precision_quant": 277, "ra_in1k_quant": 277, "rais": [3, 32, 39, 56, 241, 257, 302], "rand": [265, 305, 319], "randint": [265, 338], "randn": 236, "random": [22, 55, 111, 141, 144, 182, 199, 254, 257, 265, 291, 299, 338], "random2": 199, "random_data_reader_input_shap": [55, 291], "random_hadamard_matrix": 141, "random_quant": 293, "randomdataread": [55, 257], "randomdatareaderinputdatarang": 257, "randomdatareaderinputshap": [257, 291], "randomli": [199, 299], "rang": [1, 3, 39, 54, 55, 57, 93, 151, 183, 202, 241, 249, 251, 257, 259, 265, 279, 280, 286, 287, 292, 294, 295, 296, 297, 298, 299, 303, 307, 337], "rank": 291, "rapid": [55, 111], "rate": [252, 257, 315], "rather": [32, 261, 306], "ratio": 257, "raw": [1, 267, 272, 273, 274, 275, 276, 278, 281, 282, 283, 284, 286, 287, 292, 317, 320], "rceil": 320, "rconfigpath": [254, 257], "re": [241, 243, 257, 294], "reach": [279, 280, 298, 302], "read": [257, 258, 320], "reader": [1, 20, 22, 55, 56, 257, 258, 291, 298], "readi": [193, 198, 279, 280, 302], "readm": 250, "real": [55, 150, 151, 178, 182, 251, 260, 261, 290, 299, 306, 326], "real_quant": [150, 178, 323, 326], "realli": 303, "realquant": 177, "realquantizerbas": 178, "reap": 298, "reason": [54, 150, 183, 320], "rec": [266, 268, 269], "receiv": [32, 33, 50, 54], "recipi": 310, "recogn": 159, "recommend": [82, 85, 95, 150, 250, 257, 260, 291, 293, 301, 337], "reconstruct": [26, 286, 287, 292], "reconvert": 291, "record": [118, 241, 261, 306, 319], "recov": 150, "recreat": 55, "recres": 1, "recurs": 39, "reduc": [39, 54, 55, 57, 193, 194, 198, 217, 236, 251, 257, 259, 261, 266, 282, 283, 286, 287, 291, 292, 294, 295, 296, 297, 298, 299, 301, 302, 306, 307, 336, 337, 339], "reduce_mean": 217, "reduce_rang": [4, 39, 54, 55, 57, 61, 63, 241, 257, 300], "reducemean": [53, 257, 301], "reduct": [236, 251], "redund": [82, 85, 291, 301], "ref_input": [1, 307], "refer": [1, 55, 61, 177, 228, 251, 252, 253, 254, 255, 259, 263, 265, 266, 267, 276, 282, 285, 289, 291, 294, 295, 296, 297, 298, 303, 304, 305, 306, 307, 308, 315, 316, 317, 318, 319, 320, 324, 330, 331, 333, 335, 336, 337], "referenc": [26, 27], "refin": [261, 299, 306, 342], "reflect": [251, 265], "reg_param": 27, "regard": [226, 236], "regardless": [296, 297], "regist": [9, 20, 124, 177, 233, 243, 259, 266, 267, 268, 269, 271, 275, 277, 291, 294, 295, 296, 297, 301, 308], "register_custom_ops_librari": [259, 285, 291, 294, 295, 296, 297, 301], "register_forward_hook": 205, "regress": [261, 306], "regular": [9, 124, 177, 233, 243], "rel": [118, 252, 257, 303, 307, 337], "relat": [55, 252, 253, 254, 255, 317, 320], "relativecr": 118, "releas": [177, 250, 257, 287, 308], "release_vers": 250, "relev": [199, 252, 253, 254, 255, 266, 267, 276, 282, 285, 303, 307, 308, 315, 316, 317, 318, 319], "reli": [299, 309, 310, 316], "reliev": [254, 278], "reload": [198, 234, 323, 342], "relu": [9, 33, 39, 53, 54, 55, 124, 177, 215, 233, 243, 253, 257, 301], "relu6": 33, "relu9": 257, "relu_input": 55, "relu_output_0": 257, "remain": [236, 251, 293, 298], "remov": [1, 51, 54, 55, 82, 85, 104, 105, 106, 107, 108, 251, 257, 299, 338, 339, 342], "remove_bf16_cast": 100, "remove_initi": 55, "remove_initializer_from_input": 100, "remove_nod": 55, "remove_qdq": 100, "remove_qdq_between_op": 100, "remove_qdq_mul_add": 100, "removedropoutnod": 222, "removeinputinit": 257, "removeqdqbetweenop": 257, "removeqdqconvclip": 257, "removeqdqconvgelu": 257, "removeqdqconvleakyrelu": 257, "removeqdqconvprelu": 257, "removeqdqconvrelu": 257, "removeqdqinstancenorm": 257, "removeqdqmuladd": 257, "removeqdqtransform": 50, "removeqdqtransformspipelin": 51, "removerelu": 32, "renam": 250, "renorm": 236, "reorder": [150, 151, 177, 178, 198, 323, 326], "repeat": 299, "replac": [32, 33, 50, 109, 110, 140, 150, 198, 202, 211, 218, 219, 220, 221, 230, 244, 249, 251, 253, 257, 258, 291, 302, 304, 318, 320, 333, 341, 342], "replace_bfloat16_qdq_cast": 100, "replace_conv2d_qtconv2d": 218, "replace_conv2dbn_quantizedconv_modul": 219, "replace_convtranspose2d_qtconvtranspose2d": 220, "replace_inf_in_onnx_weight": 110, "replace_inf_valu": [110, 293], "replace_inf_weight": [100, 293], "replace_linear_qtlinear": 221, "replaceclip6relu": [253, 257], "replacewholemodel": 32, "repo": [267, 275, 284, 286, 292], "report": [250, 310], "repr": 32, "repres": [6, 7, 9, 33, 39, 124, 147, 149, 177, 182, 192, 233, 241, 243, 251, 257, 259, 263, 264, 279, 280, 291, 295, 296, 297, 299, 302, 303, 305, 316, 333, 334], "represent": [55, 183, 202, 241, 243, 251, 257, 259, 295, 296, 303], "reproduc": 257, "request": [272, 278, 282, 283, 302, 315, 316, 318, 321, 324], "requir": [6, 7, 32, 53, 119, 177, 178, 182, 183, 192, 199, 213, 215, 217, 218, 219, 220, 221, 236, 241, 250, 251, 252, 257, 259, 263, 291, 293, 295, 298, 301, 308, 310, 316, 317, 324, 333, 336], "require_acceler": 119, "require_torch_gpu": 119, "requires_grad": 236, "res_orig": 337, "res_quant_nonsmooth": 337, "res_quant_smooth": 337, "rescal": 337, "reserv": [270, 276, 278, 294, 295, 296, 297, 298, 299], "reset": 241, "reset_it": 55, "reset_min_max_v": 241, "reset_model": 150, "reshap": [140, 212, 257, 290, 301, 302], "resiz": [93, 301], "resnet": [267, 275, 284, 286, 292, 319, 342], "resnet152": [253, 270, 271], "resnet152_cle_quant": 271, "resnet152_quant": 271, "resnet18": 319, "resnet50": [260, 267, 284, 286, 289, 292], "resnet50_quant": 260, "resnetv17_conv0_fwd": [286, 292], "resnetv17_stage1_conv0_fwd": [286, 292], "resolv": [273, 274, 276, 281, 317], "resourc": [252, 259, 279, 280, 295, 299], "respect": [32, 257, 303, 314, 320, 339], "respons": [118, 299, 308, 312], "rest": 338, "restor": [150, 251], "result": [1, 3, 6, 7, 20, 33, 39, 55, 82, 85, 118, 192, 249, 251, 252, 257, 260, 263, 267, 278, 284, 287, 299, 317, 333, 336, 337, 339, 342], "retain": [252, 267, 298], "retent": [286, 287, 292], "retrain": [249, 251, 252], "retri": 119, "retriev": [6, 7, 39, 147, 192, 230, 236, 302, 303, 338], "retrieve_dataset": 312, "retry_flaky_test": 119, "return": [1, 3, 6, 7, 9, 20, 22, 26, 31, 32, 33, 34, 39, 50, 51, 53, 55, 56, 78, 82, 85, 86, 91, 93, 95, 99, 106, 107, 108, 110, 111, 118, 124, 147, 149, 150, 159, 177, 192, 193, 198, 199, 213, 228, 233, 236, 241, 243, 244, 258, 260, 265, 280, 284, 290, 299, 305, 316, 337], "return_tensor": [198, 302, 304, 320, 333], "reus": [95, 265], "rewind": 260, "rewrit": 291, "right": [1, 251, 270, 276, 278, 279, 280, 294, 295, 296, 297, 298, 299, 303, 307, 338], "rigid": 252, "rigor": [300, 341], "rm": [272, 273, 274, 276, 281], "rmatrixdim": [254, 257], "rmax": [39, 55], "rmax_overrid": 55, "rmin": [39, 55], "rmin_overrid": 55, "rmin_real_rang": 55, "rmove": 104, "rmsnorm": 144, "robust": [249, 261, 286, 287, 292, 306], "rocm": [250, 258, 266, 285, 294, 295, 296, 297, 304, 342], "rocmexecutionprovid": [294, 295, 296, 297], "roi": 290, "root": [57, 144], "rope": 140, "rotat": [71, 140, 199, 254, 257, 278, 316, 328, 336, 342], "rotate_in_channel": [71, 144], "rotate_in_channels2": 140, "rotate_out_channel": [71, 144], "rotate_out_channels2": 140, "rotated_quantized_model": 278, "rotated_smoothed_quantized_model": 278, "rotation_config": [254, 278], "rotation_config_info": 71, "rotation_matrix": 71, "rotationconfig": [143, 199], "rotationprocessor": 143, "roug": [310, 311, 313], "round": [7, 25, 26, 27, 199, 202, 242, 251, 252, 257, 286, 287, 292, 294, 299, 302, 303, 338, 341], "round_func": 7, "round_impl": 7, "round_method": [198, 199, 304, 319, 320, 333, 337, 338, 341], "round_mod": 192, "roundhalftoeven": 7, "rounding_kei": 192, "rounding_mod": [6, 257, 296, 297], "roundtyp": [198, 199, 202, 302, 304, 319, 320, 333, 337, 338, 341], "rsmnorm": 340, "rst": 310, "rule": 305, "run": [1, 20, 25, 32, 55, 61, 177, 213, 215, 217, 241, 250, 251, 257, 258, 261, 265, 266, 267, 270, 285, 290, 293, 304, 306, 310, 311, 312, 313, 314, 315, 317, 318, 322, 324, 331, 337, 339, 341, 342], "run_checks_after_each_pass": 213, "run_onnx_model": 55, "runnabl": 342, "running_mean": [219, 228], "running_var": [219, 228], "runtest": 32, "runtim": [3, 39, 55, 159, 249, 250, 257, 259, 260, 263, 285, 291, 299, 301, 320, 333, 342], "runtime_check": 159, "runtime_except": 290, "runtimeexcept": 290, "runwayml": 309, "ryzen": [258, 342], "s16s16_mixed_s8s8": [277, 300], "s16s16_mixed_s8s8_aaw": 277, "s16s8": 96, "s16s8_asw": 300, "s16s8_asws_adaqu": 300, "s16s8_asws_adaround": 300, "s2": 303, "s8s8": 94, "s8s8_aaw": [268, 269, 271, 277, 279, 280, 300], "s8s8_aaws_adaqu": [252, 268, 300], "s8s8_aaws_adaround": [252, 269, 300], "s_": 303, "s_2": 337, "s_b": 303, "sacrific": 294, "safe": 324, "safetensor": [150, 151, 154, 198, 249, 304, 323, 324, 335, 342], "safetensor_path": [154, 156], "safetensors_path": [198, 335], "sai": [307, 337, 339], "same": [7, 32, 33, 50, 151, 177, 182, 199, 202, 228, 236, 243, 251, 257, 263, 270, 286, 287, 291, 292, 294, 296, 297, 298, 312, 316, 320, 323, 332, 333], "sampl": [20, 149, 236, 253, 254, 255, 270, 299, 311, 312, 314, 324, 333], "sample_1": 270, "sample_2": 270, "sampler": 149, "samsum": 314, "satisfactori": 299, "satisfi": [55, 95, 215], "save": [1, 3, 6, 7, 22, 56, 61, 99, 110, 114, 150, 182, 192, 198, 199, 205, 252, 257, 258, 270, 279, 280, 284, 286, 287, 291, 292, 298, 307, 312, 313, 314, 319, 320, 324, 326, 337, 342], "save_as_external_data": [282, 283, 291], "save_dir": [150, 315, 335], "save_distribution_histogram": 205, "save_for_backward": [6, 7, 192], "save_for_forward": [6, 7, 192], "save_metrics_to_csv": [313, 314], "save_model": [82, 85, 99, 114], "save_param": [150, 198, 335], "save_path": [1, 205], "save_prefix": 1, "save_pretrain": 323, "save_pruned_model": 315, "save_tensor_hist": 100, "save_torch_model": 22, "save_weights_hist": 100, "saved_path": 309, "saved_tensor": [6, 7, 192], "savetensorhistfig": 257, "sb": 303, "scalar": [55, 262, 332, 337], "scale": [3, 7, 39, 54, 55, 72, 93, 124, 141, 150, 151, 177, 178, 183, 192, 199, 202, 236, 240, 241, 242, 249, 251, 252, 253, 257, 258, 259, 261, 262, 264, 267, 275, 279, 280, 286, 287, 292, 294, 295, 296, 297, 298, 304, 306, 319, 320, 323, 326, 332, 334, 335, 336, 337, 341, 342], "scale2po": [55, 93], "scale_clamp_min": [199, 337, 341], "scale_grad_by_freq": 236, "scale_shap": 178, "scale_typ": [183, 198, 199, 304, 319, 320, 333, 337, 338, 341], "scaledactiv": 124, "scaledfakequant": 243, "scaledfakequantizefunct": 192, "scaledrealquant": 178, "scaledrealquantizefunct": 192, "scaletyp": [198, 199, 202, 302, 304, 319, 320, 333, 337, 338, 341], "scaling_lay": [199, 337, 341], "scenario": 299, "schedul": 213, "scheme": [199, 202, 251, 252, 254, 257, 258, 303, 304, 307, 318, 319, 341, 342], "score": [309, 310, 313, 314], "scratch": 252, "screen": 257, "script": [267, 270, 291, 318, 320, 339, 340, 342], "sd": 309, "sd1": [309, 342], "sdxl": [249, 309, 342], "seamless": [249, 342], "seamlessli": 249, "sean": 202, "search": [1, 107, 108, 270, 324, 336], "search_algo": [280, 284], "search_cache_dir": [280, 284], "search_evalu": [280, 284, 299], "search_forward": 1, "search_metr": [280, 284], "search_metric_toler": [280, 284], "search_model": 299, "search_spac": [279, 280, 284, 299], "search_space_advanc": 284, "search_space_with_gpu": [279, 280], "search_space_xint8": [279, 280, 284], "search_stop_condit": 299, "searched_candid": 299, "searchspac": 1, "second": [241, 257, 299, 336], "section": [235, 248, 266, 267, 268, 269, 271, 275, 277, 304], "see": [6, 7, 159, 192, 202, 236, 242, 258, 260, 272, 278, 282, 283, 294, 304, 307, 310, 311, 312, 315, 316, 337, 339, 340], "seed": [22, 257], "seen": 337, "select": [61, 150, 199, 250, 251, 252, 254, 257, 296, 298, 299, 302, 314, 317, 336], "selective_upd": 27, "selectiveupd": 257, "self": [9, 32, 33, 39, 50, 124, 159, 177, 199, 233, 241, 243, 258, 260, 265, 337], "self_attn": [199, 337, 339, 341], "self_attn_layer_norm": 337, "sensit": [257, 307, 336, 337], "sentencepiecetokentyp": 156, "separ": [6, 7, 39, 192, 257, 291, 298], "seper": [312, 314], "seq_len": [314, 316, 324], "seqlen": 318, "sequenc": [3, 34, 236, 314], "sequenti": 199, "serer": 267, "seri": [270, 279, 280], "serial": 323, "serv": [265, 308], "sess_opt": [259, 294, 295, 296, 297, 301], "session": [20, 259, 265, 280, 284, 285, 291, 294, 295, 296, 297, 301], "sessionopt": [259, 285, 291, 294, 295, 296, 297, 301], "set": [6, 7, 8, 9, 22, 32, 33, 50, 55, 56, 61, 82, 85, 99, 150, 177, 186, 192, 193, 198, 199, 205, 241, 243, 252, 253, 254, 257, 258, 259, 260, 263, 264, 278, 284, 285, 290, 291, 293, 294, 295, 296, 297, 298, 299, 300, 301, 304, 305, 308, 311, 312, 313, 314, 316, 320, 333, 334, 339, 340], "set_algo_config": 199, "set_bia": 8, "set_modules_original_bia": 9, "set_modules_original_weight": 9, "set_op_by_nam": 244, "set_weight": 8, "settl": 202, "setup": [32, 199, 311, 320], "setup_config_per_lay": 230, "setup_context": [6, 7, 192], "setup_se": 22, "sever": [1, 251, 256, 257, 258, 296, 297, 299, 303, 320, 336, 337, 340, 341], "sgd": 236, "sh": [270, 309], "shallow": 177, "shape": [7, 32, 33, 50, 55, 79, 82, 85, 99, 177, 182, 236, 243, 257, 262, 266, 291, 293, 302, 303, 320, 332, 337], "shape_infer": 291, "share": [95, 151, 199, 202, 249, 251, 252, 257, 259, 263, 266, 267, 286, 287, 292, 294, 295, 296, 297, 298, 302, 304, 307, 310, 331, 333, 335, 336, 337, 338], "shift": [7, 76, 257, 264, 303, 334, 336, 337, 338], "short": [252, 303, 338], "should": [1, 3, 6, 7, 9, 32, 33, 50, 118, 119, 124, 140, 149, 150, 151, 177, 192, 193, 198, 199, 205, 226, 233, 236, 241, 243, 254, 257, 260, 279, 280, 291, 296, 299, 300, 301, 302, 305, 307, 308, 312, 315, 316, 320, 338, 339, 341], "should_quantize_nod": 74, "show": [111, 112, 150, 252, 253, 254, 255, 274, 278, 281, 287, 298, 304, 319, 331], "shown": [291, 303, 312, 320], "shuffl": [265, 305], "sigma": 337, "sigmoid": [55, 61, 236, 257, 287, 301], "sigmoidqdqtoqoptransform": 50, "sign": [99, 202, 257, 296, 297, 303, 319, 336, 338], "signatur": [32, 159], "signific": [249, 252, 257, 298, 336], "significantli": [252, 259, 263, 266, 268, 269, 287, 295, 298, 299, 333, 336, 339], "similar": [6, 98, 99, 235, 251, 279, 280, 293, 319], "similarli": 303, "simpl": [159, 193, 198, 236, 252, 259, 265, 294, 295, 296, 297, 298, 303, 305, 307, 320], "simpler": 251, "simpli": [251, 252, 263, 294, 296, 297, 298, 303, 304, 307, 331, 333, 335, 336, 337, 340], "simplifi": [82, 85, 249, 251, 257, 293, 301, 316, 337], "simplifymodel": 257, "simul": [55, 75, 251, 257], "simulate_bf16": 293, "simulate_dpu_softmax": 35, "simulate_transform": 74, "simulatedpu": 257, "simultan": 298, "sinc": [22, 32, 140, 236, 293, 339], "singl": [8, 22, 25, 32, 217, 241, 243, 257, 297, 302, 303, 310, 337, 338, 342], "single_gpu": 258, "situat": [1, 254, 278, 279, 280], "size": [3, 53, 61, 71, 76, 99, 140, 141, 149, 193, 198, 199, 212, 236, 249, 251, 252, 257, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 286, 287, 290, 291, 292, 301, 302, 303, 315, 316, 319, 320, 324, 336, 339, 341, 342], "skip": [6, 7, 177, 192, 257, 291, 336], "skip_evalu": 317, "skip_finetun": 317, "skip_onnx_shap": 291, "skip_optim": 291, "skip_prun": 315, "skip_quant": [226, 309, 316, 317], "skip_symbolic_shap": 291, "slice": [53, 257, 287, 291, 301], "slice_1": 287, "small": [199, 267, 285, 294, 297, 303, 339], "smaller": [53, 251, 297, 303, 332], "smooth": [3, 76, 199, 255, 257, 289, 336, 340, 341], "smooth_config": 336, "smooth_fc_fc": 340, "smooth_quant": [255, 281], "smoothalpha": [255, 257, 299], "smoothed_quantized_model": 281, "smoothli": [279, 280], "smoothquant": [76, 181, 199, 249, 257, 258, 304, 316, 320, 328, 336, 339, 341, 342], "smoothquant_config": 337, "smoothquantconfig": [127, 199, 337, 341], "smoothquantprocessor": [127, 341], "so": [1, 32, 55, 61, 95, 180, 183, 213, 217, 228, 236, 251, 255, 257, 259, 270, 285, 291, 294, 301, 323, 338, 339], "softmax": [75, 257, 301, 318], "sole": 257, "solut": [249, 290, 299, 331], "some": [39, 55, 61, 150, 180, 183, 212, 215, 252, 253, 255, 257, 279, 280, 286, 287, 291, 292, 293, 298, 300, 307, 316, 317, 319, 323, 326, 337, 340, 341], "sometim": [251, 252, 291, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337, 340], "sort": 298, "sourc": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 121, 122, 124, 127, 128, 131, 135, 136, 138, 139, 140, 141, 143, 144, 147, 149, 150, 151, 154, 156, 159, 162, 169, 177, 178, 180, 181, 182, 183, 186, 192, 193, 194, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 230, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244], "space": [1, 270, 279, 280, 284, 291, 299, 300, 337], "space1": [279, 280, 284], "space2": [279, 280, 284], "spacetodepth": 301, "span": [266, 267, 268, 269, 271, 275, 277], "spars": 236, "sparseadam": 236, "spdx": [270, 276, 278, 294, 295, 296, 297, 298, 299], "spec": 320, "special": [150, 180, 228, 249, 337], "special_tokens_map": [273, 274, 276, 281, 323], "specif": [4, 39, 53, 54, 55, 57, 63, 183, 199, 202, 217, 244, 251, 252, 257, 259, 279, 280, 284, 286, 287, 292, 294, 295, 296, 297, 298, 299, 302, 303, 304, 305, 307, 308, 309, 312, 323, 326, 331, 335, 336, 337, 338, 341], "specifi": [3, 31, 32, 39, 53, 54, 55, 56, 57, 61, 107, 108, 110, 118, 150, 151, 154, 182, 183, 193, 198, 199, 205, 226, 236, 241, 244, 252, 253, 254, 257, 258, 260, 279, 280, 291, 293, 298, 299, 307, 310, 311, 313, 314, 319, 324, 337, 341, 342], "specific_tensor_precis": [55, 57, 257, 298], "sped": 339, "speed": [251, 259, 285, 286, 287, 292, 295, 296, 297, 298, 299], "speedup": 149, "spinquant": [140, 278, 339, 340], "split": [1, 21, 53, 169, 180, 244, 257, 301, 303], "split_large_kernel_pool": 53, "split_model_info": 169, "split_params_for_dbrxexpert": 180, "splite": 1, "splitlargekernelpool": 257, "splitquantmodulecalledoveronc": 217, "sq": 254, "sq_alpha": 278, "sqrt": [140, 339], "squar": [3, 26, 57, 144, 183, 257, 261, 306, 336], "squeez": 301, "src": 228, "ssim": [1, 279, 280, 284], "ssim_metr": 1, "stabil": [296, 319], "stabilityai": 309, "stabl": [150, 205, 249, 252, 257, 309, 342], "stackoverflow": 138, "stage": 251, "stand": 303, "standalon": 301, "standard": [7, 78, 121, 122, 127, 128, 131, 135, 136, 139, 143, 150, 156, 178, 194, 199, 202, 206, 209, 222, 241, 251, 252, 259, 295, 310, 319, 339, 342], "start": [55, 57, 61, 107, 108, 147, 199, 236, 257, 270, 279, 280, 284, 286, 287, 292, 300, 331], "start_bin": 241, "start_node1": 257, "start_node2": 257, "startup": 118, "stat": [183, 205], "state": [31, 144, 177, 249, 251, 252, 294, 296, 297, 298, 303, 304, 307, 320, 331, 335, 336, 337], "state_dict": [150, 177, 180, 320], "static": [4, 6, 7, 26, 39, 54, 63, 159, 192, 198, 199, 249, 252, 257, 258, 260, 261, 263, 280, 284, 302, 304, 306, 307, 323, 325, 333, 335, 336, 338, 339, 340, 341, 342], "static_group": [199, 341], "staticmethod": [6, 7, 192, 280, 284], "statisfi": 270, "statist": [183, 205, 243, 261, 306, 307, 337], "statu": 290, "std": 202, "stem": 257, "step": [39, 118, 249, 250, 251, 253, 257, 258, 280, 284, 285, 291, 293, 298, 299, 302, 304, 316, 319, 333, 337, 338, 340, 342], "still": [7, 39, 302, 304, 320, 333], "stop": [252, 257, 270, 279, 280, 284, 299], "storag": [259, 265, 266, 267, 268, 269, 270, 271, 275, 277, 286, 287, 291, 292, 295, 298, 302, 303, 320], "store": [3, 6, 7, 150, 192, 205, 236, 251, 257, 260, 291, 293, 303, 307, 320, 323, 335, 338], "str": [1, 3, 4, 5, 6, 13, 18, 20, 21, 22, 27, 31, 32, 33, 34, 36, 39, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 74, 76, 92, 99, 107, 108, 110, 111, 114, 118, 119, 136, 138, 140, 147, 150, 151, 154, 156, 169, 177, 180, 182, 192, 193, 198, 199, 203, 205, 226, 230, 234, 235, 236, 241, 243, 244, 257, 258, 260, 279, 280, 284, 291, 316, 341], "straightforward": 251, "strategi": [217, 249, 258, 259, 284, 286, 287, 292, 295, 303, 304, 336, 339, 340, 342], "streamlin": [248, 308], "strength": 318, "strictli": 303, "stride": [218, 219, 220, 234, 235, 241], "strike": [251, 296, 307], "string": [1, 54, 118, 147, 150, 199, 241, 243, 252, 254, 257, 291], "strive": 307, "strongli": 250, "struct": 320, "structur": [9, 71, 108, 124, 159, 177, 193, 198, 233, 243, 250, 252, 289, 299, 320, 330, 340], "stuck": 331, "studio": [250, 301], "stuff": 235, "style": [118, 149], "sub": [32, 33, 50, 71, 257, 260, 267, 296, 301, 303, 342], "sub_1": 287, "sub_block_s": [6, 257, 296], "sub_block_shift_bit": [6, 257, 296], "subblock": 303, "subclass": [6, 7, 9, 32, 124, 149, 177, 192, 233, 243, 308], "subdivid": 302, "subgraph": [21, 39, 57, 61, 257, 287], "subgraphs_to_exclud": [55, 57, 61, 257, 300], "subgroup": 297, "submit": [272, 278, 282, 283, 315, 316], "submodul": [9, 124, 177, 233, 243, 244], "subscal": 303, "subscript": 303, "subsequ": [250, 265], "subset": [266, 267, 268, 269, 271, 275, 277, 303], "substanti": 251, "substitut": 251, "subtract": 302, "subtyp": 159, "success": [250, 251, 291], "successfulli": [55, 291, 304, 320, 322], "suffici": [259, 294, 295, 326], "suit": 291, "suitabl": [31, 257, 298], "sum": [236, 257], "summar": 310, "summari": [57, 205, 307, 311, 313, 314], "summarize_activ": 205, "summarize_weight": 205, "summary_io_quantization_error": 307, "summary_ref_input_error": 307, "summary_ref_output_error": 307, "summary_weight_error": 307, "super": [9, 124, 177, 233, 243, 265, 337], "superior": [249, 319, 336], "suppli": [118, 319], "support": [3, 8, 9, 22, 39, 55, 61, 72, 149, 150, 198, 199, 217, 236, 249, 251, 257, 259, 260, 261, 262, 265, 266, 267, 278, 279, 280, 284, 285, 287, 291, 293, 294, 295, 296, 298, 299, 302, 303, 306, 307, 309, 311, 313, 314, 318, 319, 320, 321, 322, 323, 325, 326, 331, 332, 335, 336, 337, 338, 339, 340, 341, 342], "suppress": 57, "sure": [6, 7, 192, 251, 257, 312, 341], "swish": 257, "symbol": 291, "symint": [218, 220], "symmetr": [3, 9, 54, 55, 61, 183, 198, 199, 241, 249, 251, 254, 257, 258, 262, 264, 268, 269, 271, 275, 277, 286, 287, 292, 300, 301, 304, 319, 320, 332, 333, 334, 336, 337, 338, 340, 341, 342], "system": [118, 249, 258, 304, 308, 342], "systemat": 299, "t": [6, 7, 39, 140, 159, 183, 186, 192, 236, 241, 244, 316, 320, 337], "t_expon": 244, "tabl": [202, 236, 257, 291, 296, 319, 320], "tag": 320, "tail": 337, "tailor": [259, 263, 295, 298, 333, 337], "take": [3, 213, 236, 250, 252, 257, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 286, 287, 299, 320, 323, 337, 339], "taken": [236, 291, 337], "tanh": 301, "tar": [266, 267, 268, 269, 271, 275, 277], "target": [25, 53, 55, 90, 149, 150, 183, 193, 198, 199, 202, 243, 249, 251, 257, 284, 286, 287, 292, 298, 299, 301, 323], "target_devic": 199, "target_opset": 90, "target_opset_vers": 90, "target_vers": 55, "targetindic": 257, "targetoptyp": [257, 298], "targetquanttyp": [257, 298], "targettensor": 257, "task": [249, 272, 273, 274, 276, 279, 280, 281, 282, 283, 299, 310, 311, 313, 314, 342], "tbd": 342, "teardown": 32, "tech": [316, 342], "technic": 182, "techniqu": [53, 251, 252, 253, 254, 255, 256, 259, 267, 286, 287, 292, 295, 296, 298, 303, 318, 336], "technologi": [251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337], "tee": 317, "tell": 1, "tempfil": 150, "templat": [279, 280], "ten": 331, "tensor": [3, 6, 7, 8, 22, 26, 32, 33, 39, 50, 54, 55, 57, 78, 80, 81, 82, 83, 84, 85, 86, 99, 121, 122, 124, 127, 128, 131, 135, 136, 140, 141, 144, 149, 150, 151, 169, 177, 178, 180, 182, 183, 192, 193, 198, 199, 202, 205, 218, 219, 220, 221, 228, 236, 238, 241, 243, 244, 249, 251, 253, 257, 258, 259, 261, 262, 265, 268, 269, 291, 294, 295, 296, 297, 298, 303, 304, 306, 307, 319, 320, 323, 324, 332, 336, 337, 338, 339, 340, 342], "tensor_arrai": 114, "tensor_dtyp": 32, "tensor_float": 99, "tensor_float16": 99, "tensor_hist": 257, "tensor_quant": [178, 205], "tensor_shap": 32, "tensor_stat": 205, "tensor_sync": 7, "tensor_to_its_receiving_nod": 54, "tensordata": 149, "tensorproto": [13, 32, 33, 39, 55, 99, 114], "tensors_rang": [4, 39, 54, 63], "tensors_to_quant": 54, "term": [193, 198, 251, 252, 294, 296, 297, 298, 302, 303, 304, 307, 331, 335, 336, 337], "termin": [55, 299, 304, 331], "test": [32, 119, 257, 260, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 291, 299, 319, 320], "test_bf16": 317, "test_cas": 119, "test_imag": [284, 286, 287, 292], "test_prompt": 309, "test_siz": 309, "test_w_uint4_asym_finetun": 317, "testcas": 32, "text": [118, 198, 236, 272, 273, 274, 276, 281, 282, 283, 302, 303, 304, 307, 320, 333], "text_encod": 309, "text_encoder_2": 309, "text_to_imag": 309, "textual": 118, "tf": 31, "than": [9, 22, 32, 55, 71, 76, 99, 140, 236, 238, 241, 250, 251, 252, 257, 259, 260, 261, 264, 279, 280, 284, 291, 295, 298, 302, 303, 306, 320, 324, 334, 336, 337, 339], "thank": 320, "thankfulli": 320, "thei": [6, 7, 9, 118, 159, 186, 192, 193, 198, 199, 202, 217, 238, 243, 251, 257, 303, 305, 320, 339, 340, 341], "them": [9, 124, 140, 177, 178, 213, 217, 233, 236, 243, 251, 259, 260, 284, 286, 287, 291, 292, 293, 295, 302, 303, 308, 316, 320, 324, 336, 341], "themselv": 257, "theoret": 319, "theori": 140, "therebi": [251, 254, 278, 292], "therefor": [236, 291, 293, 298, 310], "thi": [3, 5, 6, 7, 9, 26, 31, 32, 33, 39, 50, 53, 54, 55, 56, 61, 72, 99, 124, 138, 140, 149, 150, 154, 162, 177, 182, 186, 192, 193, 194, 198, 199, 205, 213, 215, 217, 228, 233, 236, 241, 243, 250, 252, 253, 254, 255, 257, 258, 259, 260, 261, 263, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 309, 310, 312, 315, 316, 317, 318, 319, 320, 323, 324, 325, 326, 331, 333, 335, 336, 337, 338, 339, 340, 341], "think": [251, 320], "thorough": 341, "thoroughli": 302, "those": [32, 33, 50, 236, 259, 295], "though": [6, 7, 192, 337], "thread": [31, 118], "threadnam": 118, "three": [7, 257, 263, 267, 291, 293, 299, 305, 318, 320, 333, 338], "three_level_spac": 1, "threshold": [202, 249, 257, 279, 280, 291, 299, 319, 342], "through": [150, 181, 205, 213, 244, 249, 251, 257, 263, 265, 296, 297, 298, 303, 308, 312, 316, 318, 333, 336, 337], "throughput": 202, "thsound": 270, "thu": [257, 303, 307, 339], "thudm": [315, 316, 317], "tied_paramet": 180, "tile": 302, "time": [3, 55, 118, 119, 236, 250, 251, 252, 257, 259, 263, 270, 279, 280, 295, 298, 299, 300, 303, 308, 319, 320, 331, 333, 337], "time_limit": 299, "timm": [266, 268, 269, 271, 277], "tinygsm8k": 312, "tisquanttyp": 257, "titl": 205, "to_bfp": [257, 296, 301], "to_bfp_prim": [257, 296], "to_real_quantize_param": 178, "todo": [1, 54], "togeth": [302, 339], "token": [150, 154, 198, 278, 282, 283, 302, 304, 310, 314, 316, 317, 320, 323, 333, 336, 338, 340], "tokenization_auto": 316, "tokenized_output": [198, 302, 304, 320, 333], "tokenizer_config": [150, 154, 273, 274, 276, 281, 323], "tokenizer_dir": [154, 156], "tokenizer_path": [150, 322], "toler": [55, 270, 279, 280, 284, 298, 299], "too": [9, 124, 177, 199, 233, 243, 252], "tool": [249, 250, 251, 252, 257, 266, 282, 283, 291, 307, 309, 321, 324, 336], "tool_vers": 55, "toolkit": [249, 320, 342], "top": [244, 298], "top1": [257, 267, 298], "top1acctarget": 257, "top5": 267, "topic": [265, 284, 286, 287, 292, 309, 315, 318, 336, 341], "torch": [6, 7, 8, 9, 13, 14, 15, 16, 17, 22, 25, 26, 27, 76, 250, 265, 266, 268, 269, 271, 272, 273, 274, 276, 277, 278, 281, 282, 283, 289, 291, 302, 303, 304, 307, 308, 309, 310, 315, 316, 317, 318, 319, 320, 321, 322, 323, 325, 326, 330, 331, 333, 335, 336, 338, 339, 340, 342], "torch_compil": 198, "torch_doctest_autograd": [6, 7, 192], "torch_extens": 331, "torch_model": 22, "torchmodel": 8, "torchscript": 291, "torchvis": [309, 319], "total": 303, "touch": 140, "tow": 98, "toward": 202, "tqt": [202, 242, 249, 342], "tqtobserv": 242, "tqtquantiz": 192, "tqtspec": 199, "tqtthresholdinitmeth": 202, "trace": [22, 212], "trace_model": 181, "track": 205, "trade": [249, 279, 280], "tradit": [252, 298], "tradition": 320, "trail": 338, "train": [9, 22, 26, 27, 124, 177, 181, 193, 198, 199, 219, 226, 228, 233, 235, 236, 243, 249, 251, 252, 257, 259, 263, 266, 279, 280, 284, 285, 286, 287, 292, 294, 295, 296, 297, 317, 319, 320, 328, 333, 337, 340, 342], "train_batch_s": 319, "train_load": 319, "train_model_param": [22, 25, 26], "train_torch": 22, "train_torch_module_api": 22, "trainloss": 26, "trainparamet": [22, 25, 26, 27], "trans_opsfunc_2_quant_modul": 211, "transa": 15, "transb": 15, "transfer": [215, 217], "transform": [5, 30, 31, 32, 34, 50, 51, 53, 54, 55, 57, 71, 74, 140, 150, 181, 198, 199, 205, 206, 251, 254, 257, 258, 261, 274, 276, 278, 281, 291, 300, 302, 304, 306, 316, 317, 319, 320, 333, 336, 337, 338, 339], "transform_for_annot": 226, "transforms_pipelin": 30, "transformspipelin": 34, "transpos": [61, 71, 178, 257, 301], "trasnform": 55, "travers": [107, 108, 251], "treat": [236, 244, 257], "treatment": 180, "tree": [9, 33, 124, 177, 233, 243, 266, 268, 269, 270, 271, 275, 277, 279, 280], "trick": 140, "trigger": [279, 280], "true": [3, 5, 6, 7, 27, 39, 53, 55, 57, 61, 71, 72, 74, 76, 82, 85, 99, 118, 144, 150, 177, 181, 183, 192, 198, 199, 234, 235, 236, 252, 253, 254, 255, 257, 259, 260, 261, 265, 279, 280, 284, 285, 286, 287, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 304, 306, 316, 319, 325, 333, 338, 341], "true_sequenti": [199, 341], "truli": 199, "trust_remote_cod": [313, 314, 316], "try": [257, 319, 340], "tsv": 309, "tune": [57, 249, 257, 261, 284, 286, 287, 292, 299, 302, 306, 317, 336, 337, 342], "tupl": [6, 7, 9, 13, 27, 31, 39, 51, 55, 57, 61, 74, 107, 141, 150, 178, 180, 182, 192, 205, 241, 257], "turbo": [309, 342], "turn": [252, 257, 291], "tutori": [294, 295, 296, 297, 298, 302, 320, 338], "tw": 337, "two": [3, 6, 7, 98, 107, 183, 192, 199, 202, 228, 241, 249, 251, 252, 257, 261, 265, 267, 280, 284, 286, 287, 292, 294, 296, 298, 299, 301, 310, 319, 320, 326, 337, 340, 342], "txt": [266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 291, 292, 310, 312, 317], "type": [3, 5, 6, 7, 8, 9, 13, 22, 32, 39, 53, 54, 55, 57, 61, 80, 81, 82, 83, 84, 85, 99, 107, 111, 150, 159, 177, 183, 192, 193, 198, 199, 217, 236, 243, 249, 252, 257, 258, 261, 263, 267, 272, 278, 291, 296, 297, 298, 299, 302, 304, 305, 310, 311, 313, 314, 316, 318, 319, 320, 324, 333, 336, 337, 338, 341, 342], "typedef": 320, "typic": [118, 199, 249, 251, 252, 261, 262, 265, 279, 280, 286, 287, 292, 298, 303, 305, 306, 307, 319, 332, 336, 337, 340], "typo": [300, 341], "u": [302, 307, 339], "u16s8": 96, "u16s8_aaw": 300, "u16s8_aaws_adaqu": 300, "u16s8_aaws_adaround": 300, "u16u8": 97, "u4w": 317, "u8s8": 94, "u8s8_aaw": 300, "u8s8_aaws_adaqu": 300, "u8s8_aaws_adaround": 300, "u8u8": 97, "uint16": [55, 249, 257, 258, 293, 298, 300, 301, 342], "uint32": [249, 258, 298, 301, 342], "uint4": [150, 198, 202, 249, 257, 301, 304, 311, 313, 314, 316, 320, 325, 333, 341, 342], "uint4_int4_flag": [150, 325], "uint4_per_group_asym_spec": 341, "uint4perchannelspec": 199, "uint4pergroupspec": 199, "uint4pertensorspec": 199, "uint8": [55, 150, 202, 249, 257, 258, 272, 273, 293, 298, 300, 301, 316, 320, 325, 341, 342], "uint8_dynamic_qu": [272, 273, 300], "uint8_t": 320, "uint8perchannelspec": 199, "uint8pergroupspec": 199, "uint8pertensorspec": 199, "ultim": 140, "ultra": 202, "unchang": [99, 252, 339], "uncom": 333, "undefin": 177, "under": [215, 266, 267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 308, 316, 319, 320, 322, 342], "undergo": 251, "understand": [302, 312, 341], "unfortun": 291, "unifi": [228, 252], "uniform": [240, 241, 242, 252, 298, 319], "uniformli": 202, "uniformscalingobserv": 241, "uninstal": [279, 280], "union": [56, 57, 150, 154, 193, 198, 320], "uniqu": [95, 199], "unit": 54, "unknown": [33, 55, 291], "unless": [57, 199, 251, 252, 294, 296, 297, 298, 303, 304, 307, 331, 335, 336, 337], "unlik": [252, 294, 319], "unnecessari": [82, 85], "unquant": 251, "unset": 257, "unsign": [202, 257, 336], "unsqueez": [236, 301], "unsupport": [250, 257], "until": [1, 298, 299, 302, 323, 326], "unutbu": 138, "unzip": [250, 289, 319, 330], "up": [6, 7, 26, 27, 32, 39, 192, 252, 257, 258, 278, 286, 287, 292, 305, 339], "up_proj": 341, "updat": [31, 39, 177, 236, 243, 252, 257, 291, 317, 337], "update_bia": 27, "update_buff": 243, "updatebia": 257, "upgrdt": 105, "upon": [272, 278, 282, 283, 315, 316], "upper_op": 107, "upsample_r": 241, "upward": [107, 108, 257], "url": [270, 279, 280], "us": [1, 3, 6, 7, 9, 20, 26, 31, 32, 33, 39, 50, 53, 54, 55, 56, 57, 61, 76, 80, 81, 82, 83, 84, 85, 90, 92, 95, 98, 99, 104, 110, 111, 118, 121, 122, 127, 128, 131, 135, 136, 139, 140, 143, 150, 151, 154, 156, 159, 177, 178, 181, 182, 183, 192, 193, 194, 198, 199, 202, 205, 206, 209, 217, 222, 230, 235, 236, 241, 244, 249, 250, 251, 254, 255, 257, 258, 259, 261, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 278, 279, 280, 283, 284, 285, 286, 287, 289, 290, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 307, 308, 310, 311, 312, 314, 315, 318, 323, 325, 326, 328, 330, 333, 334, 335, 336, 339, 340, 341, 342], "usabl": [323, 326], "usag": [6, 7, 182, 192, 251, 259, 291, 294, 295, 296, 297, 298, 320, 342], "use_compiler_version_cpu_kernel": 257, "use_dynamic_qu": [57, 257, 300], "use_external_data_format": [3, 55, 57, 61, 257, 290, 291, 300], "use_fast": 316, "use_gptq": 274, "use_moving_averag": 278, "use_pof2": 55, "use_sc": 55, "use_temp_fil": 156, "use_unsigned_relu": 39, "used_scale_zp_map": 39, "usefp32scal": 257, "usegptq": 257, "usematmulnbit": 257, "useqdqvitiscustomop": 257, "user": [1, 55, 56, 118, 177, 186, 193, 198, 226, 228, 251, 257, 260, 265, 288, 289, 298, 308, 309, 315, 316, 318, 320, 329, 333, 338, 340, 341], "userandomdata": [257, 260], "userandomhad": [254, 257], "usestim": 118, "useunsignedrelu": 257, "usual": [3, 118, 257, 279, 280], "util": [27, 76, 99, 121, 122, 127, 128, 131, 135, 136, 182, 186, 193, 198, 205, 250, 251, 257, 260, 265, 266, 267, 268, 269, 270, 271, 277, 285, 288, 294, 299, 302, 304, 305, 308, 310, 313, 320, 333, 337, 338], "v": [199, 257, 316, 336, 339], "v0": [287, 315, 316, 342], "v01": [316, 342], "v1": [267, 284, 286, 289, 292, 309, 317, 320, 342], "v2": 319, "v_proj": [150, 199, 316, 337, 339, 341], "vae": 309, "vai_lib_path": [259, 285, 295, 301], "vaiml": 257, "val": [266, 268, 269, 270, 271, 275, 277], "val_data": [266, 267, 268, 269, 270, 271, 275, 277], "val_imag": [266, 267, 268, 269, 271, 275, 277], "val_load": 319, "valid": [1, 6, 7, 39, 118, 186, 192, 238, 257, 266, 267, 268, 269, 270, 271, 275, 277, 319, 320, 342], "valu": [1, 3, 6, 7, 9, 11, 39, 54, 55, 61, 78, 86, 99, 110, 118, 147, 150, 183, 192, 199, 202, 205, 236, 238, 241, 243, 252, 253, 254, 255, 257, 258, 259, 261, 262, 264, 279, 280, 285, 291, 295, 296, 297, 298, 302, 303, 304, 306, 307, 316, 319, 320, 324, 332, 334, 336, 337, 339], "valueerror": [3, 39, 56, 241, 290], "values_idx": 1, "var": 177, "vari": [263, 298, 333], "variabl": [9, 124, 177, 233, 243, 250, 301, 307, 316], "variat": [266, 267, 268, 269], "varieti": [249, 251, 318, 320], "variou": [33, 53, 61, 213, 248, 249, 251, 252, 253, 254, 255, 257, 259, 263, 295, 298, 299, 308, 333, 336, 340], "vector": [236, 339], "verbos": [150, 291], "veri": [150, 257, 291, 303, 307, 337, 340], "verif": 184, "verifi": [249, 250, 337], "verifici": 186, "versatil": [252, 320], "version": [39, 55, 90, 150, 193, 198, 228, 234, 237, 241, 251, 257, 258, 291, 301, 309, 316, 317], "vi": 257, "via": [178, 285, 310, 318, 324], "view": 236, "vilid": 270, "vision": [267, 275, 284, 286, 292, 310, 311, 316, 330, 342], "visit": 324, "visual": [205, 250, 301], "vit": 257, "viti": [39, 54, 257], "vitisbfpquant": 54, "vitisdequantizelinear": [257, 301, 342], "vitisextendedquant": 54, "vitisinstancenorm": 342, "vitislstm": 342, "vitisonnxquant": [39, 54], "vitisqdq": [101, 293, 298], "vitisqdqcpuquant": [4, 63], "vitisqdqnpucnnquant": 54, "vitisqdqquant": [4, 54, 63], "vitisquantformat": [55, 57, 257, 258, 259, 294, 295, 296, 297, 298, 301], "vitisquantizelinear": [257, 301, 342], "vitisquanttyp": [3, 55, 57, 257, 259, 294, 295, 296, 297, 298, 301], "vjp": [6, 7, 192], "vllm": [150, 321, 323, 326, 342], "vlm": [311, 313, 314], "vmaf": [98, 293], "vnni": 61, "vocab": [159, 273, 274, 276, 281], "w": [6, 7, 192, 217, 236, 303, 309, 337], "w3": 336, "w4": 336, "w8": 342, "w8a8": [339, 340, 342], "w_alpha": 13, "w_bfp16": 316, "w_bfp16_a_bfp16": 316, "w_fp8_a_fp8": [316, 336], "w_fp8_a_fp8_per_tensor": 341, "w_fp8_a_fp8_per_tensor_config": 341, "w_int4_per_channel_sym": 325, "w_int4_per_group_sym": [316, 325], "w_int8_a_int8_per_tensor": [339, 341], "w_int8_a_int8_per_tensor_config": 341, "w_int8_a_int8_per_tensor_sym": [316, 336, 339], "w_int8_per_channel_a_int8_per_tensor": 340, "w_int8_per_channel_a_int8_per_tensor_kv_cache_int8_per_tensor": 340, "w_int8_per_channel_a_int8_per_token": 340, "w_int8_per_channel_a_int8_per_token_kv_cache_int8_per_token": 340, "w_int8_per_tensor": 339, "w_int8_per_tensor_a_int8_per_tensor": 340, "w_int8_per_tensor_a_int8_per_tensor_kv_cache_int8_per_tensor": 340, "w_mx6": 316, "w_mx6_a_mx6": 316, "w_mx_fp8": 316, "w_mx_fp8_a_mx_fp8": 316, "w_uint4_a_bfloat16_per_group_asym": 325, "w_uint4_asym": 317, "w_uint4_per_group": 341, "w_uint4_per_group_asym": [316, 324, 325, 336], "w_uint4_per_group_config": 341, "wa": [118, 205, 257, 266, 268, 269, 271, 274, 275, 276, 277, 278, 281, 301], "wai": [6, 7, 9, 121, 122, 124, 127, 128, 131, 135, 136, 139, 140, 143, 156, 177, 178, 182, 192, 194, 199, 206, 209, 222, 233, 236, 241, 243, 280, 284, 301, 303], "walk": 309, "want": [33, 39, 54, 183, 198, 255, 257, 259, 266, 267, 295, 296, 297, 304, 310, 316, 320, 339, 340, 341], "warm_start": 27, "warn": [57, 118, 186, 194, 199, 257, 291], "we": [1, 9, 33, 39, 55, 140, 150, 180, 183, 199, 212, 213, 215, 217, 226, 235, 242, 250, 255, 257, 259, 265, 267, 270, 278, 279, 280, 284, 291, 293, 294, 295, 296, 298, 300, 301, 305, 307, 308, 310, 311, 312, 316, 318, 319, 320, 326, 336, 337, 338, 339, 340, 341], "websit": [266, 267, 268, 269, 271, 275, 277], "weight": [7, 8, 9, 13, 14, 15, 16, 17, 22, 25, 26, 33, 39, 54, 55, 57, 61, 66, 71, 76, 110, 112, 140, 144, 150, 151, 169, 177, 178, 180, 181, 182, 183, 198, 199, 202, 205, 218, 219, 220, 221, 228, 235, 236, 249, 251, 252, 253, 255, 257, 258, 263, 279, 280, 283, 285, 286, 287, 291, 292, 294, 297, 298, 300, 301, 303, 304, 307, 309, 317, 319, 320, 322, 323, 325, 326, 333, 335, 336, 338, 339, 340, 341, 342], "weight_fli": 150, "weight_format": [150, 151, 323, 326], "weight_group": 150, "weight_merge_group": 151, "weight_nam": [39, 54], "weight_qtyp": [4, 39, 54, 63], "weight_scal": [39, 54], "weight_spec": 182, "weight_stat": 307, "weight_stats_hook": 205, "weight_tensor_nam": 298, "weight_tensor_name1": 298, "weight_tensor_name2": 298, "weight_typ": [36, 55, 57, 61, 252, 253, 254, 255, 257, 259, 279, 280, 284, 285, 294, 295, 296, 297, 298, 299, 300, 301], "weight_zero_point": 323, "weightonlyquantconfig": 66, "weights_onli": 54, "weights_only_quant": [276, 282], "weightscal": [257, 294], "weightsonli": 257, "weightsymmetr": [61, 257, 279, 280, 284, 299], "weighttargetquanttyp": [257, 298], "well": [180, 205, 251, 257, 259, 263, 293, 295, 301, 308, 316, 333, 337], "were": [6, 7, 192, 252], "wether": 284, "wget": [267, 273, 274, 275, 276, 281, 284, 286, 287, 292], "what": [32, 33, 50, 228, 302], "wheel": 250, "when": [3, 9, 32, 55, 61, 118, 124, 150, 177, 180, 183, 199, 233, 236, 243, 250, 252, 253, 254, 257, 259, 260, 263, 270, 279, 280, 284, 285, 291, 293, 294, 295, 296, 297, 298, 299, 301, 303, 304, 307, 309, 315, 316, 317, 320, 331, 333, 335, 336, 337, 339, 342], "where": [55, 56, 107, 118, 156, 186, 199, 212, 236, 251, 257, 258, 259, 261, 279, 280, 291, 295, 296, 297, 298, 301, 303, 306, 314, 337, 339], "whether": [3, 6, 7, 9, 32, 33, 39, 53, 54, 55, 124, 150, 151, 177, 181, 183, 192, 199, 228, 233, 243, 252, 253, 254, 255, 257, 291, 299, 302, 303, 307, 319, 338, 341], "which": [1, 3, 7, 20, 22, 25, 32, 33, 34, 39, 50, 71, 76, 118, 149, 150, 154, 198, 199, 213, 226, 228, 236, 241, 244, 250, 251, 252, 257, 259, 263, 266, 267, 268, 269, 271, 274, 275, 276, 277, 278, 279, 280, 281, 284, 291, 294, 295, 297, 298, 299, 300, 301, 302, 303, 305, 307, 310, 316, 319, 320, 326, 333, 335, 336, 337, 339, 342], "while": [243, 251, 252, 257, 259, 263, 267, 290, 294, 295, 296, 297, 298, 299, 301, 302, 303, 308, 320, 326, 333, 336], "whl": [250, 289, 330], "who": 257, "whole": [177, 183, 270, 303, 320], "whose": [32, 54, 119, 252, 257, 291, 298], "why": [9, 302, 303], "wide": [249, 286, 287, 292, 298, 307, 320, 336, 338], "wider": 257, "width": [183, 251, 257, 279, 280, 298, 299, 303, 308, 336], "wiki": 320, "wiki2": 315, "wikitext": [305, 317, 320, 339], "wikitext2": [272, 273, 274, 276, 278, 281, 282, 283, 313, 320], "wikitext_for_gptq_benchmark": 336, "wildcard": 151, "window": [249, 250, 258, 259, 294, 295, 296, 297, 301, 304, 331, 342], "wise": [27, 252, 286, 287, 292, 298, 299, 341, 342], "wish": [257, 291], "with_cast": 293, "with_mixed_precis": 277, "within": [55, 119, 183, 199, 202, 205, 241, 252, 254, 257, 259, 267, 270, 278, 279, 280, 284, 295, 296, 297, 298, 299, 303, 308, 316], "without": [22, 95, 99, 140, 236, 249, 251, 252, 257, 266, 270, 293, 294, 296, 297, 298, 302, 303, 304, 307, 308, 319, 320, 331, 335, 336, 337, 339, 342], "won": 186, "word": [150, 236, 323, 325], "work": [7, 82, 85, 99, 149, 251, 257, 291, 298, 302, 324], "worker": 309, "workflow": [285, 319, 342], "world": [299, 310], "wors": [260, 337], "would": [140, 323, 337], "wrap": 241, "wrapper": [13, 14, 15, 16, 17, 22, 25, 26, 34, 138, 140], "wrapper_fn": 138, "write": [205, 257, 320], "written": 154, "x": [9, 55, 124, 140, 141, 159, 177, 192, 215, 217, 233, 241, 243, 310, 311, 313, 314, 320, 337, 339], "x_orig": 241, "x_q": 303, "x_r": 303, "xdoctest": [6, 7, 177, 192, 236], "xilinx": 182, "xint8": [258, 260, 275, 286, 287, 292, 300], "xint8_adaqu": 300, "xint8_adaround": 300, "xint8_config": 258, "xl": 309, "xsum": 314, "xw": 337, "xzf": [266, 267, 268, 269, 271, 275, 277], "y": 337, "yaml": 228, "ye": 291, "yet": 316, "yield": [118, 149, 252, 260, 336], "yolo": [287, 319, 342], "yolov": [279, 280], "yolov3": [270, 279, 280], "yolov3_float_onnx_path": 270, "yolov8": 287, "yolov8n": 287, "you": [6, 7, 9, 32, 33, 124, 177, 192, 198, 233, 241, 243, 250, 251, 252, 257, 259, 262, 263, 266, 267, 268, 269, 270, 271, 275, 277, 279, 280, 284, 285, 286, 287, 291, 292, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 307, 308, 309, 310, 314, 316, 317, 319, 320, 326, 330, 331, 333, 335, 336, 337], "your": [6, 7, 9, 124, 177, 192, 233, 241, 243, 249, 250, 251, 252, 257, 258, 260, 265, 266, 267, 268, 269, 271, 275, 277, 279, 280, 284, 286, 287, 292, 293, 301, 305, 309, 310, 316, 318, 336, 342], "your_model": 316, "z": [55, 251, 320], "zero": [39, 55, 182, 183, 199, 202, 234, 235, 236, 244, 251, 252, 257, 264, 279, 280, 290, 294, 302, 319, 323, 326, 334], "zero_point": [7, 55, 150, 177, 192, 320, 335], "zero_point_shap": 178, "zeropint": 178, "zeropoint": 150, "zeropointtyp": 202, "zhang": 181, "zip": [289, 319, 320, 330], "zp": 7, "\u2463": 316, "\u2464": 316, "\u2465": 316}, "titles": ["<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.auto_search</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.bias_correction</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.calibrate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.cpu_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.equalization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.base_fn_quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.base_qdq_quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.create_model_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_base_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_conv_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_gemm_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_matmul_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.create_torch.quant_norm_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.fast_finetune</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.onnx_evaluate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.onnx_subgraph</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.torch_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.torch_utils_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model_loss</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.finetuning.train_torch.train_model_param</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.gptq.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.model_transformer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.model_transformer_test</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.transforms</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.graph_transformations.transforms_pipeline</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision.auto_mixprecision</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.mprecision.mixed_bfp</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.onnx_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.custom_ops.build_vai_custom_op</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.custom_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.concat</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.hardsigmoid</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.layernorm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.prelu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.qdq_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.operators.vai_ops.softmax</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations.convert_transforms</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations.convert_transforms_pipeline</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimizations</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.optimize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.qdq_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quant_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config.custom_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.bfp_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.cpu_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.extended_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.matmul_nbits_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.npu_cnn_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.npu_transformer_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.onnx_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quantizers.qdq_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.refine</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.registry</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.simulate_dpu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.simulate_dpu_softmax</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.smooth_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_a8w8_npu_to_a8w8_cpu</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_customqdq_to_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_dynamic_to_fixed</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_bf16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_bfp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp16_to_fp32</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_bf16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_bfp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_fp32_to_fp16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_lstm_to_customlstm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_nchw_to_nhwc</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_onnx_to_onnxtxt</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_onnxtxt_to_onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_opset_version</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_qdq_to_qop</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_quant_to_float</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_resize_fs_to_pof2s</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_s8s8_to_u8s8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_shared_initializer_to_unique</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_u16s8_to_s16s8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.convert_u16u8_to_u8u8</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.evaluate</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.float16</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.insert_clip_bfloat16_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.print_a16w8_a8w8_nodes</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.random_quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_bf16_cast</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_initializer_from_input</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq_between_ops</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.remove_qdq_mul_add</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.replace_bfloat16_qdq_cast</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.replace_inf_weights</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.save_tensor_hist</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.tools.save_weights_hist</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.onnx.utils.model_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.import_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.log</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.shares.utils.testing_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.auto_smooth</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.awq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.modules.act</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.scale</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.awq.smooth</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning.blockwise_tuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning.blockwise_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.blockwise_tuning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.gptq.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.gptq</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.osscar</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.osscar.osscar</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.monkeypatch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.quarot</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.quarot.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.hadamard</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.rotation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.rotation.rotation_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.auto_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.module</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.prepare</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.algorithm.utils.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.constants</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.gguf_model_converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.gguf_model_writer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.tensor_convert</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.gguf_export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.llm_info</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.llm_info_builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.builder.native_model_info_builder</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.converter.llm_info_converter</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.json_export.utils.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export.model_post_process</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_export.quant_config_parser</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_import</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.main_import.pretrained_config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules.qparamslinear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.nn.modules.realquantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.onnx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.export.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.algos</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.mapping</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions.brevitas.verification</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.extensions</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation.extensions</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation.hw_emulation_interface</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel.hw_emulation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.kernel</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.model_transformation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.pruning.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.api</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.config_verification</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.type</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.config.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.constants</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.debug</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.fx.base</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.fx</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.activate_dropout</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.model_optimization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.modify_reshape_param</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.opt_pass_manager</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.post_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.post_quant.opt_pass_after_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.opt_pass_before_quant</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_conv2d_to_qtconv2d</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_conv_bn_to_qt_model</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_convtranspose2d_to_qtconvtranspose2d</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.pre_quant.replace_linear_to_qtlinear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.remove_dropout_node</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.optimization.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.insert_quantizer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.processor</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.processor.processor_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.graph.torch_utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.model_transformation</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.mixin</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_conv</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_conv_bn_fused</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_embed</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.modules.quantize_linear</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.nn.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.lsq_observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.observer.tqt_observer</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.tensor_quantize</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.quantization.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.utils</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.torch.utils.pack</span></code>", "<code class=\"xref py py-mod docutils literal notranslate\"><span class=\"pre\">quark.version</span></code>", "Basic Usage", "Welcome to AMD Quark Documentation!", "Installation Guide", "Quantization with AMD Quark", "Quantization Using AdaQuant and AdaRound", "Quantizing Using CrossLayerEqualization (CLE)", "QuaRot", "SmoothQuant (SQ)", "Accuracy Improvement Algorithms", "Full List of Quantization Configuration Features", "AMD Quark for ONNX", "BFP16 (Block floating point) Quantization", "Adding Calibration Datasets", "Calibration Methods", "Quantization Schemes", "Quantization Strategies", "Quantization Symmetry", "Using ONNX Model Inference and Saving Input Data in NPY Format", "Block Floating Point (BFP) Example", "Microscaling (MX) Example", "Quark ONNX Quantization Example", "Quark ONNX Quantization Example", "Quark ONNX Quantization Example", "Quark ONNX Example for CrossLayerEqualization (CLE)", "Dynamic Quantization for Llama-2-7b", "Dynamic Quantization for OPT-125M", "Quantizating a model with GPTQ", "Quantizing a ResNet50-v1-12 Model", "Quantizing an OPT-125M Model", "Quantization using Mixed Precision", "Quark ONNX Quantization Example", "Auto Search for Best Practice of RyzenAI ONNX Model Quantization", "Auto Search for Best Practice of RyzenAI ONNX Model Quantization", "Quantization using SmoothQuant", "Quantizating Llama-2-7b model using MatMulNBits quantizer", "Quark ONNX Quantization Example", "Best Practice for Ryzen AI in Quark ONNX", "Accelerate with GPUs", "Best Practice for Ryzen AI in Quark ONNX", "Best Practice for Ryzen AI in Quark ONNX", "AMD Quark APIs for ONNX", "Accessing ONNX Examples", "Frequently Asked Questions (FAQ)", "Optional Utilities", "Best Practice for Ryzen AI in AMD Quark ONNX", "Tools", "Introduction", "Introduction", "Introduction", "Microscaling (MX)", "Mixed Precision", "Automatic Search for Model Quantization", "Configuring ONNX Quantization", "Supported Data and Op Types", "Using MX (Microscaling)", "Two Level Quantization Formats (MX4, MX6, MX9: shared Microexponents)", "AMD Quark for PyTorch", "Adding Calibration Datasets", "Calibration Methods", "Debugging quantization degradation in AMD Quark", "Brevitas Integration", "Diffusion Model Quantization using Quark", "Language Model Evaluations in Quark", "LM-Evaluation Harness Evaluations", "LM-Evaluation Harness (Offline)", "Perplexity Evaluations", "Rouge &amp; Meteor Evaluations", "Pruning", "Language Model Post Training Quantization (PTQ) Using Quark", "Language Model QAT Using Quark", "Integration with AMD Pytorch-light (APL)", "Vision Model Quantization Using Quark FX Graph Mode", "Bridge from Quark to llama.cpp", "Exporting Quantized Models", "GGUF Exporting", "HuggingFace Format", "Exporting Using ONNX Runtime Gen AI Model Builder", "ONNX Exporting", "Quark Format", "Extensions for PyTorch", "Language Model Optimization", "Quark APIs for PyTorch", "Accessing PyTorch Examples", "Frequently Asked Questions (FAQ)", "Quantization Schemes", "Quantization Strategies", "Quantization Symmetry", "Save &amp; Load Quantized Models", "Best Practices for Post-Training Quantization (PTQ)", "Activation/weight smoothing (SmoothQuant)", "BFP16 (Block floating point) Quantization", "Rotation-based quantization with QuaRot", "Quantizing with Rotation and SmoothQuant", "Configuring PyTorch Quantization", "Release Notes"], "titleterms": {"0": 342, "1": [301, 302, 310, 312, 315, 316, 317, 318, 319, 320, 338, 341, 342], "12": 275, "125m": [273, 276], "2": [272, 282, 301, 302, 310, 312, 315, 316, 317, 318, 319, 320, 338, 341, 342], "3": [301, 302, 310, 312, 316, 317, 318, 320, 338, 341], "4": [301, 302, 310, 312, 316, 338, 341], "5": [302, 310, 312, 316, 342], "6": [312, 316, 342], "7": [316, 342], "7b": [272, 282], "8": [316, 342], "For": [260, 265], "It": 320, "Not": 316, "Of": 312, "Their": 251, "These": 303, "With": [271, 274, 277, 281], "a16w8": 293, "a8w8": 293, "a_float16": 316, "acceler": 285, "access": [289, 330], "accuraci": [256, 259, 289, 293, 294, 295, 296, 297], "act": 124, "activ": 337, "activate_dropout": 209, "actual": 251, "ad": [260, 305], "adaqu": [252, 266, 268], "adaround": [252, 269], "ai": [284, 286, 287, 289, 292, 324], "algo": 181, "algoconfig": 341, "algorithm": [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 256, 336], "amd": [249, 251, 252, 258, 259, 260, 288, 289, 290, 292, 294, 296, 297, 298, 302, 304, 307, 318, 331], "an": [276, 312], "analysi": 298, "api": [56, 120, 150, 154, 182, 193, 198, 288, 329], "apl": 318, "appli": [302, 336], "ar": [251, 302, 303], "argument": [252, 253, 254, 255, 311, 313, 314], "ask": [290, 331], "asymmetr": 316, "attribut": [41, 228], "auto": [279, 280, 284], "auto_config": 145, "auto_mixprecis": 36, "auto_search": [1, 270], "auto_smooth": 121, "automat": [298, 299, 316], "awq": [121, 122, 123, 124, 125, 126, 127, 316, 324], "base": [206, 298, 339], "base_fn_quant": 6, "base_qdq_quant": 7, "baselin": 312, "basic": [248, 258, 304], "benchmark": 309, "benefit": [252, 259, 295, 298], "best": [279, 280, 284, 286, 287, 292, 336], "between": 293, "bf16": 294, "bfloat16": [293, 301], "bfp": 266, "bfp16": [259, 266, 285, 293, 295, 301, 316, 338], "bfp_quantiz": 62, "bias_correct": 2, "block": [259, 266, 302, 338], "blockwise_tun": [128, 129, 130], "blockwise_util": 129, "brevita": [181, 182, 183, 184, 185, 186, 308], "bridg": 320, "build_vai_custom_op": 40, "builder": [161, 162, 163, 164, 324], "c": 331, "cach": 316, "calcul": 302, "calibr": [3, 260, 261, 284, 285, 286, 287, 292, 302, 305, 306, 309], "call": [279, 280], "class": [1, 3, 4, 5, 6, 7, 8, 9, 13, 14, 15, 16, 17, 21, 22, 25, 26, 27, 31, 32, 33, 34, 39, 50, 51, 53, 54, 55, 56, 57, 59, 63, 66, 71, 76, 111, 118, 121, 122, 123, 124, 127, 128, 130, 131, 132, 134, 135, 136, 139, 140, 143, 144, 149, 150, 151, 156, 159, 160, 162, 177, 178, 181, 182, 183, 186, 188, 192, 193, 194, 195, 198, 199, 202, 206, 209, 213, 215, 217, 222, 229, 232, 233, 234, 235, 236, 237, 240, 241, 242, 243, 260], "classif": [289, 319], "cle": [253, 271], "code": [260, 265, 319], "compar": 312, "compil": 331, "compon": 299, "concat": 43, "concept": [259, 295], "conclus": [299, 302, 308], "config": [57, 58, 59, 151, 152, 183, 194, 199, 200, 201, 202, 203, 279, 280, 341], "config_verif": 200, "configur": [257, 300, 302, 316, 338, 341], "consider": 251, "constant": [153, 204], "content": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 41, 50, 51, 53, 54, 55, 56, 57, 59, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 120, 121, 122, 123, 124, 127, 128, 130, 131, 132, 134, 135, 136, 138, 139, 140, 141, 143, 144, 147, 149, 150, 151, 154, 156, 159, 160, 162, 169, 177, 178, 180, 181, 182, 183, 186, 188, 190, 192, 193, 194, 195, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 229, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244], "context": 303, "controlnet": 309, "conv": 293, "convert": [165, 166, 251, 293], "convert_a8w8_npu_to_a8w8_cpu": 77, "convert_customqdq_to_qdq": 78, "convert_dynamic_to_fix": 79, "convert_fp16_to_bf16": 80, "convert_fp16_to_bfp16": 81, "convert_fp16_to_fp32": 82, "convert_fp32_to_bf16": 83, "convert_fp32_to_bfp16": 84, "convert_fp32_to_fp16": 85, "convert_lstm_to_customlstm": 86, "convert_nchw_to_nhwc": 87, "convert_onnx_to_onnxtxt": 88, "convert_onnxtxt_to_onnx": 89, "convert_opset_vers": 90, "convert_qdq_to_qop": 91, "convert_quant_to_float": 92, "convert_resize_fs_to_pof2": 93, "convert_s8s8_to_u8s8": 94, "convert_shared_initializer_to_uniqu": 95, "convert_transform": 50, "convert_transforms_pipelin": 51, "convert_u16s8_to_s16s8": 96, "convert_u16u8_to_u8u8": 97, "correspond": 320, "cpp": 320, "cpu": [293, 316], "cpu_quant": [4, 63], "create_model": 8, "create_model_op": 9, "create_model_test": 10, "create_model_util": 11, "create_torch": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], "crosslayerequ": [253, 271], "custom": 300, "custom_config": 58, "custom_op": [40, 41], "data": [251, 260, 265, 266, 267, 268, 269, 270, 271, 275, 277, 284, 286, 287, 292, 301, 302, 303], "dataload": 305, "dataread": 260, "dataset": [260, 305, 309, 312], "debug": [205, 307], "default": 300, "degrad": 307, "depend": 309, "design": 308, "detail": [265, 310], "detect": 319, "diagram": 299, "dict": 305, "differ": 336, "diffus": 309, "do": [320, 338], "document": 249, "doe": [251, 320, 337], "dtype": 320, "dump": 291, "dynam": [272, 273, 289, 302], "eager": 335, "effici": 249, "enabl": [252, 259, 295, 296, 297, 298], "entir": 309, "entropi": 261, "environ": [285, 331], "equal": 5, "establish": 341, "eval": 312, "evalu": [98, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 291, 293, 310, 311, 312, 313, 314, 315, 316, 317], "exampl": [252, 253, 254, 255, 258, 259, 260, 266, 267, 268, 269, 270, 271, 278, 283, 289, 295, 296, 297, 304, 318, 319, 322, 323, 325, 326, 330, 335], "exclud": 336, "expand": 340, "experi": [319, 320], "experiment": 316, "explan": 341, "export": [150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 291, 309, 310, 312, 316, 317, 320, 321, 322, 323, 324, 325, 326], "extended_quant": 64, "extens": [181, 182, 183, 184, 185, 186, 187, 189, 308, 327], "fake": 251, "faq": [290, 331], "fast": [252, 285], "fast_finetun": 18, "featur": [249, 257, 258, 304, 308, 310, 342], "file": 309, "fine": 319, "finetun": [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 252, 285, 317], "flexibl": 249, "float": [259, 266, 291, 338], "float16": [99, 293, 301, 315, 316], "float32": [293, 301], "flow": 299, "folder": 293, "format": [265, 267, 301, 303, 323, 326], "fp8": 316, "fp8_e4m3": 316, "frequent": [290, 331], "from": [250, 287, 312, 320], "full": 257, "function": [1, 3, 5, 9, 11, 18, 20, 22, 32, 36, 41, 53, 55, 59, 61, 72, 74, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 114, 119, 120, 138, 140, 141, 144, 147, 150, 154, 169, 180, 188, 190, 198, 199, 203, 205, 211, 212, 218, 219, 220, 221, 225, 226, 228, 229, 230, 235, 238, 244], "further": [259, 294, 295, 296, 297], "fx": [206, 207, 319, 335], "gen": 324, "gener": 316, "get": [310, 312, 319], "gguf": [320, 322], "gguf_export": [154, 155, 156, 157, 158, 159, 316], "gguf_model_convert": 155, "gguf_model_writ": 156, "gptq": [28, 29, 131, 132, 274], "gpu": 285, "grain": 319, "graph": [206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 319, 335], "graph_transform": [30, 31, 32, 33, 34], "guid": [250, 310, 319], "guidelin": 278, "hadamard": 141, "happen": 251, "har": [310, 311, 312], "hardsigmoid": 44, "hardwar": 303, "hf": 323, "how": [252, 259, 294, 295, 296, 297, 298, 302, 303, 320, 337, 338], "hqq": 282, "huggingfac": 323, "hw_emul": [189, 190, 191], "hw_emulation_interfac": 190, "i": [251, 295, 296, 297, 298, 302, 320], "imag": [289, 293, 319], "import": [310, 323, 326], "import_util": 116, "improv": [256, 259, 289, 294, 295, 296, 297], "inf": 293, "infer": [265, 285, 287], "input": [260, 265, 293], "insert_clip_bfloat16_qdq": 101, "insert_quant": 225, "instal": [250, 302, 316, 324, 338], "int": 316, "int16": 301, "int32": 301, "integ": [251, 303], "integr": [308, 318], "interest": 312, "intern": 251, "introduct": [278, 294, 295, 296, 302, 318, 320, 338, 340], "issu": [290, 331], "json_export": [161, 162, 163, 164, 165, 166, 167, 168, 169], "json_safetensors_export": 316, "kei": [249, 259, 295], "kernel": [189, 190, 191, 192], "kv": 316, "languag": [289, 310, 316, 317, 328], "layer": 336, "layernorm": 46, "level": 303, "licens": [270, 276, 278, 294, 295, 296, 297, 298, 299], "light": 318, "list": [257, 305, 316], "llama": [272, 282, 316, 320], "llama2": 315, "llm": 317, "llm_eval_haness_offlin": 310, "llm_eval_har": 310, "llm_info": 162, "llm_info_build": 163, "llm_info_convert": 166, "lm": [310, 311, 312], "load": [309, 335], "log": 118, "lsq_observ": 240, "main_export": [170, 171, 172], "main_import": [173, 174], "map": [185, 303], "matmul_nbits_quant": 66, "matmulnbit": 282, "mean": 251, "meteor": 314, "method": [261, 306], "microexpon": [296, 303], "microsc": [267, 297, 302], "minmax": 261, "mix": [277, 293, 298, 301], "mixed_bfp": 38, "mixin": 233, "mode": [319, 335], "model": [249, 251, 259, 260, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 289, 290, 291, 292, 293, 295, 296, 297, 299, 301, 302, 309, 310, 311, 312, 313, 314, 315, 316, 317, 319, 320, 321, 324, 328, 335, 338, 340, 341], "model_optim": 211, "model_post_process": 171, "model_transform": [31, 196, 230], "model_transformer_test": 32, "model_util": 114, "modelquant": 308, "modify_reshape_param": 212, "modul": [1, 3, 4, 5, 6, 7, 8, 9, 11, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 27, 31, 32, 33, 34, 36, 39, 50, 51, 53, 54, 55, 56, 57, 61, 63, 66, 71, 72, 74, 76, 78, 86, 91, 92, 93, 99, 106, 107, 108, 110, 111, 114, 118, 119, 120, 121, 122, 124, 125, 127, 128, 131, 135, 136, 138, 139, 140, 141, 143, 144, 147, 149, 150, 151, 154, 156, 159, 162, 169, 176, 177, 178, 180, 181, 182, 183, 186, 190, 193, 194, 198, 199, 202, 203, 205, 206, 209, 211, 212, 213, 215, 217, 218, 219, 220, 221, 222, 225, 226, 228, 230, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 244, 341], "monkeypatch": 138, "more": [300, 340], "mprecis": [36, 37, 38], "mse": 261, "multi": [260, 265], "mx": [267, 297, 302, 316], "mx4": 303, "mx6": [303, 316], "mx9": [296, 303], "name": 293, "native_model_info_build": 164, "nchw": 293, "new": 342, "nhwc": 293, "nn": [175, 176, 177, 178, 231, 232, 233, 234, 235, 236, 237, 238, 341], "nonoverflow": 261, "note": [303, 342], "npu": 293, "npu_cnn_quant": 67, "npu_transformer_quant": 68, "npy": 265, "object": 319, "observ": [239, 240, 241, 242], "obtain": 303, "ocp": 316, "offlin": 312, "oga": [312, 324], "older": 250, "onli": [289, 302, 309, 316], "onnx": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 179, 249, 252, 258, 259, 265, 268, 269, 270, 271, 278, 279, 280, 283, 284, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 300, 309, 310, 311, 312, 313, 314, 324, 325], "onnx_evalu": 20, "onnx_quant": [39, 69], "onnx_subgraph": 21, "op": 301, "oper": [40, 41, 42, 43, 44, 45, 46, 47, 48, 49], "opt": [273, 276], "opt_pass_after_qu": 215, "opt_pass_before_qu": 217, "opt_pass_manag": 213, "optim": [50, 51, 52, 53, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 312, 328], "option": [287, 291, 341], "origin": 317, "osscar": [134, 135], "other": [301, 311, 313, 314], "outlier": 336, "overal": 341, "overview": [299, 308], "pack": 246, "packag": [41, 59, 123, 130, 132, 134, 160, 188, 192, 195, 229, 232], "paramet": 341, "parti": 309, "path": 260, "per_group": 316, "percentil": 261, "perplex": 313, "pip": [266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 292], "pipelin": 309, "point": [259, 266, 338], "post": [316, 336], "post_quant": [214, 215], "ppl": [310, 313], "practic": [279, 280, 284, 286, 287, 292, 336], "pre": 291, "pre_quant": [216, 217, 218, 219, 220, 221], "precis": [277, 293, 298, 301], "prelu": 47, "prepar": [148, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 292, 315, 316, 317, 324], "prerequisit": 250, "pretrain": 312, "pretrained_config": 174, "print": 293, "print_a16w8_a8w8_nod": 102, "process": [279, 280, 291, 312], "processor": [136, 224, 225, 226, 227], "processor_util": 227, "prompt": 309, "prune": [193, 194, 195, 196, 197, 315], "ptq": [316, 319, 336], "pytorch": [249, 291, 304, 318, 327, 329, 330, 331, 341], "qat": [317, 319, 336], "qdq_op": 48, "qdq_quantiz": [54, 70], "qparamslinear": 177, "quant": [281, 320], "quant_base_op": 13, "quant_config_pars": 172, "quant_conv_op": 14, "quant_gemm_op": 15, "quant_matmul_op": 16, "quant_norm_op": 17, "quant_util": 55, "quantiti": 293, "quantiz": [56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 249, 251, 252, 253, 257, 259, 260, 262, 263, 264, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 286, 287, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 307, 309, 316, 319, 320, 321, 324, 332, 333, 334, 335, 336, 338, 339, 340, 341], "quantizationconfig": [308, 341], "quantizationspec": 341, "quantize_conv": 234, "quantize_conv_bn_fus": 235, "quantize_emb": 236, "quantize_linear": 237, "quark": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 249, 251, 252, 258, 259, 260, 268, 269, 270, 271, 278, 283, 284, 286, 287, 288, 289, 290, 292, 294, 295, 296, 297, 298, 302, 304, 307, 309, 310, 316, 317, 319, 320, 324, 326, 329, 330, 331, 337, 338], "quarot": [71, 137, 138, 139, 140, 254, 278, 339], "question": [290, 331], "quick": 319, "random": [260, 293], "random_quant": 103, "realquant": 178, "recip": [311, 313, 314, 315, 316, 317], "refer": 312, "refin": 72, "registri": 73, "releas": [289, 330, 342], "reload": 317, "remove_bf16_cast": 104, "remove_dropout_nod": 222, "remove_initializer_from_input": 105, "remove_qdq": 106, "remove_qdq_between_op": 107, "remove_qdq_mul_add": 108, "replac": 293, "replace_bfloat16_qdq_cast": 109, "replace_conv2d_to_qtconv2d": 218, "replace_conv_bn_to_qt_model": 219, "replace_convtranspose2d_to_qtconvtranspose2d": 220, "replace_inf_weight": 110, "replace_linear_to_qtlinear": 221, "requir": [266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 281, 282, 283, 284, 286, 287, 292, 302], "resnet50": 275, "result": [266, 291, 310, 319, 320, 340], "retriev": 312, "rotat": [141, 142, 143, 144, 339, 340], "rotation_util": 144, "roug": 314, "run": [309, 316, 320], "runtim": 324, "ryzen": [284, 286, 287, 289, 292], "ryzenai": [279, 280, 284], "safetensor": [309, 315, 317], "save": [265, 315, 335], "save_tensor_hist": 111, "save_weights_hist": 112, "scale": [126, 302, 303], "scheme": [262, 320, 332, 336], "score": 312, "script": [309, 315, 316, 317, 319], "search": [279, 280, 284, 299], "sensit": 298, "set": [279, 280, 302, 338, 341], "setup": 285, "share": [115, 116, 117, 118, 119, 303], "simul": 291, "simulate_dpu": 74, "simulate_dpu_softmax": 75, "singl": [260, 265], "smooth": [127, 281, 337], "smooth_quant": 76, "smoothquant": [255, 278, 281, 337, 340], "softmax": 49, "some": 320, "someth": 251, "sq": 255, "start": [310, 319], "static": 316, "step": [308, 312, 320, 341], "str": 305, "strategi": [263, 333], "streamlin": 249, "submodul": [30, 35, 52, 59, 60, 100, 113, 133, 152, 160, 184, 195, 201, 229], "summari": 301, "support": [258, 301, 304, 310, 312, 315, 316, 317], "symmetri": [264, 334], "tabl": [301, 310], "task": [312, 319], "tensor": [302, 305, 341], "tensor_convert": 158, "tensor_quant": 243, "test": 309, "testing_util": 119, "thi": [251, 289, 302, 330], "third": 309, "tip": 291, "tool": [77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 293], "torch": [120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 287, 305, 311, 313, 314, 337, 341], "torch_util": [22, 228], "torch_utils_test": 23, "tqt": 319, "tqt_observ": 242, "train": [316, 336], "train_model": 25, "train_model_loss": 26, "train_model_param": 27, "train_torch": [24, 25, 26, 27], "transform": 33, "transforms_pipelin": 34, "try": 336, "turn": 302, "tutori": 316, "two": [293, 303], "type": [202, 251, 301, 303], "u16u8": 293, "u8u8": 293, "uint4": 324, "unet": 309, "uniform": 303, "up": [302, 341], "upgrad": 252, "us": [252, 253, 260, 265, 277, 281, 282, 293, 302, 309, 316, 317, 319, 320, 324, 337, 338], "usag": [248, 299, 316], "user": [310, 319], "util": [113, 114, 116, 117, 118, 119, 140, 145, 146, 147, 148, 149, 159, 168, 169, 180, 197, 203, 223, 238, 244, 245, 246, 291], "v": 310, "v1": 275, "vai_op": [43, 44, 45, 46, 47, 48, 49], "valu": [251, 293], "verif": [186, 250], "version": [247, 250, 342], "vision": 319, "vlm": 310, "w_uint4": 316, "we": 251, "weight": [289, 293, 302, 316, 337], "welcom": 249, "well": 302, "what": [251, 295, 296, 297, 298, 320], "when": 251, "without": [268, 269, 271, 274, 277, 278, 281, 309, 315, 316], "work": [320, 337, 338], "your": 320, "zip": 250}})