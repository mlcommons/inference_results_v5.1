
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.quantization.config.config &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/quantization/config/config';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.quantization.config.config</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.quantization.config.config</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#
&quot;&quot;&quot;Quark Quantization Config API for PyTorch&quot;&quot;&quot;
from __future__ import annotations
import json
import torch.nn as nn
from abc import ABC, abstractmethod
from dataclasses import dataclass, field, asdict
from typing import Union, Optional, Dict, List, Type, Any, cast, TypeVar
from quark.torch.quantization.observer.observer import ObserverBase, PerBlockMXObserver, PlaceholderObserver, \
    PerTensorMinMaxObserver, PerTensorHistogramObserver, PerTensorMSEObserver, PerTensorPercentileObserver, \
    PerChannelMinMaxObserver, PerGroupMinMaxObserver, PerBlockBFPObserver, PerTensorHistogramObserverPro, OBSERVER_MAP
from quark.torch.quantization.config.type import Dtype, ScaleType, RoundType, QSchemeType, DeviceType, QuantizationMode, TQTThresholdInitMeth, ZeroPointType
from quark.torch.quantization.constants import QUARK_LAYER_TYPES
from quark.shares.utils.log import ScreenLogger
from quark.torch.quantization.config.utils import dataclass_pretty_string

logger = ScreenLogger(__name__)

PER_TENSOR_OBSERVER_METHOD_MAP: Dict[str, Type[ObserverBase]] = {
    &quot;min_max&quot;: PerTensorMinMaxObserver,
    &quot;histogram&quot;: PerTensorHistogramObserver,
    &quot;histogrampro&quot;: PerTensorHistogramObserverPro,
    &quot;MSE&quot;: PerTensorMSEObserver,
    &quot;percentile&quot;: PerTensorPercentileObserver
}

SCALE_TYPE_MAP = {&quot;float&quot;: ScaleType.float, &quot;power_of_2&quot;: ScaleType.pof2}

ROUND_METHOD_MAP = {&quot;round&quot;: RoundType.round, &quot;floor&quot;: RoundType.floor, &quot;half_even&quot;: RoundType.half_even}

ZERO_POINT_TYPE_MAP = {&quot;int32&quot;: ZeroPointType.int32, &quot;float32&quot;: ZeroPointType.float32}


def get_per_tensor_observer(observer_method: Optional[str] = None) -&gt; Optional[Type[ObserverBase]]:
    if observer_method:
        assert observer_method in PER_TENSOR_OBSERVER_METHOD_MAP, \
            f&quot;Invalid observer method. Valid observer methods are {list(PER_TENSOR_OBSERVER_METHOD_MAP.keys())}&quot;
        observer_cls = PER_TENSOR_OBSERVER_METHOD_MAP[observer_method]
    else:
        observer_cls = None
    return observer_cls


def get_scale_type(scale_type: Optional[str] = None) -&gt; Optional[ScaleType]:
    if scale_type:
        assert scale_type in SCALE_TYPE_MAP, f&quot;Invalid scale type. Valid scale types are {list(SCALE_TYPE_MAP.keys())}&quot;
        ret = SCALE_TYPE_MAP[scale_type]
    else:
        ret = None
    return ret


def get_round_method(round_method: Optional[str] = None) -&gt; Optional[RoundType]:
    if round_method:
        assert round_method in ROUND_METHOD_MAP, f&quot;Invalid round method. Valid round methods are {list(ROUND_METHOD_MAP.keys())}&quot;
        ret = ROUND_METHOD_MAP[round_method]
    else:
        ret = None
    return ret


def get_zero_point_type(zero_point_type: Optional[str] = None) -&gt; Optional[ZeroPointType]:
    if zero_point_type:
        assert zero_point_type in ZERO_POINT_TYPE_MAP, f&quot;Invalid zero point type, Valid zero point type method are {list(ZERO_POINT_TYPE_MAP.keys())}&quot;
        ret = ZERO_POINT_TYPE_MAP[zero_point_type]
    else:
        ret = None
    return ret


T = TypeVar(&#39;T&#39;, bound=&#39;ConfigBase&#39;)


@dataclass(eq=True)
<div class="viewcode-block" id="ConfigBase">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.ConfigBase">[docs]</a>
class ConfigBase(ABC):

    name = &quot;&quot;

    @classmethod
    def from_dict(cls: Type[T], data: Dict[str, Any]) -&gt; T:
        return cls(**data)

    def update_from_dict(self, data: Dict[str, Any]) -&gt; None:
        for field_name in data:
            setattr(self, field_name, data[field_name])

    def to_dict(self) -&gt; Dict[str, Any]:
        return asdict(self)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="Config">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Config">[docs]</a>
class Config(ConfigBase):
    &quot;&quot;&quot;
    A class that encapsulates comprehensive quantization configurations for a machine learning model, allowing for detailed and hierarchical control over quantization parameters across different model components.

    :param QuantizationConfig global_quant_config: Global quantization configuration applied to the entire model unless overridden at the layer level.
    :param Dict[str, QuantizationConfig] layer_type_quant_config: A dictionary mapping from layer types (e.g., nn.Conv2d, nn.Linear) to their quantization configurations.
    :param Dict[str, QuantizationConfig] layer_quant_config: A dictionary mapping from layer names to their quantization configurations, allowing for per-layer customization. Default is an empty dictionary.
    :param Dict[str, QuantizationConfig] kv_cache_quant_config: A dictionary mapping from layer names to kv_cache quantization configurations. Default is an empty dictionary.
    :param List[str] exclude: A list of layer names to be excluded from quantization, enabling selective quantization of the model. Default is an empty list.
    :param Optional[AlgoConfig] algo_config: Optional configuration for the quantization algorithm, such as GPTQ and AWQ. After this process, the datatype/fake_datatype of weights will be changed with quantization scales. Default is None.
    :param QuantizationMode quant_mode: The quantization mode to be used (eager_mode or fx_graph_mode). Default is eager_mode.
    :param List[PreQuantOptConfig] pre_quant_opt_config: Optional pre-processing optimization, such as Equalization and SmoothQuant. After this process, the value of weights will be changed, but the dtype/fake_dtype will be the same. Default is an empty list.
    :param Optional[int] log_severity_level: 0:DEBUG, 1:INFO, 2:WARNING. 3:ERROR, 4:CRITICAL/FATAL. Default is 1.
    &quot;&quot;&quot;

    # Global quantization configuration applied to the entire model unless overridden at the layer level.
    global_quant_config: QuantizationConfig

    # A dictionary mapping from layer types (e.g., nn.Conv2d, nn.Linear) to their quantization configurations.
    layer_type_quant_config: Dict[Type[nn.Module], QuantizationConfig] = field(default_factory=dict)

    # A dictionary mapping from layer names to their quantization configurations, allowing for per-layer customization.
    layer_quant_config: Dict[str, QuantizationConfig] = field(default_factory=dict)

    # A dictionary mapping from layer names to kv_cache quantization configurations.
    kv_cache_quant_config: Dict[str, QuantizationConfig] = field(default_factory=dict)

    # A list of layer names to be excluded from quantization, enabling selective quantization of the model.
    exclude: List[str] = field(default_factory=list)

    # Optional configuration for the quantization algorithm, such as GPTQ and AWQ
    # After this process, the datatype/fake_datatype of weights will be changed with quantization scales.
    algo_config: Optional[AlgoConfig] = None

    # Optional pre-processing optimization, such as Equalization and SmoothQuant.
    # After this process, the value of weights will be changed, but the dtype/fake_dtype will be the same.
    pre_quant_opt_config: List[PreQuantOptConfig] = field(default_factory=list)

    # The quantization mode to be used (eager_mode or fx_graph_mode)
    quant_mode: QuantizationMode = QuantizationMode.eager_mode

    # Log level for printing on screen
    log_severity_level: Optional[int] = 1

    def to_dict(self) -&gt; Dict[str, Any]:
        config_dict: Dict[str, Any] = {
            &quot;global_quant_config&quot;: self.global_quant_config.to_dict(),
            &quot;exclude&quot;: self.exclude,
            &quot;algo_config&quot;: self.algo_config.to_dict() if self.algo_config is not None else None,
            &quot;quant_method&quot;: &quot;quark&quot;
        }

        layer_type_quant_config_dict: Dict[str, Any] = {}
        for type, config in self.layer_type_quant_config.items():
            layer_type_quant_config_dict[type.__name__] = config.to_dict()
        config_dict[&quot;layer_type_quant_config&quot;] = layer_type_quant_config_dict

        layer_quant_config_dict: Dict[str, Any] = {}
        for name, config in self.layer_quant_config.items():
            layer_quant_config_dict[name] = config.to_dict()
        config_dict[&quot;layer_quant_config&quot;] = layer_quant_config_dict

        config_dict[&quot;quant_mode&quot;] = self.quant_mode.name

        return config_dict

    def __str__(self) -&gt; str:
        s = dataclass_pretty_string(self)
        return s

    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -&gt; Config:
        global_quant_config = QuantizationConfig.from_dict(config_dict[&quot;global_quant_config&quot;])

        # TODO: Deprecate legacy configuration and remove the None check here.
        # Legacy (quark&lt;1.0) configuration used to allow layer_type_quant_config=None in the serialized config, inconstitant with
        # the type hints of the dataclass.
        layer_type_quant_config = {}
        if config_dict[&quot;layer_type_quant_config&quot;] is not None:
            for layer_type_name, layer_type_quantization_config in config_dict[&quot;layer_type_quant_config&quot;].items():
                if layer_type_name in QUARK_LAYER_TYPES:
                    layer_type_quant_config[QUARK_LAYER_TYPES[layer_type_name]] = QuantizationConfig.from_dict(
                        layer_type_quantization_config)
                else:
                    raise NotImplementedError(
                        f&quot;Quark does not support reloading a quantization `Config` from a dictionary using custom `layer_type_quantization_config`. Found `&#39;{layer_type_name}&#39;` in `layer_type_quantization_config`, which is not among the supported {QUARK_LAYER_TYPES}.&quot;
                    )

        # TODO: Deprecate legacy configuration and remove the None check here.
        # Legacy (quark&lt;1.0) configuration used to allow layer_quant_config=None in the serialized config, inconstitant with
        # the type hints of the dataclass.
        if config_dict[&quot;layer_quant_config&quot;] is not None:
            layer_quant_config = {
                layer_name: QuantizationConfig.from_dict(quant_config_dict)
                for layer_name, quant_config_dict in config_dict[&quot;layer_quant_config&quot;].items()
            }
        else:
            layer_quant_config = {}

        # TODO: Deprecate legacy (quark&lt;1.0) configuration and remove the check here.
        # `exclude` used to be serialized as `None` when there was no exclude layer, instead of `[]`.
        if config_dict[&quot;exclude&quot;] is None:  # pragma: no cover
            exclude = []
        else:
            exclude = config_dict[&quot;exclude&quot;]

        algo_config = _load_quant_algo_config_from_dict(
            config_dict[&quot;algo_config&quot;]) if config_dict[&quot;algo_config&quot;] is not None else None

        if &quot;quant_mode&quot; in config_dict:
            quant_mode = QuantizationMode[config_dict[&quot;quant_mode&quot;]]  # Access by name and not by value.
        else:
            # TODO: Deprecate legacy (quark&lt;1.0) configuration and remove the check here.
            # The key `&quot;quant_mode&quot;` used not to be serialized in the legacy quantization_config, inconstitant with
            # the type hints of the dataclass.
            quant_mode = QuantizationMode.eager_mode

        log_severity_level = 1  # `log_severity_level` is not saved.

        # TODO: Currently, `pre_quant_opt_config` is not saved but it might be nice to have?
        return cls(global_quant_config=global_quant_config,
                   layer_type_quant_config=layer_type_quant_config,
                   layer_quant_config=layer_quant_config,
                   exclude=exclude,
                   algo_config=algo_config,
                   pre_quant_opt_config=[],
                   quant_mode=quant_mode,
                   log_severity_level=log_severity_level)

<div class="viewcode-block" id="Config.set_algo_config">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Config.set_algo_config">[docs]</a>
    def set_algo_config(self, algo_config: Optional[AlgoConfig]) -&gt; None:
        &quot;&quot;&quot;
        Sets the algorithm configuration for quantization.

        :param Optional[AlgoConfig] algo_config: The quantization algorithm configuration to be set.
        &quot;&quot;&quot;
        self.algo_config = algo_config</div>


<div class="viewcode-block" id="Config.add_pre_optimization_config">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Config.add_pre_optimization_config">[docs]</a>
    def add_pre_optimization_config(self, pre_quant_opt_config: PreQuantOptConfig) -&gt; None:
        &quot;&quot;&quot;
        Adds a pre-processing optimization configuration to the list of existing pre-quant optimization configs.

        :param PreQuantOptConfig pre_quant_opt_config: The pre-quantization optimization configuration to add.
        &quot;&quot;&quot;
        assert isinstance(pre_quant_opt_config, PreQuantOptConfig)
        self.pre_quant_opt_config.append(pre_quant_opt_config)</div>
</div>



@dataclass(eq=True)
<div class="viewcode-block" id="QuantizationConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuantizationConfig">[docs]</a>
class QuantizationConfig:
    &quot;&quot;&quot;
    A data class that specifies quantization configurations for different components of a module, allowing hierarchical control over how each tensor type is quantized.

    :param Optional[QuantizationSpec] input_tensors: Input tensors quantization specification. If None, following the hierarchical quantization setup. e.g. If the input_tensors in layer_type_quant_config is None, the configuration from global_quant_config will be used instead. Defaults to None. If None in global_quant_config, input_tensors are not quantized.
    :param Optional[QuantizationSpec] output_tensors: Output tensors quantization specification. Defaults to None. If None, the same as above.
    :param Optional[QuantizationSpec] weight: The weights tensors quantization specification. Defaults to None. If None, the same as above.
    :param Optional[QuantizationSpec] bias: The bias tensors quantization specification. Defaults to None. If None, the same as above.
    :param Optional[DeviceType] target_device: Configuration specifying the target device (e.g., CPU, GPU, IPU) for the quantized model.

    &quot;&quot;&quot;

    input_tensors: Optional[QuantizationSpec] = None

    output_tensors: Optional[QuantizationSpec] = None

    weight: Optional[QuantizationSpec] = None

    bias: Optional[QuantizationSpec] = None

    target_device: Optional[DeviceType] = None

    def to_dict(self) -&gt; Dict[str, Any]:
        return {
            &quot;input_tensors&quot;: self.input_tensors.to_dict() if self.input_tensors is not None else None,
            &quot;output_tensors&quot;: self.output_tensors.to_dict() if self.output_tensors is not None else None,
            &quot;weight&quot;: self.weight.to_dict() if self.weight is not None else None,
            &quot;bias&quot;: self.bias.to_dict() if self.bias is not None else None,
            &quot;target_device&quot;: self.target_device.value if self.target_device is not None else None
        }

    @classmethod
    def from_dict(cls, quantization_config: Dict[str, Any]) -&gt; QuantizationConfig:
        input_tensors = QuantizationSpec.from_dict(quantization_config[&quot;input_tensors&quot;])
        output_tensors = QuantizationSpec.from_dict(quantization_config[&quot;output_tensors&quot;])
        weight = QuantizationSpec.from_dict(quantization_config[&quot;weight&quot;])
        bias = QuantizationSpec.from_dict(quantization_config[&quot;bias&quot;])

        # TODO: Deprecate legacy configuration.
        # Legacy (quark&lt;1.0) saved quantization_config does not have the key `&quot;target_device&quot;`.
        target_device = quantization_config[&quot;target_device&quot;] if &quot;target_device&quot; in quantization_config else None
        target_device = DeviceType(target_device) if target_device is not None else None

        return cls(input_tensors=input_tensors,
                   output_tensors=output_tensors,
                   weight=weight,
                   bias=bias,
                   target_device=target_device)</div>



<div class="viewcode-block" id="DataTypeSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.DataTypeSpec">[docs]</a>
class DataTypeSpec(ConfigBase):

    @abstractmethod
    def to_quantization_spec(self) -&gt; QuantizationSpec:
        pass</div>



@dataclass
<div class="viewcode-block" id="Uint4PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerTensorSpec">[docs]</a>
class Uint4PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint4,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Uint4PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerChannelSpec">[docs]</a>
class Uint4PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    zero_point_type: Optional[str] = &quot;int32&quot;

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint4,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                zero_point_type=get_zero_point_type(self.zero_point_type))</div>



@dataclass
<div class="viewcode-block" id="Uint4PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint4PerGroupSpec">[docs]</a>
class Uint4PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint4,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass
<div class="viewcode-block" id="Int4PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerTensorSpec">[docs]</a>
class Int4PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int4,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Int4PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerChannelSpec">[docs]</a>
class Int4PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int4,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Int4PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int4PerGroupSpec">[docs]</a>
class Int4PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int4,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass
<div class="viewcode-block" id="Uint8PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerTensorSpec">[docs]</a>
class Uint8PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint8,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Uint8PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerChannelSpec">[docs]</a>
class Uint8PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint8,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Uint8PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Uint8PerGroupSpec">[docs]</a>
class Uint8PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.uint8,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass
<div class="viewcode-block" id="Int8PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerTensorSpec">[docs]</a>
class Int8PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int8,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Int8PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerChannelSpec">[docs]</a>
class Int8PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int8,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="Int8PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Int8PerGroupSpec">[docs]</a>
class Int8PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.int8,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass
<div class="viewcode-block" id="FP8E4M3PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerTensorSpec">[docs]</a>
class FP8E4M3PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e4m3,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="FP8E4M3PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerChannelSpec">[docs]</a>
class FP8E4M3PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e4m3,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="FP8E4M3PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E4M3PerGroupSpec">[docs]</a>
class FP8E4M3PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e4m3,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass
<div class="viewcode-block" id="FP8E5M2PerTensorSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerTensorSpec">[docs]</a>
class FP8E5M2PerTensorSpec(DataTypeSpec):
    observer_method: Optional[str] = None
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e5m2,
                                observer_cls=get_per_tensor_observer(self.observer_method),
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_tensor,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="FP8E5M2PerChannelSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerChannelSpec">[docs]</a>
class FP8E5M2PerChannelSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e5m2,
                                observer_cls=PerChannelMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_channel,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic)</div>



@dataclass
<div class="viewcode-block" id="FP8E5M2PerGroupSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.FP8E5M2PerGroupSpec">[docs]</a>
class FP8E5M2PerGroupSpec(DataTypeSpec):
    symmetric: Optional[bool] = None
    scale_type: Optional[str] = None
    round_method: Optional[str] = None
    ch_axis: Optional[int] = None
    is_dynamic: Optional[bool] = None
    group_size: Optional[int] = None

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.fp8_e5m2,
                                observer_cls=PerGroupMinMaxObserver,
                                symmetric=self.symmetric,
                                scale_type=get_scale_type(self.scale_type),
                                round_method=get_round_method(self.round_method),
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                is_dynamic=self.is_dynamic,
                                group_size=self.group_size)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="Float16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Float16Spec">[docs]</a>
class Float16Spec(DataTypeSpec):

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.float16, observer_cls=PlaceholderObserver)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="Bfloat16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.Bfloat16Spec">[docs]</a>
class Bfloat16Spec(DataTypeSpec):

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.bfloat16, observer_cls=PlaceholderObserver)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="MXSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.MXSpec">[docs]</a>
class MXSpec(DataTypeSpec):
    mx_element_dtype: str = &quot;fp8_e4m3&quot;
    ch_axis: int = -1
    block_size: int = 32
    is_dynamic: bool = True

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.mx,
                                observer_cls=PerBlockMXObserver,
                                qscheme=QSchemeType.per_group,
                                mx_element_dtype=Dtype.from_str(self.mx_element_dtype),
                                ch_axis=self.ch_axis,
                                group_size=self.block_size,
                                is_dynamic=self.is_dynamic,
                                round_method=RoundType.half_even)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="MX6Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.MX6Spec">[docs]</a>
class MX6Spec(DataTypeSpec):
    ch_axis: int = -1
    block_size: int = 32
    is_dynamic: bool = True

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.mx6,
                                observer_cls=PerBlockMXObserver,
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                group_size=self.block_size,
                                is_dynamic=self.is_dynamic,
                                round_method=RoundType.half_even)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="MX9Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.MX9Spec">[docs]</a>
class MX9Spec(DataTypeSpec):
    ch_axis: int = -1
    block_size: int = 32
    is_dynamic: bool = True

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.mx9,
                                observer_cls=PerBlockMXObserver,
                                qscheme=QSchemeType.per_group,
                                ch_axis=self.ch_axis,
                                group_size=self.block_size,
                                is_dynamic=self.is_dynamic,
                                round_method=RoundType.half_even)</div>



@dataclass
<div class="viewcode-block" id="BFP16Spec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.BFP16Spec">[docs]</a>
class BFP16Spec(DataTypeSpec):
    is_dynamic: bool = True
    ch_axis: int = -1

    def to_quantization_spec(self) -&gt; QuantizationSpec:
        return QuantizationSpec(dtype=Dtype.bfp16,
                                symmetric=True,
                                observer_cls=PerBlockBFPObserver,
                                qscheme=QSchemeType.per_group,
                                is_dynamic=self.is_dynamic,
                                ch_axis=self.ch_axis,
                                scale_type=ScaleType.float,
                                group_size=8,
                                round_method=RoundType.half_even)</div>



@dataclass(eq=True)
<div class="viewcode-block" id="QuantizationSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuantizationSpec">[docs]</a>
class QuantizationSpec:
    &quot;&quot;&quot;
    A data class that defines the specifications for quantizing tensors within a model.

    :param Dtype dtype: The data type for quantization (e.g., int8, int4).
    :param Optional[bool] is_dynamic: Specifies whether dynamic or static quantization should be used. Default is None, which indicates no specification.
    :param Optional[Type[ObserverBase]] observer_cls: The class of observer to be used for determining quantization parameters like min/max values. Default is None.
    :param Optional[QSchemeType] qscheme: The quantization scheme to use, such as per_tensor, per_channel or per_group. Default is None.
    :param Optional[int] ch_axis: The channel axis for per-channel quantization. Default is None.
    :param Optional[int] group_size: The size of the group for per-group quantization, also the block size for MX datatypes. Default is None.
    :param Optional[bool] symmetric: Indicates if the quantization should be symmetric around zero. If True, quantization is symmetric. If None, it defers to a higher-level or global setting. Default is None.
    :param Optional[RoundType] round_method: The rounding method during quantization, such as half_even. If None, it defers to a higher-level or default method. Default is None.
    :param Optional[ScaleType] scale_type: Defines the scale type to be used for quantization, like power of two or float. If None, it defers to a higher-level setting or uses a default method. Default is None.
    :param Optional[Dtype] mx_element_dtype: Defines the data type to be used for the element type when using mx datatypes, the shared scale effectively uses FP8 E8M0.
    &quot;&quot;&quot;

    ###################################################################################################
    # Quantization Specification for Dtype in [Bfloat16, FP8, Int, MX]

    dtype: Dtype

    observer_cls: Optional[Type[ObserverBase]] = None

    ###################################################################################################
    # Quantization Specification for Dtype in [FP8, Int, MX]

    is_dynamic: Optional[bool] = None

    qscheme: Optional[QSchemeType] = None

    ch_axis: Optional[int] = None

    group_size: Optional[int] = None

    ###################################################################################################
    # Quantization Specification for Dtype in [Int]

    symmetric: Optional[bool] = None

    round_method: Optional[RoundType] = None

    scale_type: Optional[ScaleType] = None

    qat_spec: Optional[QATSpec] = None
    ###################################################################################################
    # Quantization Specification for Dtype in [MX]
    mx_element_dtype: Optional[Dtype] = None
    ###################################################################################################
    # Quantization zero point Specification for Dtype
    zero_point_type: Optional[ZeroPointType] = ZeroPointType.int32

    def __post_init__(self) -&gt; None:
        self._verify_config()

    def _verify_config(self) -&gt; None:
        if self.qscheme == QSchemeType.per_group:
            assert self.ch_axis is not None
            assert self.group_size is not None

    def set_group_size(self, group_size: int) -&gt; None:
        assert isinstance(group_size, int) and (group_size &gt; 0 or group_size == -1), \
            &quot;Group size must be a positive integer or -1 (which means group size equals to dimension size).&quot;
        self.group_size = group_size

    def to_dict(self) -&gt; Dict[str, Any]:
        # TODO: qat_spec, mx_element_dtype missing.
        return {
            &quot;dtype&quot;: self.dtype.name,
            &quot;is_dynamic&quot;: self.is_dynamic,
            &quot;qscheme&quot;: self.qscheme.name if self.qscheme is not None else None,
            &quot;ch_axis&quot;: self.ch_axis,
            &quot;group_size&quot;: self.group_size,
            &quot;symmetric&quot;: self.symmetric,
            &quot;round_method&quot;: self.round_method.name if self.round_method is not None else None,
            &quot;scale_type&quot;: self.scale_type.name if self.scale_type is not None else None,
            &quot;observer_cls&quot;: self.observer_cls.__name__ if self.observer_cls is not None else None
        }

    @classmethod
    def from_dict(cls, config_dict: Optional[Dict[str, Any]]) -&gt; Optional[QuantizationSpec]:
        if config_dict is None:
            return None

        dtype = Dtype[config_dict[&quot;dtype&quot;]]

        if config_dict.get(&quot;mx_element_dtype&quot;, None) is not None:
            mx_element_dtype = Dtype[config_dict[&quot;mx_element_dtype&quot;]]
        else:
            mx_element_dtype = None

        if config_dict.get(&quot;qscheme&quot;, None) is not None:
            qscheme = QSchemeType[config_dict[&quot;qscheme&quot;]]
        else:
            qscheme = None

        if config_dict.get(&quot;round_method&quot;, None) is not None:
            round_method = RoundType[config_dict[&quot;round_method&quot;]]
        else:
            round_method = None

        if config_dict.get(&quot;scale_type&quot;, None) is not None:
            scale_type = ScaleType[config_dict[&quot;scale_type&quot;]]
        else:
            scale_type = None

        # TODO: Deprecate legacy configuration.
        # Accomodate the legacy (quark&lt;1.0) export which used custom keys.
        is_dynamic = config_dict[&quot;is_dynamic&quot;] if &quot;is_dynamic&quot; in config_dict else config_dict[&quot;dynamic&quot;]
        ch_axis = config_dict[&quot;ch_axis&quot;] if &quot;ch_axis&quot; in config_dict else config_dict[&quot;axis&quot;]

        group_size = config_dict[&quot;group_size&quot;]
        symmetric = config_dict[&quot;symmetric&quot;]

        if &quot;observer_cls&quot; in config_dict:
            if config_dict[&quot;observer_cls&quot;] in OBSERVER_MAP:
                observer_cls = OBSERVER_MAP[config_dict[&quot;observer_cls&quot;]]
            else:  # pragma: no cover
                logger.warning(
                    f&quot;Unknown observer_cls={config_dict[&#39;observer_cls&#39;]}. Loading the QuantizationSpec with observer_cls=PlaceholderObserver.&quot;
                )
                observer_cls = PlaceholderObserver
        else:  # pragma: no cover
            # quark&lt;1.0 used not to save the `observer_cls` in `QuantizationSpec.to_dict()`.
            observer_cls = PlaceholderObserver

        return cls(dtype=dtype,
                   is_dynamic=is_dynamic,
                   qscheme=qscheme,
                   ch_axis=ch_axis,
                   group_size=group_size,
                   symmetric=symmetric,
                   round_method=round_method,
                   scale_type=scale_type,
                   mx_element_dtype=mx_element_dtype,
                   observer_cls=observer_cls)  # type: ignore[arg-type]</div>



@dataclass
<div class="viewcode-block" id="TQTSpec">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.TQTSpec">[docs]</a>
class TQTSpec(ConfigBase):
    threshold_init_meth: Optional[TQTThresholdInitMeth] = None</div>



QATSpec = Union[TQTSpec]


<div class="viewcode-block" id="load_pre_optimization_config_from_file">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.load_pre_optimization_config_from_file">[docs]</a>
def load_pre_optimization_config_from_file(file_path: str) -&gt; PreQuantOptConfig:
    &quot;&quot;&quot;
    Load pre-optimization configuration from a JSON file.

    :param file_path: The path to the JSON file containing the pre-optimization configuration.
    :type file_path: str
    :return: The pre-optimization configuration.
    :rtype: PreQuantOptConfig
    &quot;&quot;&quot;
    with open(file_path, &#39;r&#39;) as file:
        algo_config_info = json.load(file)
    return _load_pre_optimization_config_from_dict(algo_config_info)</div>



<div class="viewcode-block" id="load_quant_algo_config_from_file">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.load_quant_algo_config_from_file">[docs]</a>
def load_quant_algo_config_from_file(file_path: str) -&gt; AlgoConfig:
    &quot;&quot;&quot;
    Load quantization algorithm configuration from a JSON file.

    :param file_path: The path to the JSON file containing the quantization algorithm configuration.
    :type file_path: str
    :return: The quantization algorithm configuration.
    :rtype: AlgoConfig
    &quot;&quot;&quot;
    with open(file_path, &#39;r&#39;) as file:
        algo_config_info = json.load(file)
    return _load_quant_algo_config_from_dict(algo_config_info)</div>



def _load_pre_optimization_config_from_dict(pre_optimization_config_dict: Dict[str, Any]) -&gt; PreQuantOptConfig:
    &quot;&quot;&quot;
    Load pre-optimization configuration from a dictionary.

    :param pre_optimization_config_dict: A dictionary containing the pre-optimization configuration.
    :type pre_optimization_config_dict: Dict[str, Any]
    :return: The pre-optimization configuration.
    :rtype: PreQuantOptConfig
    :raises ValueError: If the configuration name is not recognized.
    &quot;&quot;&quot;
    if pre_optimization_config_dict[&quot;name&quot;] == &quot;rotation&quot;:
        return cast(PreQuantOptConfig, RotationConfig.from_dict(pre_optimization_config_dict))
    elif pre_optimization_config_dict[&quot;name&quot;] == &quot;quarot&quot;:
        return cast(PreQuantOptConfig, QuaRotConfig.from_dict(pre_optimization_config_dict))
    elif pre_optimization_config_dict[&quot;name&quot;] == &quot;smooth&quot;:
        return cast(PreQuantOptConfig, SmoothQuantConfig.from_dict(pre_optimization_config_dict))
    else:
        raise ValueError(f&quot;Unknown algorithm name {pre_optimization_config_dict[&#39;name&#39;]}&quot;)


def _load_quant_algo_config_from_dict(algo_config_dict: Dict[str, Any]) -&gt; AlgoConfig:
    &quot;&quot;&quot;
    Load quantization algorithm configuration from a dictionary.

    :param algo_config_dict: A dictionary containing the quantization algorithm configuration.
    :type algo_config_dict: Dict[str, Any]
    :return: The quantization algorithm configuration.
    :rtype: AlgoConfig
    :raises ValueError: If the configuration name is not recognized.
    &quot;&quot;&quot;
    if algo_config_dict[&quot;name&quot;] == &quot;awq&quot;:
        return cast(AlgoConfig, AWQConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;gptq&quot;:  # pragma: no cover
        return cast(AlgoConfig, GPTQConfig.from_dict(algo_config_dict))
    elif algo_config_dict[&quot;name&quot;] == &quot;autosmoothquant&quot;:  # pragma: no cover:
        return cast(AlgoConfig, AutoSmoothQuantConfig.from_dict(algo_config_dict))
    else:
        raise ValueError(f&quot;Unknown algorithm name {algo_config_dict[&#39;name&#39;]}&quot;)


@dataclass
<div class="viewcode-block" id="AlgoConfigBase">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AlgoConfigBase">[docs]</a>
class AlgoConfigBase(ConfigBase):
    pass</div>



@dataclass
<div class="viewcode-block" id="PreQuantOptConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.PreQuantOptConfig">[docs]</a>
class PreQuantOptConfig(AlgoConfigBase):
    pass</div>



@dataclass
<div class="viewcode-block" id="AlgoConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AlgoConfig">[docs]</a>
class AlgoConfig(AlgoConfigBase):
    pass</div>



@dataclass
<div class="viewcode-block" id="SmoothQuantConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.SmoothQuantConfig">[docs]</a>
class SmoothQuantConfig(PreQuantOptConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for Smooth Quantization.

    :param str name: The name of the configuration, typically used to identify different quantization settings. Default is &quot;smoothquant&quot;.
    :param int alpha: The factor of adjustment in the quantization formula, influencing how aggressively weights are quantized. Default is 1.
    :param float scale_clamp_min: The minimum scaling factor to be used during quantization, preventing the scale from becoming too small. Default is 1e-3.
    :param List[Dict[str, str]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is None.
    :param str model_decoder_layers: Specifies any particular decoder layers in the model that might have unique quantization requirements. Default is None.
    &quot;&quot;&quot;
    name: str = &quot;smooth&quot;
    alpha: float = 1
    scale_clamp_min: float = 1e-3
    scaling_layers: List[Dict[str, str]] = field(default_factory=list)
    model_decoder_layers: str = field(default_factory=str)
    # For GQA models, num_attention_heads and num_key_value_heads must be set in config file,
    # otherwise o_project in GQA will not be smoothed (other linears are still smoothed).
    num_attention_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.
    num_key_value_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.</div>



@dataclass
<div class="viewcode-block" id="RotationConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.RotationConfig">[docs]</a>
class RotationConfig(PreQuantOptConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for rotation settings in processing algorithms.

    :param str name: The name of the configuration, typically used to identify different rotation settings. Default is &quot;rotation&quot;.
    :param bool random: A boolean flag indicating whether the rotation should be applied randomly. This can be useful for data augmentation purposes where random rotations may be required. Default is False.
    :param List[Dict[str, str]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is None.
    &quot;&quot;&quot;
    name: str = &quot;rotation&quot;
    random: bool = False
    scaling_layers: List[Dict[str, str]] = field(default_factory=list)</div>



@dataclass
<div class="viewcode-block" id="QuaRotConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.QuaRotConfig">[docs]</a>
class QuaRotConfig(RotationConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for the QuaRot algorithm.
    :param str name: The name of the configuration, typically used to identify different rotation settings. Default is &quot;quarot&quot;.
    :param bool random: A boolean flag indicating whether R1 should be applied randomly. This can be useful for data augmentation purposes where random rotations may be required. Default is False.
    :param bool random2: A boolean flag indicating whether R2 should be applied randomly. This can be useful for data augmentation purposes where random rotations may be required. Default is False.
        random and random2 are only relevant if we are using Hadamard rotations for R1 and R2. If optimized_rotation_path specified,
        then we will load R1 and R2 matrices from a file instad of using Hadamard matrices.
    :param List[Dict[str, str]] scaling_layers: Specific settings for scaling layers, allowing customization of quantization parameters for different layers within the model. Default is None.
    :param bool had: A boolean flag indicating whether online hadamard operations R3 and R4 should be performed.
    :param Optional[str] optimized_rotation_path: The path to the file &#39;R.bin&#39; that has saved optimized R1 and (per decoder) R2 matrices.
        If this is specified, R1 and R2 rotations will be loaded from this file. Otherwise they will be Hadamard matrices.
    :param bool kv_cache_quant: A boolean flag indicating whether there is kv-cache quantization. R3 rotation is applied only if there is.
    :param bool act_quant: A boolean flag indicating whether there is kv-cache quantization. R3 rotation is applied only if there is.
    :param str backbone: A string indicating the path to the model backbone.
    :param str model_decoder_layers: A string indicating the path to the list of decoder layers.
    :param str v_proj: A string indicating the path to the v projection layer, starting from the decoder layer it is in.
    :param str o_proj: A string indicating the path to the o projection layer, starting from the decoder layer it is in.
    :param str self_attn: A string indicating the path to the self attention block, starting from the decoder layer it is in.
    :param str mlp: A string indicating the path to the multilayer perceptron layer, starting from the decoder layer it is in.
    &quot;&quot;&quot;
    name: str = &quot;quarot&quot;
    random: bool = False
    random2: bool = False
    scaling_layers: List[Dict[str, str]] = field(default_factory=list)
    online_had: bool = True
    optimized_rotation_path: Optional[str] = None
    kv_cache_quant: bool = True
    act_quant: bool = True
    backbone: str = &quot;model&quot;
    model_decoder_layers: str = &quot;model.layers&quot;
    v_proj: str = &quot;self_attn.v_proj&quot;
    o_proj: str = &quot;self_attn.o_proj&quot;
    self_attn: str = &quot;self_attn&quot;
    mlp: str = &quot;mlp&quot;</div>



@dataclass
<div class="viewcode-block" id="AutoSmoothQuantConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AutoSmoothQuantConfig">[docs]</a>
class AutoSmoothQuantConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for AutoSmoothQuant.

    :param str name: The name of the quantization configuration. Default is &quot;autosmoothquant&quot;.
    :param List[Dict[str, str]] scaling_layers: Configuration details for scaling layers within the model, specifying custom scaling parameters per layer. Default is None.
    :param str compute_scale_loss: Calculate the best scale loss, &quot;MSE&quot; or &quot;MAE&quot;. Default is &quot;MSE&quot;.
    :param str model_decoder_layers: Specifies the layers involved in model decoding that may require different quantization parameters. Default is None.
    &quot;&quot;&quot;
    name: str = &quot;autosmoothquant&quot;
    scaling_layers: Optional[List[Dict[str, str]]] = None
    model_decoder_layers: Optional[str] = None
    compute_scale_loss: Optional[str] = &quot;MSE&quot;
    # For GQA models, num_attention_heads and num_key_value_heads must be set in config file,
    # otherwise o_project in GQA will not be smoothed (other linears are still smoothed).
    num_attention_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.
    num_key_value_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.</div>



@dataclass
<div class="viewcode-block" id="AWQConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.AWQConfig">[docs]</a>
class AWQConfig(AlgoConfig):
    &quot;&quot;&quot;
    Configuration for Activation-aware Weight Quantization (AWQ).

    :param str name: The name of the quantization configuration. Default is &quot;awq&quot;.
    :param List[Dict[str, str]] scaling_layers: Configuration details for scaling layers within the model, specifying custom scaling parameters per layer. Default is None.
    :param str model_decoder_layers: Specifies the layers involved in model decoding that may require different quantization parameters. Default is None.
    &quot;&quot;&quot;
    name: str = &quot;awq&quot;
    scaling_layers: List[Dict[str, str]] = field(default_factory=list)
    model_decoder_layers: str = field(default_factory=str)
    # For GQA models, num_attention_heads and num_key_value_heads must be set in config file,
    # otherwise o_project in GQA will not be smoothed (other linears are still smoothed).
    num_attention_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.
    num_key_value_heads: int = -1  # o_project in GQA will not be smoothed if num_attention_heads is not set in config file.</div>



@dataclass
<div class="viewcode-block" id="GPTQConfig">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html#quark.torch.quantization.config.config.GPTQConfig">[docs]</a>
class GPTQConfig(AlgoConfig):
    &quot;&quot;&quot;
    A data class that defines the specifications for Accurate Post-Training Quantization for Generative Pre-trained Transformers (GPTQ).

    :param str name: The configuration name. Default is &quot;gptq&quot;.
    :param float damp_percent: The percentage used to dampen the quantization effect, aiding in the maintenance of accuracy post-quantization. Default is 0.01.
    :param bool desc_act: Indicates whether descending activation is used, typically to enhance model performance with quantization. Default is True.
    :param bool static_groups: Specifies whether the order of groups for quantization are static or can be dynamically adjusted. Default is True. Quark export only support static_groups as True.
    :param bool true_sequential: Indicates whether the quantization should be applied in a truly sequential manner across the layers. Default is True.
    :param List[str] inside_layer_modules: Lists the names of internal layer modules within the model that require specific quantization handling. Default is None.
    :param str model_decoder_layers: Specifies custom settings for quantization on specific decoder layers of the model. Default is None.
    &quot;&quot;&quot;
    name: str = &quot;gptq&quot;
    damp_percent: float = 0.01
    desc_act: bool = True
    static_groups: bool = True
    true_sequential: bool = True
    inside_layer_modules: List[str] = field(default_factory=list)
    model_decoder_layers: str = field(default_factory=str)</div>

</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>