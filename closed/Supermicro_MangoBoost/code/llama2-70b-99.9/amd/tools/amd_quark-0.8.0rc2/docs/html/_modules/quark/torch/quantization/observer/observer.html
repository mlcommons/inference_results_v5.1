
<!DOCTYPE html>


<html lang="en" data-content_root="../../../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>quark.torch.quantization.observer.observer &#8212; Quark 0.8.0rc2 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../../../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=6b4ca4e1" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_header.css?v=4044f309" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/rocm_footer.css?v=25204c5a" />
    <link rel="stylesheet" type="text/css" href="../../../../../_static/fonts.css?v=fcff5274" />
  
  <!-- So that users can add custom icons -->
  <script src="../../../../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../../../../_static/documentation_options.js?v=d42b94c0"></script>
    <script src="../../../../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="../../../../../_static/code_word_breaks.js?v=327952c4"></script>
    <script async="async" src="../../../../../_static/renameVersionLinks.js?v=929fe5e4"></script>
    <script async="async" src="../../../../../_static/rdcMisc.js?v=01f88d96"></script>
    <script async="async" src="../../../../../_static/theme_mode_captions.js?v=15f4ec5d"></script>
    <script src="../../../../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/quark/torch/quantization/observer/observer';</script>
    <script async="async" src="https://download.amd.com/js/analytics/analyticsinit.js"></script>
    <link rel="icon" href="https://www.amd.com/themes/custom/amd/favicon.ico"/>
    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" />
<script type="text/javascript">
    window.addEventListener("load", function(event) {
        var coll = document.querySelectorAll('.toggle > .header');  // sdelect the toggles header.
        var i;

        for (i = 0; i < coll.length; i++) {                        
            coll[i].innerText = "Show code ▼\n\n";
            
            coll[i].addEventListener("click", function() {
                var content = this.nextElementSibling;  // code block.
                if (content.style.display === "block") {
                    content.style.display = "none";
                    this.innerText = "Show code ▼\n\n";
                } else {
                    content.style.display = "block";
                    this.innerText = "Hide code ▶";
                }
            });
        }
    });
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  

<header class="common-header" >
    <nav class="navbar navbar-expand-xl">
        <div class="container-fluid main-nav rocm-header">
            
            <button class="navbar-toggler collapsed" id="nav-icon" data-tracking-information="mainMenuToggle" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
                <span></span>
                <span></span>
                <span></span>
            </button>
            
            <div class="header-logo">
                <a class="navbar-brand" href="https://www.amd.com/">
                    <img src="../../../../../_static/images/amd-header-logo.svg" alt="AMD Logo" title="AMD Logo" width="90" class="d-inline-block align-text-top hover-opacity"/>
                </a>
                <div class="vr vr mx-40 my-25"></div>
                <a class="klavika-font hover-opacity" href="https://quark.docs.amd.com">Quark</a>
                <a class="header-all-versions" href="https://quark.docs.amd.com/latest/versions.html">Version List</a>
            </div>
            <div class="icon-nav text-center d-flex ms-auto">
            </div>
        </div>
    </nav>
    
    <nav class="navbar navbar-expand-xl second-level-nav">
        <div class="container-fluid main-nav">
            <div class="navbar-nav-container collapse navbar-collapse" id="navbarSupportedContent">
                <ul class="navbar-nav nav-mega me-auto mb-2 mb-lg-0 col-xl-10">
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark" id="navgithub" role="button" aria-expanded="false" target="_blank" >
                                GitHub
                            </a>
                        </li>
                    
                        <li class="nav-item">
                            <a class="nav-link top-level header-menu-links" href="https://gitenterprise.xilinx.com/AMDNeuralOpt/Quark/issues/new/choose" id="navsupport" role="button" aria-expanded="false" target="_blank" >
                                Support
                            </a>
                        </li>
                    
                </ul>
            </div>
        </div>
    </nav>
    
</header>


  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../../../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Quark 0.8.0rc2 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../release_note.html">Release Information</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started with AMD Quark</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../intro.html">Introduction to Quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../install.html">Installation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../basic_usage.html">Basic Usage</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/basic_usage_pytorch.html">AMD Quark for PyTorch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/basic_usage_onnx.html">AMD Quark for ONNX</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_examples.html">Accessing PyTorch Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_diffusers.html">Diffusion Model Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">AMD Quark Extension for Brevitas Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Language Model Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model PTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_vision.html">Vision Model Quantization using FX Graph Mode</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_examples.html">Accessing ONNX Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_BFP.html">Block Floating Point (BFP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_MX.html">MX Formats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaround.html">Fast Finetune AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_adaquant.html">Fast Finetune AdaQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_cle.html">Cross-Layer Equalization (CLE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">GPTQ</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_mixed_precision.html">Mixed Precision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_smoothquant.html">Smooth Quant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_quarot.html">QuaRot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_llama2.html">Quantizing an Llama-2-7b Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_dynamic_quantization_opt.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_image_classification.html">Quantizing a ResNet50-v1-12 Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_language_models.html">Quantizing an OPT-125M Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int4_matmul_nbits_llama2.html">Quantizing an Llama-2-7b Model Using the ONNX MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_weights_only_quant_int8_qdq_llama2.html">Quantizating Llama-2-7b model using MatMulNBits</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/image_classification_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Image Classification Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/object_detection_example_quark_onnx_ryzen_ai_best_practice.html">Best Practice for Quantizing an Object Detection Model</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced AMD Quark Features for PyTorch</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/user_guide_config_description.html">Configuring PyTorch Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_methods.html">Calibration Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/calibration_datasets.html">Calibration Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_save_load.html">Save and Load Quantized Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export.html">Exporting Quantized Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_onnx.html">ONNX Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_hf.html">HuggingFace Format</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/export/quark_export_gguf.html">GGUF Format</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/export/gguf_llamacpp.html">Bridge from Quark to llama.cpp</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_quark.html">Quark Format</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/export/quark_export_oga.html">ONNX Runtime Gen AI Model Builder</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/quark_torch_best_practices.html">Best Practices for Post-Training Quantization (PTQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/debug.html">Debugging quantization Degradation</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/llm_quark.html">Language Model Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_pruning.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_ptq.html">Language Model Post Training Quantization (PTQ) Using Quark</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_qat.html">Language Model QAT Using Quark</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval.html">Language Model Evaluations in Quark</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_perplexity.html">Perplexity Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_rouge_meteor.html">Rouge &amp; Meteor Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness.html">LM-Evaluation Harness Evaluations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_llm_eval_harness_offline.html">LM-Evaluation Harness (Offline)</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_rotation.html">Quantizing with Rotation and SmoothQuant</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/tutorial_quarot.html">Rotation-based quantization with QuaRot</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/smoothquant.html">Activation/Weight Smoothing (SmoothQuant)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/tutorial_bfp16.html">Block Floating Point 16</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/extensions.html">Extensions</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_pytorch_light.html">Integration with AMD Pytorch-light (APL)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../pytorch/example_quark_torch_brevitas.html">Brevitas Integration</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_mx.html">Using MX (Microscaling)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/adv_two_level.html">Two Level Quantization Formats</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Advanced Quark Features for ONNX</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/user_guide_config_description.html">Configuring ONNX Quantization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/appendix_full_quant_config_features.html">Full List of Quantization Config Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_methods.html">Calibration methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/calibration_datasets.html">Calibration datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_strategies.html">Quantization Strategies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_schemes.html">Quantization Schemes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/config/quantization_symmetry.html">Quantization Symmetry</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/user_guide_supported_optype_datatype.html">Data and OP Types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/gpu_usage_guide.html">Accelerate with GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_mix_precision.html">Mixed Precision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/bfp16.html">Block Floating Point 16 (BFP16)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tutorial_bf16_quantization.html">BF16 Quantization</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/accuracy_improvement_algorithms.html">Accuracy Improvement Algorithms</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/cle.html">Quantizing Using CrossLayerEqualization (CLE)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/ada.html">Quantization Using AdaQuant and AdaRound</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/accuracy_algorithms/sq.html">SmoothQuant (SQ)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../onnx/example_quark_onnx_gptq.html">Quantizating a model with GPTQ</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/optional_utilities.html">Optional Utilities</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/tools.html">Tools</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">APIs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../pytorch/pytorch_apis.html">PyTorch APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/api/index.html">Pruning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/api/index.html">Export</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/pruning/config/index.html">Pruner Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/quantization/config/config/index.html">Quantizer Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/torch/export/config/config/index.html">Exporter Configuration</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../../../onnx/onnx_apis.html">ONNX APIs</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/api/index.html">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/optimize/index.html">Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/calibrate/index.html">Calibration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/onnx_quantizer/index.html">ONNX Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/qdq_quantizer/index.html">QDQ Quantizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quantization/config/config/index.html">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../autoapi/quark/onnx/quant_utils/index.html">Quantization Utilities</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Troubleshooting and Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../../../../pytorch/pytorch_faq.html">PyTorch FAQ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../onnx/onnx_faq.html">ONNX FAQ</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-angle-right"></span>
  </label></div>
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../../../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../../../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">quark.torch.quantization.observer.observer</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for quark.torch.quantization.observer.observer</h1><div class="highlight"><pre>
<span></span>#
# Copyright (C) 2023, Advanced Micro Devices, Inc. All rights reserved.
# SPDX-License-Identifier: MIT
#

from __future__ import annotations
from typing import Tuple, TYPE_CHECKING, Any, cast, Optional
from abc import ABC, abstractmethod
import torch.nn as nn
import torch
from torch.ao.quantization import HistogramObserver
if TYPE_CHECKING:
    from quark.torch.quantization.config.config import QuantizationSpec
from quark.torch.quantization.config.type import Dtype, QSchemeType, ZeroPointType, ScaleType
from quark.torch.quantization.nn.utils import check_min_max_valid
from quark.torch.quantization.utils import calculate_qmin_qmax, reshape_to_blocks, get_dtype_params, t_exponent
from quark.shares.utils.log import ScreenLogger, log_errors
from quark.shares.utils.import_utils import is_torch_greater_or_equal_2_5
from quark.torch.kernel.hw_emulation.hw_emulation_interface import fake_quantize_int  # type: ignore

logger = ScreenLogger(__name__)


<div class="viewcode-block" id="ObserverBase">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.ObserverBase">[docs]</a>
class ObserverBase(ABC, nn.Module):

    def __init__(self, qspec: QuantizationSpec, device: Optional[torch.device] = None) -&gt; None:
        super().__init__()
        self.dtype = qspec.dtype

        self.scale_torch_dtype = None
        if qspec.scale_type in [ScaleType.float32, ScaleType.float16, ScaleType.bfloat16]:
            self.scale_torch_dtype = qspec.scale_type.to_torch_dtype()

    @abstractmethod
    def forward(self, x: torch.Tensor) -&gt; Any:
        pass

    @abstractmethod
    def _calculate_qparams(self) -&gt; Any:
        pass</div>



<div class="viewcode-block" id="PlaceholderObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PlaceholderObserver">[docs]</a>
class PlaceholderObserver(ObserverBase):
    r&quot;&quot;&quot;
    Observer only passes its configuration to the quantized module&#39;s ``.from_float()``.

    Does not have any calculation.

    Only can be used for quantization to float16 and bfloat16 which doesn&#39;t require determining
    ranges.
    &quot;&quot;&quot;

    def __init__(
        self,
        qspec: QuantizationSpec,
        device: Optional[torch.device] = None,
    ) -&gt; None:
        super().__init__(qspec, device)

    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:
        return x

    def _calculate_qparams(self) -&gt; None:
        pass

<div class="viewcode-block" id="PlaceholderObserver.extra_repr">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PlaceholderObserver.extra_repr">[docs]</a>
    def extra_repr(self) -&gt; str:
        return f&quot;dtype={self.dtype}&quot;</div>
</div>



<div class="viewcode-block" id="UniformScalingObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.UniformScalingObserver">[docs]</a>
class UniformScalingObserver(ObserverBase):
    &quot;&quot;&quot;
    Observer for uniform scaling quantizer. For example &#39;int uniform quantizer&#39; or &#39;fp8 uniform scaling&#39;.

    &quot;&quot;&quot;

    eps: torch.Tensor
    min_val: torch.Tensor
    max_val: torch.Tensor

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 eps: float = torch.finfo(torch.float32).eps) -&gt; None:
        super().__init__(qspec, device)

        self.qspec = qspec
        self.symmetric = qspec.symmetric
        self.scale_type = qspec.scale_type
        self.qscheme = qspec.qscheme
        self.is_dynamic = qspec.is_dynamic
        self.zero_point_type = qspec.zero_point_type

        self.register_buffer(&quot;min_val&quot;, torch.tensor(float(&quot;inf&quot;), device=device))
        self.register_buffer(&quot;max_val&quot;, torch.tensor(float(&quot;-inf&quot;), device=device))
        self.register_buffer(&quot;eps&quot;, torch.tensor(eps, device=device))

        self.quant_min, self.quant_max = calculate_qmin_qmax(qspec.dtype)

    def _calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        r&quot;&quot;&quot;Calculates the quantization parameters.&quot;&quot;&quot;
        return self.calculate_qparams(self.min_val, self.max_val)

<div class="viewcode-block" id="UniformScalingObserver.calculate_qparams">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.UniformScalingObserver.calculate_qparams">[docs]</a>
    def calculate_qparams(self, min_val: torch.Tensor, max_val: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        r&quot;&quot;&quot;Calculates the quantization parameters.&quot;&quot;&quot;
        if self.dtype in [Dtype.fp8_e4m3, Dtype.fp8_e5m2]:
            return self.calculate_fp8_quant_parameters(min_val, max_val)
        else:
            return self.calculate_int_quant_params(min_val, max_val)</div>


    def calculate_int_quant_params(self, min_val: torch.Tensor,
                                   max_val: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:

        # TODO setup eps device when init
        self.eps = self.eps.to(min_val.dtype).to(min_val.device)

        if not check_min_max_valid(min_val, max_val):
            return torch.tensor([1.0], device=min_val.device.type), torch.tensor([0], device=min_val.device.type)

        quant_min, quant_max = self.quant_min, self.quant_max
        assert isinstance(quant_min, int)
        assert isinstance(quant_max, int)
        min_val_neg = torch.min(min_val, torch.zeros_like(min_val))
        max_val_pos = torch.max(max_val, torch.zeros_like(max_val))

        device = min_val_neg.device
        scale = torch.ones(min_val_neg.size(), dtype=torch.float32, device=device)

        # TODO: This makes the assumption that signed integer dtype is used when doing symmetric quantization. There
        # is no enforcement on that in quark.
        zero_point = torch.zeros(min_val_neg.size(), dtype=torch.int32, device=device)

        if self.symmetric:
            max_val_pos = torch.max(-min_val_neg, max_val_pos)
            scale = max_val_pos / (float(quant_max - quant_min) / 2)
            scale = torch.max(scale, self.eps)
        else:
            if self.zero_point_type == ZeroPointType.float32:
                scale = (max_val - min_val) / float(quant_max - quant_min)
                scale = torch.where(scale &gt; self.eps, scale, torch.ones_like(scale))
                zero_point = -1 * min_val / scale
            else:
                # AWQ
                scale = (max_val_pos - min_val_neg) / float(quant_max - quant_min)
                # TODO: reset eps&#39;s device
                self.eps = self.eps.to(scale.device)
                scale = torch.max(scale, self.eps)
                zero_point = quant_min - torch.round(min_val_neg / scale).to(torch.int)
                zero_point = torch.clamp(zero_point, quant_min, quant_max)

        return scale.to(self.scale_torch_dtype), zero_point

    def calculate_fp8_quant_parameters(self, min_val: torch.Tensor,
                                       max_val: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        min_val_neg = torch.min(min_val, torch.zeros_like(min_val))
        max_val_pos = torch.max(max_val, torch.zeros_like(max_val))

        device = min_val_neg.device
        scale = torch.ones(min_val_neg.size(), dtype=torch.float32, device=device)
        zero_point = torch.zeros(min_val_neg.size(), dtype=torch.int32, device=device)

        amax = torch.maximum(torch.abs(min_val_neg), torch.abs(max_val_pos))
        _, max_norm = calculate_qmin_qmax(self.dtype)
        scale = amax / max_norm
        return scale.to(self.scale_torch_dtype), zero_point

<div class="viewcode-block" id="UniformScalingObserver.extra_repr">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.UniformScalingObserver.extra_repr">[docs]</a>
    def extra_repr(self) -&gt; str:
        return f&quot;min_val={self.min_val}, max_val={self.max_val}&quot;</div>


<div class="viewcode-block" id="UniformScalingObserver.reset_min_max_vals">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.UniformScalingObserver.reset_min_max_vals">[docs]</a>
    def reset_min_max_vals(self) -&gt; None:
        &quot;&quot;&quot;Resets the min/max values.&quot;&quot;&quot;
        self.min_val = torch.tensor(float(&quot;inf&quot;))
        self.max_val = torch.tensor(float(&quot;-inf&quot;))</div>


    def reset_min_max_for_dynamic(self) -&gt; None:
        if self.is_dynamic:
            self.reset_min_max_vals()</div>



<div class="viewcode-block" id="PerTensorMinMaxObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorMinMaxObserver">[docs]</a>
class PerTensorMinMaxObserver(UniformScalingObserver):

    def __init__(self, qspec: QuantizationSpec, device: Optional[torch.device] = None) -&gt; None:
        super().__init__(qspec, device)

<div class="viewcode-block" id="PerTensorMinMaxObserver.forward">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorMinMaxObserver.forward">[docs]</a>
    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        r&quot;&quot;&quot;Records the running minimum and maximum of ``x``.&quot;&quot;&quot;
        if x_orig.numel() == 0:
            return x_orig

        x = x_orig.detach()  # avoid keeping autograd tape
        self.reset_min_max_for_dynamic()
        min_val_cur, max_val_cur = torch.aminmax(x)
        min_val = torch.min(min_val_cur, self.min_val)
        max_val = torch.max(max_val_cur, self.max_val)
        self.min_val.copy_(min_val)
        self.max_val.copy_(max_val)

        self.min_val = self.min_val.to(x_orig.dtype)
        self.max_val = self.max_val.to(x_orig.dtype)
        return x_orig</div>
</div>



<div class="viewcode-block" id="PerChannelMinMaxObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerChannelMinMaxObserver">[docs]</a>
class PerChannelMinMaxObserver(UniformScalingObserver):

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 eps: float = torch.finfo(torch.float32).eps) -&gt; None:
        super().__init__(qspec, device)

        self.qspec = qspec
        self.ch_axis = qspec.ch_axis

    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        return self._forward(x_orig)

    @log_errors
    def _forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        x_orig_device = x_orig.device
        if x_orig.numel() == 0:
            return x_orig
        x = x_orig.detach()  # avoid keeping autograd tape
        self.reset_min_max_for_dynamic()
        min_val = self.min_val.to(x_orig_device)
        max_val = self.max_val.to(x_orig_device)
        x_dim = x.size()

        new_axis_list = [i for i in range(len(x_dim))]  # noqa: C416
        if self.ch_axis is not None:
            new_axis_list[self.ch_axis] = 0
        else:
            raise ValueError(&quot;ch_axis cannot be None&quot;)
        new_axis_list[0] = self.ch_axis
        y = x.permute(new_axis_list)
        # Need to match dtype of min/max because the updates to buffers
        # are done in place and types need to match for comparisons
        y = y.to(self.min_val.dtype)
        y = torch.flatten(y, start_dim=1)
        if min_val.numel() == 0 or max_val.numel() == 0:
            min_val, max_val = torch.aminmax(y, dim=1)
        else:
            min_val_cur, max_val_cur = torch.aminmax(y, dim=1)
            min_val = torch.min(min_val_cur, min_val)
            max_val = torch.max(max_val_cur, max_val)
        self.min_val.resize_(min_val.shape)
        self.max_val.resize_(max_val.shape)
        self.min_val.copy_(min_val)
        self.max_val.copy_(max_val)

        input_origin_type = x_orig.dtype
        self.min_val = self.min_val.to(input_origin_type)
        self.max_val = self.max_val.to(input_origin_type)
        return x_orig</div>



<div class="viewcode-block" id="PerBlockMXObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerBlockMXObserver">[docs]</a>
class PerBlockMXObserver(ObserverBase):

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 eps: float = torch.finfo(torch.float32).eps) -&gt; None:
        super().__init__(qspec=qspec, device=device)
        self.qspec = qspec

        assert self.qspec.dtype in [Dtype.mx, Dtype.mx6, Dtype.mx9]
        assert qspec.group_size is not None
        assert qspec.ch_axis is not None
        if self.qspec.dtype == Dtype.mx:
            assert qspec.mx_element_dtype is not None
            self.quant_bit = None
        else:
            if self.qspec.dtype == Dtype.mx6:
                self.quant_bit = 5
            elif self.qspec.dtype == Dtype.mx9:
                self.quant_bit = 8

        self.block_size = qspec.group_size
        self.axis = qspec.ch_axis
        self.mx_element_dtype = qspec.mx_element_dtype
        self.eps = eps
        self.is_dynamic = qspec.is_dynamic
        self.amax = torch.tensor(0.0, dtype=torch.float)

        self.register_buffer(&quot;scale&quot;, torch.tensor(0.0, dtype=self.scale_torch_dtype, device=device))
        self.register_buffer(&quot;zero_point&quot;, torch.tensor(0.0, dtype=torch.float, device=device))

    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        block_x = reshape_to_blocks(x_orig, self.block_size, self.axis)
        if self.qspec.dtype in [Dtype.mx6, Dtype.mx9]:
            block_x.nan_to_num_(nan=0.0, posinf=0.0, neginf=0.0)
        amax, _ = torch.max(torch.abs(block_x), dim=-1, keepdim=True)
        if self.is_dynamic:
            self.amax = amax
        else:
            self.amax = torch.max(self.amax, amax)

        return x_orig

    def calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        if self.qspec.dtype == Dtype.mx and self.mx_element_dtype is not None:
            _, _, emax = get_dtype_params(self.mx_element_dtype)
            scale = torch.pow(2, torch.floor(torch.log2(self.amax)) - emax)
            scale = scale.masked_fill(scale == 0.0, self.eps)
            self.scale = scale
            self.zero_point = torch.zeros_like(scale)
        else:
            self.scale = t_exponent(self.amax)

        self.scale = self.scale.to(self.scale_torch_dtype)
        return self.scale, self.zero_point

    def _calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        return self.calculate_qparams()</div>



<div class="viewcode-block" id="PerBlockBFPObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerBlockBFPObserver">[docs]</a>
class PerBlockBFPObserver(ObserverBase):

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 eps: float = torch.finfo(torch.float32).eps) -&gt; None:
        super().__init__(qspec=qspec, device=device)
        self.qspec = qspec
        self.block_size = qspec.group_size
        self.axis = qspec.ch_axis
        self.eps = eps
        self.is_dynamic = qspec.is_dynamic
        self.amax = torch.tensor(0.0, dtype=torch.float)

        self.register_buffer(&quot;scale&quot;, torch.tensor(0.0, dtype=torch.float, device=device))
        self.register_buffer(&quot;zero_point&quot;, torch.tensor(0.0, dtype=torch.float, device=device))

    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        block_x = reshape_to_blocks(x_orig.detach(), self.block_size, self.axis)  # type: ignore
        block_x = block_x.nan_to_num(nan=0.0, posinf=0.0, neginf=0.0)
        amax, _ = torch.max(torch.abs(block_x), dim=-1)
        if self.is_dynamic:
            self.amax = amax
        else:
            self.amax = torch.max(self.amax, amax)
        scale, zero_point = self.calculate_qparams()
        quantized_block_x_int = fake_quantize_int(inputs=block_x,
                                                  scale=scale,
                                                  zero_point=zero_point,
                                                  axis=-1,
                                                  group_size=self.block_size,
                                                  quant_min=-129,
                                                  quant_max=128,
                                                  qscheme=QSchemeType.per_group.value) / scale.unsqueeze(-1)
        bool_mask = torch.logical_or(quantized_block_x_int &gt;= 128, quantized_block_x_int &lt; -128)
        scale_adjust = torch.pow(2, torch.any(bool_mask, dim=-1).to(torch.float32))
        self.amax *= scale_adjust
        return x_orig

    def calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        scale = torch.pow(2, torch.floor(torch.log2(self.amax)) - 6)
        scale = scale.masked_fill(scale == 0.0, self.eps)
        self.scale = scale.to(self.scale_torch_dtype)
        self.zero_point = torch.zeros_like(scale).to(torch.int32)
        return self.scale, self.zero_point

    def _calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        return self.calculate_qparams()</div>



<div class="viewcode-block" id="PerGroupMinMaxObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerGroupMinMaxObserver">[docs]</a>
class PerGroupMinMaxObserver(UniformScalingObserver):

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 eps: float = torch.finfo(torch.float32).eps) -&gt; None:
        super().__init__(qspec, device)

        self.qspec = qspec
        self.ch_axis = cast(int, qspec.ch_axis)
        self.group_size = cast(int, qspec.group_size)
        self.group_count = 1  # Set default value

    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        return self._forward(x_orig)

    def _forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        x_orig_device = x_orig.device
        if x_orig.numel() == 0:
            return x_orig
        x = x_orig.detach()  # avoid keeping autograd tape
        self.reset_min_max_for_dynamic()
        min_val = self.min_val.to(x_orig_device)
        max_val = self.max_val.to(x_orig_device)

        x_dim = x.size()

        # Get group_count, group_count * group_size = dim_of_axis
        x_group_channel_element_num = x_dim[self.ch_axis]
        if self.group_size == -1:
            self.group_count = 1
        elif x_group_channel_element_num % self.group_size != 0:
            raise ValueError(
                f&quot;The number of element per dimension ch_axis={self.ch_axis} is {x_group_channel_element_num} which is not divisible by group_size={self.group_size}. The `group_size` used should be updated in the QuantizationSpec.&quot;
            )
        else:
            self.group_count = x_group_channel_element_num // self.group_size

        # Transpose the dim of ch_axis and the end dim of the tensor.
        new_axis_list = [i for i in range(len(x_dim))]  # noqa: C416
        if self.ch_axis is not None:
            new_axis_list[self.ch_axis] = -1
        else:
            raise ValueError(&quot;ch_axis cannot be None&quot;)
        new_axis_list[-1] = self.ch_axis
        if self.group_size is not None:
            x = x.permute(new_axis_list)
            if self.group_size &gt; 0:
                x = x.reshape(-1, self.group_size)
        else:
            raise ValueError(&quot;group_size cannot be None&quot;)

        # Need to match dtype of min/max because the updates to buffers
        # are done in place and types need to match for comparisons
        if min_val.numel() == 0 or max_val.numel() == 0:
            min_val, max_val = torch.aminmax(x, dim=1)
        else:
            min_val_cur, max_val_cur = torch.aminmax(x, dim=1)
            min_val = torch.min(min_val_cur, min_val)
            max_val = torch.max(max_val_cur, max_val)
        self.min_val.resize_(min_val.shape)
        self.max_val.resize_(max_val.shape)
        self.min_val.copy_(min_val)
        self.max_val.copy_(max_val)

        input_origin_type = x_orig.dtype
        self.min_val = self.min_val.to(input_origin_type)
        self.max_val = self.max_val.to(input_origin_type)
        return x_orig

<div class="viewcode-block" id="PerGroupMinMaxObserver.calculate_qparams">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerGroupMinMaxObserver.calculate_qparams">[docs]</a>
    def calculate_qparams(self, min_val: torch.Tensor, max_val: torch.Tensor) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        r&quot;&quot;&quot;Calculates the quantization parameters.&quot;&quot;&quot;
        if self.dtype in [Dtype.fp8_e4m3, Dtype.fp8_e5m2]:
            _scale, _zero_point = super().calculate_fp8_quant_parameters(min_val, max_val)
        else:
            _scale, _zero_point = super().calculate_int_quant_params(min_val, max_val)
        _scale = _scale.reshape(-1, self.group_count)
        _zero_point = _zero_point.reshape(-1, self.group_count)
        return _scale, _zero_point</div>
</div>



<div class="viewcode-block" id="PerTensorHistogramObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorHistogramObserver">[docs]</a>
class PerTensorHistogramObserver(UniformScalingObserver):

    calib_bin_edges: torch.Tensor
    calib_hist: torch.Tensor

    def __init__(self, qspec: QuantizationSpec, device: Optional[torch.device] = None) -&gt; None:
        super().__init__(qspec)

        self.register_buffer(&quot;calib_bin_edges&quot;, torch.tensor([], device=device))
        self.register_buffer(&quot;calib_hist&quot;, torch.tensor([], device=device))

        # TODO: make the value can be set
        self._skip_zeros = False
        self._num_bins = 2048

<div class="viewcode-block" id="PerTensorHistogramObserver.forward">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorHistogramObserver.forward">[docs]</a>
    def forward(self, x_orig: torch.Tensor) -&gt; torch.Tensor:
        &quot;&quot;&quot;
        Records the running histogram of ``x_orig``.

        Raises:
        - ValueError: If the `self.symmetric` argument is False.

        &quot;&quot;&quot;
        self.device = x_orig.device
        x = x_orig.detach()  # avoid keeping autograd tape
        x = x.float()
        self.reset_min_max_for_dynamic()

        with torch.no_grad():
            if self._skip_zeros:
                x = x[torch.where(x != 0)]

            assert isinstance(x, torch.Tensor)
            if self.symmetric is not None and self.symmetric is False:
                x_max = x.max().item()
                x_min = x.min().item()
            else:
                if torch.min(x) &lt; 0.0:
                    x = x.abs()
                x_max = x.max().item()
                x_min = 0.0

            if self.calib_bin_edges.nelement() == 0 and self.calib_hist.nelement() == 0:
                self.calib_hist = torch.histc(x, bins=self._num_bins, min=x_min, max=x_max)
                self.calib_bin_edges = torch.linspace(x_min, x_max, self._num_bins + 1)
            else:
                if x_min &lt; self.calib_bin_edges[0]:
                    width = (self.calib_bin_edges[1] - self.calib_bin_edges[0]).item()
                    self._num_bins += int(((self.calib_bin_edges[0] - x_min) / width).ceil().item())
                    self.calib_bin_edges = torch.arange(x_min - width,
                                                        self.calib_bin_edges[-1].item(),
                                                        width,
                                                        device=x.device)
                if x_max &gt; self.calib_bin_edges[-1]:
                    width = (self.calib_bin_edges[1] - self.calib_bin_edges[0]).item()
                    self._num_bins += int(((x_max - self.calib_bin_edges[-1]) / width).ceil().item())
                    self.calib_bin_edges = torch.arange(self.calib_bin_edges[0].item(),
                                                        x_max + width,
                                                        width,
                                                        device=x.device)
                assert x_max &lt;= self.calib_bin_edges[-1]
                assert x_min &gt;= self.calib_bin_edges[0]

                hist = torch.histc(x,
                                   bins=self._num_bins,
                                   min=self.calib_bin_edges[0].item(),
                                   max=self.calib_bin_edges[-1].item())
                hist[:self.calib_hist.numel()] += self.calib_hist
                self.calib_hist = hist

            assert isinstance(self.calib_hist, torch.Tensor)
            assert isinstance(self.calib_bin_edges, torch.Tensor)
            self.calib_hist = self.calib_hist.to(self.device)
            self.calib_bin_edges = self.calib_bin_edges.to(self.device)

        return x_orig</div>
</div>



<div class="viewcode-block" id="PerTensorHistogramObserverPro">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorHistogramObserverPro">[docs]</a>
class PerTensorHistogramObserverPro(UniformScalingObserver):
    &#39;&#39;&#39;
    A wrap of pytorch version observer: HistogramObserver
    &#39;&#39;&#39;

    def __init__(self,
                 qspec: QuantizationSpec,
                 device: Optional[torch.device] = None,
                 bins: int = 256,
                 reduce_range: bool = False,
                 upsample_rate: int = 384) -&gt; None:
        super().__init__(qspec, device)
        self.qscheme = qspec.qscheme
        self.symmetric = qspec.symmetric

        if self.qscheme != QSchemeType.per_tensor:
            raise ValueError(&quot;PerTensorHistogramObserverPro only supports per_tensor&quot;)
        torch_qscheme = torch.per_tensor_symmetric if self.symmetric else torch.per_tensor_affine

        dtype = qspec.dtype
        if dtype not in [Dtype.uint4, Dtype.int8, Dtype.uint8]:
            raise ValueError(&quot;PerTensorHistogramObserverPro only supports 4bit and 8bit&quot;)
        else:
            quant_min, quant_max = calculate_qmin_qmax(dtype)
            if dtype == Dtype.uint4:
                torch_dtype = torch.quint4x2
            elif dtype == Dtype.int8:
                torch_dtype = torch.qint8
            elif dtype == Dtype.uint8:
                torch_dtype = torch.quint8

        # The argument `upsample_rate` was removed from `HistogramObserver` in torch 2.5.
        if is_torch_greater_or_equal_2_5():
            kwargs = {}
        else:  # pragma: no cover
            kwargs = {&quot;upsample_rate&quot;: upsample_rate}

        self.histogram = HistogramObserver(qscheme=torch_qscheme,
                                           dtype=torch_dtype,
                                           bins=bins,
                                           quant_min=quant_min,
                                           quant_max=quant_max,
                                           reduce_range=reduce_range,
                                           **kwargs)

    def forward(self, x_orig: torch.Tensor) -&gt; Any:
        return self.histogram(x_orig)

    def _calculate_qparams(self) -&gt; Any:
        new_min, new_max = self.histogram._non_linear_param_search()
        return self.histogram._calculate_qparams(new_min, new_max)</div>



<div class="viewcode-block" id="PerTensorPercentileObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorPercentileObserver">[docs]</a>
class PerTensorPercentileObserver(PerTensorHistogramObserver):

    def __init__(self, qspec: QuantizationSpec, device: Optional[torch.device] = None) -&gt; None:
        super().__init__(qspec, device)

        # TODO: make the value can be set
        self._skip_zeros = True
        self._num_bins = 4200
        self.percentile = 99.99999999

    def _calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        r&quot;&quot;&quot;Calculates the quantization parameters.&quot;&quot;&quot;
        self.min_val, self.max_val = self._calculate_min_and_max_using_percentile()
        return self.calculate_qparams(self.min_val, self.max_val)

    def _calculate_min_and_max_using_percentile(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        assert isinstance(self.calib_hist, torch.Tensor)
        assert isinstance(self.calib_bin_edges, torch.Tensor)
        return self.get_min_max_by_percentile(self.calib_hist, self.calib_bin_edges, self.percentile)

<div class="viewcode-block" id="PerTensorPercentileObserver.get_min_max_by_percentile">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorPercentileObserver.get_min_max_by_percentile">[docs]</a>
    def get_min_max_by_percentile(self, histogram: torch.Tensor, bin_edges: torch.Tensor,
                                  percentile: float) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &quot;&quot;&quot;
        Calculate the minimum and maximum values of a histogram at a specified percentile.

        Parameters:
        - histogram (torch.Tensor): A tensor representing the histogram of the data. Each element
        in the histogram represents the frequency of data in the corresponding bin.
        - bin_edges (torch.Tensor): A tensor containing the edge values that correspond to the
        bins represented in the histogram. There should be one more element in `bin_edges` than
        in `histogram`.
        - percentile (int): The percentile at which to determine the minimum and maximum values.
        The value should be an integer between 0 and 100.

        Returns:
        - Tuple[torch.Tensor, torch.Tensor]: A tuple containing two tensors. The first tensor
        is the value at the specified percentile, and the second tensor is the value at the
        complementary percentile (i.e., 100-percentile).

        Raises:
        - ValueError: If the `percentile` argument is not within the range 0 to 100.
        &quot;&quot;&quot;
        if percentile &lt; 0 or percentile &gt; 100:
            raise ValueError(&quot;Percentile value must be between 0 and 100.&quot;)

        # Return None if no data is available
        if bin_edges is None and histogram is None:
            return None

        # Calculate cumulative distribution function
        hist_total = histogram.sum()
        cumulative_dist = torch.cumsum(histogram / hist_total, dim=0)

        if self.symmetric is not None and self.symmetric is False:
            target_pct_one_side = (100.0 - percentile) / 200.0

            upper_idx = (cumulative_dist &gt;= target_pct_one_side).nonzero().min().item()
            assert isinstance(upper_idx, int), &quot;Index must be an integer&quot;
            max_value = bin_edges[upper_idx]

            lower_idx = (cumulative_dist &lt;= (1 - target_pct_one_side)).nonzero().min().item()
            assert isinstance(lower_idx, int), &quot;Index must be an integer&quot;
            min_value = bin_edges[lower_idx]

        else:
            target_pct = percentile / 100.0
            cumulative_dist_max = cumulative_dist[-1].item()
            assert isinstance(cumulative_dist_max, float)
            target_pct = min(target_pct, cumulative_dist_max)

            upper_idx = (cumulative_dist &gt;= target_pct).nonzero().min().item()
            assert isinstance(upper_idx, int), &quot;Index must be an integer&quot;
            max_value = bin_edges[upper_idx]

            min_value = torch.tensor(0, device=&#39;cpu&#39;)

        max_value = max_value.to(self.device)
        min_value = min_value.to(self.device)
        return min_value, max_value</div>
</div>



<div class="viewcode-block" id="PerTensorMSEObserver">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorMSEObserver">[docs]</a>
class PerTensorMSEObserver(PerTensorHistogramObserver):

    def __init__(self, qspec: QuantizationSpec, device: Optional[torch.device] = None) -&gt; None:
        super().__init__(qspec, device)

    def _calculate_qparams(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        self.min_val, self.max_val = self._calculate_min_and_max_using_mse()
        return self.calculate_qparams(self.min_val, self.max_val)

    def _calculate_min_and_max_using_mse(self) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        assert isinstance(self.calib_hist, torch.Tensor)
        assert isinstance(self.calib_bin_edges, torch.Tensor)
        return self.get_min_max_by_mse(self.calib_hist, self.calib_bin_edges)

<div class="viewcode-block" id="PerTensorMSEObserver.get_min_max_by_mse">
<a class="viewcode-back" href="../../../../../autoapi/quark/torch/quantization/observer/observer/index.html#quark.torch.quantization.observer.observer.PerTensorMSEObserver.get_min_max_by_mse">[docs]</a>
    def get_min_max_by_mse(self,
                           calib_hist: torch.Tensor,
                           calib_bin_edges: torch.Tensor,
                           stride: int = 1,
                           start_bin: int = 2045) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        &quot;&quot;&quot;Returns amax that minimizes MSE of the collected histogram.&quot;&quot;&quot;
        # If calibrator hasn&#39;t collected any data, return none
        if calib_bin_edges is None and calib_hist is None:
            return None

        counts = calib_hist
        edges = calib_bin_edges

        counts = counts.to(self.device)
        edges = edges.to(self.device)

        centers = (edges[1:] + edges[:-1]) / 2

        mses = []
        arguments = []

        min_value = torch.tensor(0, device=&#39;cpu&#39;)
        min_value = min_value.to(self.device)

        for i in range(start_bin, len(centers), stride):
            amax = centers[i]
            if self.dtype in [Dtype.int4, Dtype.uint4, Dtype.int8, Dtype.uint8]:
                quant_centers = self.int_fake_tensor_quant(centers, min_value, amax)
            elif self.dtype in [Dtype.fp8_e4m3, Dtype.fp8_e5m2]:
                quant_centers = self.scaling_fp8(centers, amax)
            else:
                raise ValueError(f&quot;Invalid dtype {self.dtype}. dtype must be a positive integer, fp8_e4m3 or fp8_e5m2.&quot;
                                 )  # pragma: no cover

            mse = ((quant_centers - centers)**2 * counts).mean()

            mses.append(mse.cpu())
            arguments.append(i)

        argmin = torch.argmin(torch.stack(mses))
        calib_amax = centers[arguments[argmin]]

        calib_amax = calib_amax.to(self.device)

        return min_value, calib_amax</div>


    def int_fake_tensor_quant(self, X: torch.Tensor, min_value: torch.Tensor, max_value: torch.Tensor) -&gt; torch.Tensor:
        scale, zero_point = self.calculate_int_quant_params(min_value, max_value)
        assert isinstance(self.quant_min, int)
        assert isinstance(self.quant_max, int)
        X = torch.fake_quantize_per_tensor_affine(X, scale.to(torch.float), zero_point.to(torch.int), self.quant_min,
                                                  self.quant_max)
        return X

    @log_errors
    def scaling_fp8(self, X: torch.Tensor, amax: torch.Tensor) -&gt; torch.Tensor:
        min_norm, max_norm = calculate_qmin_qmax(self.dtype)
        X_orig_dtype = X.dtype
        scale = amax / max_norm
        X = X / scale
        X = torch.clamp(X, min=min_norm, max=max_norm)
        if self.dtype == Dtype.fp8_e4m3:
            fp8_dtype = torch.float8_e4m3fn
        elif self.dtype == Dtype.fp8_e5m2:
            fp8_dtype = torch.float8_e5m2
        else:
            raise ValueError(
                f&quot;Invalid dtype {self.dtype}. Supported fp8 formats: fp8_e4m3, fp8_e5m2.&quot;)  # pragma: no cover
        X = X.to(fp8_dtype).to(X_orig_dtype) * scale
        return X</div>



OBSERVER_CLASSES = {
    PlaceholderObserver, UniformScalingObserver, PerTensorMinMaxObserver, PerChannelMinMaxObserver, PerBlockMXObserver,
    PerBlockBFPObserver, PerGroupMinMaxObserver, PerTensorHistogramObserver, PerTensorPercentileObserver,
    PerTensorMSEObserver, PerTensorHistogramObserverPro
}

OBSERVER_MAP = {observer_cls.__name__: observer_cls for observer_cls in OBSERVER_CLASSES}
</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            <p>
      Last updated on Feb 12, 2025.<br/>
  </p>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../../../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../../../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

<footer class="rocm-footer">
    <div class="container-lg">
        <section class="bottom-menu menu py-45">
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <ul>
                        <li><a href="https://www.amd.com/en/corporate/copyright" target="_blank">Terms and Conditions</a></li>
                        <li><a href="https://quark.docs.amd.com/latest/license.html">Quark Licenses and Disclaimers</a></li>
                        <li><a href="https://www.amd.com/en/corporate/privacy" target="_blank">Privacy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/trademarks" target="_blank">Trademarks</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf" target="_blank">Statement on Forced Labor</a></li>
                        <li><a href="https://www.amd.com/en/corporate/competition" target="_blank">Fair and Open Competition</a></li>
                        <li><a href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf" target="_blank">UK Tax Strategy</a></li>
                        <li><a href="https://www.amd.com/en/corporate/cookies" target="_blank">Cookie Policy</a></li>
                        <!-- OneTrust Cookies Settings button start -->
                        <li><a href="#cookie-settings" id="ot-sdk-btn" class="ot-sdk-show-settings">Cookie Settings</a></li>
                        <!-- OneTrust Cookies Settings button end -->
                    </ul>
                </div>
            </div>
            <div class="row d-flex align-items-center">
                <div class="col-12 text-center">
                    <div>
                        <span class="copyright">© 2024 Advanced Micro Devices, Inc</span>
                    </div>
                </div>
            </div>
        </section>
    </div>
</footer>

<!-- <div id="rdc-watermark-container">
    <img id="rdc-watermark" src="../../../../../_static/images/alpha-watermark.svg" alt="DRAFT watermark"/>
</div> -->
  </body>
</html>