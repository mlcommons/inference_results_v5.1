### Llama2-70B (BMG) Quantization
Model Source: https://huggingface.co/meta-llama/Llama-2-70b-chat-hf

Model Quantization: BF16 -> INT4

Details: /closed/Intel/code/llama3.1-8b/pytorch-xpu/calibration/quantize_70b.sh

### Llama3.1-8B (BMG) Quantization
Model Source: https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct

Model Quantization: BF16 -> INT4

Details: /closed/Intel/code/llama3.1-8b/pytorch-xpu/calibration/quantize_8b.sh
